# üìö Documentaci√≥n Completa del Sistema de Secado de Arroz

## üìã Tabla de Contenidos

1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Componentes Principales](#componentes-principales)
4. [Flujo de Datos](#flujo-de-datos)
5. [Sistema de Timestamps Incremental](#sistema-de-timestamps-incremental)
6. [Autenticaci√≥n y Seguridad](#autenticaci√≥n-y-seguridad)
7. [Configuraci√≥n](#configuraci√≥n)
8. [Procesamiento ETL](#procesamiento-etl)
9. [Integraci√≥n con Google Drive](#integraci√≥n-con-google-drive)
10. [Interfaz Google Apps Script](#interfaz-google-apps-script)
11. [Azure Functions](#azure-functions)
12. [Estructura de Datos](#estructura-de-datos)

---

## üéØ Visi√≥n General

El **Sistema de Secado de Arroz** es una soluci√≥n automatizada end-to-end para consolidar, procesar, validar y analizar datos de sensores de secado de arroz desde m√∫ltiples plantas (JPV y RB). El sistema est√° dise√±ado para:

- **Interfaz de Carga**: Google Apps Script para cargar archivos crudos de sensores a Google Drive
- **Procesar** archivos de sensores mediante ETL (Extract, Transform, Load)
- **Cruzar** datos de sensores con controles de laboratorio y aplicar calibraci√≥n
- **Validar** datos mediante modelos de Machine Learning
- **Consolidar** archivos validados en un dataset hist√≥rico √∫nico (`df_historico.csv`)
- **Reportar** m√©tricas y gr√°ficos en formato HTML
- **Evitar reprocesamiento** mediante sistema de timestamps incremental
- **Escalar** autom√°ticamente usando Azure Functions (serverless)

### Caracter√≠sticas Principales

‚úÖ **Procesamiento Incremental**: Solo procesa archivos nuevos desde la √∫ltima ejecuci√≥n  
‚úÖ **Multi-planta**: Soporta m√∫ltiples plantas (JPV, R√≠o Branco)  
‚úÖ **Multi-sensor**: Procesa datos de m√∫ltiples sensores por planta  
‚úÖ **Cruce con Laboratorio**: Identifica autom√°ticamente tachadas y variedades  
‚úÖ **Calibraci√≥n**: Aplica curvas de calibraci√≥n para temperatura y humedad  
‚úÖ **Machine Learning**: Clasificaci√≥n y validaci√≥n autom√°tica de datos  
‚úÖ **Consolidaci√≥n Hist√≥rica**: Compilaci√≥n autom√°tica de archivos validados en dataset √∫nico  
‚úÖ **Reportes Autom√°ticos**: Generaci√≥n de HTML con m√©tricas y gr√°ficos  
‚úÖ **Serverless**: Ejecuta en Azure Functions sin infraestructura propia  
‚úÖ **Autenticaci√≥n Service Account**: Sin interacci√≥n humana requerida  
‚úÖ **Costo Cero**: Uso dentro de niveles gratuitos de Azure y Google Drive

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INTERFAZ APPS SCRIPT - CARGA (Operarios)                  ‚îÇ
‚îÇ  ‚Ä¢ Carga archivos crudos extra√≠dos de sensores                  ‚îÇ
‚îÇ  ‚Ä¢ Sube archivos a carpeta correspondiente en Google Drive       ‚îÇ
‚îÇ  ‚Ä¢ Hace POST a etl_trigger para ejecutar ETL                    ‚îÇ
‚îÇ  ‚Ä¢ Al completarse, dispara ml_trigger autom√°ticamente            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ HTTP POST (JSON metadata)
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GOOGLE DRIVE                                  ‚îÇ
‚îÇ  ‚Ä¢ Archivos RAW (sensores) en carpetas por planta/sensor         ‚îÇ
‚îÇ  ‚Ä¢ Archivos de Laboratorio (Control Tachadas)                    ‚îÇ
‚îÇ  ‚Ä¢ Archivos de Calibraci√≥n (Curvas)                              ‚îÇ
‚îÇ  ‚Ä¢ Archivos Procesados (CSV) en processed/                      ‚îÇ
‚îÇ  ‚Ä¢ Archivos Validados (CSV) en validated/                       ‚îÇ
‚îÇ  ‚Ä¢ Dataset Hist√≥rico (df_historico.csv) en validated/           ‚îÇ
‚îÇ  ‚Ä¢ Reportes HTML en reportes/                                   ‚îÇ
‚îÇ  ‚Ä¢ Timestamps (JSON) en etl_timestamps/                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   AZURE FUNCTION APP                            ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  1. ETL TRIGGER (etl_trigger)                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Recibe metadata del archivo                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Lee timestamp de √∫ltima ejecuci√≥n                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Lista archivos nuevos en carpeta                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Descarga y procesa cada archivo                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Cruza con datos de laboratorio                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Aplica curvas de calibraci√≥n                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Genera CSV procesado                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Guarda en processed/                               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚îÇ                                               ‚îÇ
‚îÇ                  ‚ñº                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  2. ML TRIGGER (ml_trigger)                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Recibe solicitud para procesar archivo procesado    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Lee archivo CSV desde processed/                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Aplica modelos de Machine Learning                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Clasifica y valida datos                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Genera archivo validado con predicciones            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Guarda en validated/                               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚îÇ                                               ‚îÇ
‚îÇ                  ‚ñº                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  3. COMPILADOR TRIGGER (compilador_trigger)              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Lee todos los CSV validados desde validated/        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Concatena y elimina duplicados por ID_tachada       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Genera/actualiza df_historico.csv                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Guarda en validated/ (misma carpeta)                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚îÇ                                               ‚îÇ
‚îÇ                  ‚ñº                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  4. REPORTE TRIGGER (reporte_trigger)                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Lee df_historico.csv desde validated/              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Calcula m√©tricas estad√≠sticas                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Genera gr√°ficos embebidos (base64)                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Crea reporte HTML completo                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ Guarda en reportes/                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              SHARED CODE (shared_code/)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GoogleDriveClient: Acceso a Google Drive              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ TimestampManager: Gesti√≥n de timestamps                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ETL Core: Procesamiento de archivos                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Lab Crosser: Cruce con laboratorio                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Calibraci√≥n: Aplicaci√≥n de curvas                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ML Predictor: Modelos de Machine Learning             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Compilador Hist√≥rico: Consolidaci√≥n de archivos       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Reporte Builder: Generaci√≥n de reportes HTML          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Config: Gesti√≥n de configuraci√≥n                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INTERFAZ APPS SCRIPT - REPORTE (Latitud)                 ‚îÇ
‚îÇ  ‚Ä¢ Selector de planta                                            ‚îÇ
‚îÇ  ‚Ä¢ Al hacer clic en "Generar Reporte":                          ‚îÇ
‚îÇ    ‚îú‚îÄ> Llama a compilador_trigger (actualiza df_historico.csv)  ‚îÇ
‚îÇ    ‚îî‚îÄ> Llama a reporte_trigger (genera HTML)                   ‚îÇ
‚îÇ  ‚Ä¢ Descarga reporte y env√≠a correo con PDF adjunto              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tecnolog√≠as Utilizadas

- **Python 3.11+**: Lenguaje principal
- **Azure Functions**: Plataforma serverless (Consumption Plan)
- **Azure Storage Account**: Blob Storage para artefactos y dependencias
- **Google Drive API**: Almacenamiento y acceso a archivos
- **Pandas**: Procesamiento de datos
- **Google Service Account**: Autenticaci√≥n sin interacci√≥n
- **Matplotlib/Seaborn**: Generaci√≥n de gr√°ficos para reportes

---

## üîß Componentes Principales

### 1. **GoogleDriveClient** (`shared_code/gdrive_client.py`)

Cliente para interactuar con Google Drive usando **Service Account**.

**Funcionalidades:**
- ‚úÖ Autenticaci√≥n autom√°tica con Service Account
- ‚úÖ Descarga de archivos por `fileId` o `file_path`
- ‚úÖ Listado de archivos por carpeta o `folderId`
- ‚úÖ Subida de archivos procesados
- ‚úÖ Creaci√≥n de carpetas autom√°tica
- ‚úÖ Actualizaci√≥n de archivos existentes

**M√©todos principales:**
```python
client = GoogleDriveClient()
content = client.download_file("path/to/file.txt", file_id="1abc123...")
files = client.list_files_by_folder_id("folder_id_123")
client.upload_file_to_folder(folder_id, "output.csv", csv_bytes, mime_type="text/csv")
```

**Autenticaci√≥n:**
- Busca credenciales en:
  1. Variable de entorno `GOOGLE_SERVICE_ACCOUNT_JSON` (recomendado para Azure)
  2. Archivo `service-account-key.json` en la ra√≠z del proyecto

### 2. **TimestampManager** (`shared_code/timestamp_manager.py`)

Gestiona timestamps de √∫ltima ejecuci√≥n para procesamiento incremental.

**Funcionalidades:**
- ‚úÖ Lee timestamp de √∫ltima ejecuci√≥n por planta-secadora
- ‚úÖ Actualiza timestamp despu√©s de procesar
- ‚úÖ Guarda metadata de archivos procesados
- ‚úÖ Almacena timestamps en Google Drive (carpeta `etl_timestamps/`)

**Estructura del archivo de timestamp:**
```json
{
  "planta": "JPV",
  "secadora": "Secadora 1",
  "last_run": "2025-11-19T14:30:00.000Z",
  "last_processed_files": [
    {
      "fileId": "1abc123...",
      "fileName": "datos_001.xlsx",
      "processedAt": "2025-11-19T14:30:00.000Z",
      "status": "success",
      "records_processed": 750
    }
  ],
  "total_files_processed": 5,
  "last_updated": "2025-11-19T14:30:00.000Z"
}
```

**Ubicaci√≥n:** `etl_timestamps/last_run_timestamp_{PLANTA}_{SECADORA}.json`

### 3. **ETL Core** (`shared_code/etl_core.py`)

Procesamiento de archivos de sensores con formato unificado para JPV y RB.

**Funcionalidades:**
- ‚úÖ Lectura de archivos JPV (TXT, UTF-16)
- ‚úÖ Lectura de archivos RB (CSV, UTF-8) con detecci√≥n robusta de formatos
- ‚úÖ Normalizaci√≥n de timestamps
- ‚úÖ Conversi√≥n a formato largo (long format)
- ‚úÖ Detecci√≥n autom√°tica de formato y separadores
- ‚úÖ Estandarizaci√≥n de variables: ambas plantas producen `VOLT_HUM` y `VOLT_TEM`

**Formatos soportados:**
- **JPV**: Archivos TXT con encoding UTF-16, columnas: `Time`, `VarName`, `VarValue`
- **RB**: Archivos CSV con encoding UTF-8
  - Separadores: `;` o `,` (detecci√≥n autom√°tica)
  - Columnas de fecha: `Date`, `Fecha`
  - Columnas de hora: `Time`, `Hora`, `LOC_time`, `LOCTime`
  - Columnas de voltaje: `V_Hum`, `V_HUM`, `V_Tem`, `V_TEM`, `V_Temp`, etc.

**Salida unificada (JPV y RB):**
DataFrame con columnas:
- `timestamp`: Fecha/hora normalizada (UTC)
- `variable`: `VOLT_HUM` o `VOLT_TEM` (estandarizado para ambas plantas)
- `valor`: Valor num√©rico del voltaje (RB dividido por 100 autom√°ticamente)
- `Date_raw`: Fecha cruda (para RB) o `None` (para JPV)
- `LOC_time_raw`: Hora cruda (para RB) o `None` (para JPV)
- `planta`: Planta de origen (`JPV` o `RB`)
- `sensor_id`: ID del sensor (inferido del nombre del archivo)
- `source_file`: Nombre del archivo de origen

**Nota importante:** La calibraci√≥n (conversi√≥n de `VOLT_HUM`/`VOLT_TEM` a `HUMEDAD`/`TEMPERATURA`) se aplica **despu√©s** en el pipeline cuando hay informaci√≥n del laboratorio, no en esta etapa.

### 4. **Lab Crosser** (`shared_code/lab_crosser.py`)

Cruce de datos de sensores con controles de laboratorio.

**Funcionalidades:**
- ‚úÖ Carga archivos Excel de "Control Tachadas"
- ‚úÖ Normalizaci√≥n de IDs de tachadas
- ‚úÖ Cruce por timestamp y sensor_id
- ‚úÖ Identificaci√≥n de variedades y tachadas

**Columnas agregadas:**
- `Variedad`: Variedad de arroz
- `ID_tachada`: ID de la tachada
- `HumedadInicial`: Humedad inicial de laboratorio
- `HumedadFinal`: Humedad final de laboratorio
- `En_duda`: Indicador de datos en duda

### 5. **Calibraci√≥n** (`shared_code/calibracion.py`)

Aplicaci√≥n de curvas de calibraci√≥n para convertir voltajes a valores reales.

**Funcionalidades:**
- ‚úÖ B√∫squeda de archivos de curvas de calibraci√≥n por planta y a√±o
- ‚úÖ Conversi√≥n de VOLT_HUM ‚Üí HUMEDAD (por variedad)
- ‚úÖ Conversi√≥n de VOLT_TEM ‚Üí TEMPERATURA (global)
- ‚úÖ Aplicaci√≥n de correcciones fijas (C_fix) y variables (C_var) por sensor
- ‚úÖ Soporte para m√∫ltiples variedades con curvas espec√≠ficas

**F√≥rmulas aplicadas:**
- **TEMPERATURA**: `VT * AT + BT + C_fix_T[sensor] - C_var_T[sensor, timestamp]`
- **HUMEDAD**: `(VH¬≤) * AH + VH * BH + CH + C_fix_H[sensor] - C_var_H[sensor, timestamp]`

**Estructura de archivos de calibraci√≥n:**
- Hoja "TEMPERATURA": Constantes AT, BT y correcciones por sensor
- Hojas por variedad: Constantes AH, BH, CH y correcciones por variedad y sensor

### 6. **ML Predictor** (`shared_code/ml_predictor.py`)

Aplicaci√≥n de modelos de Machine Learning para clasificaci√≥n y validaci√≥n.

**Funcionalidades:**
- ‚úÖ Lee archivos procesados desde `processed/`
- ‚úÖ Aplica modelos de ML para clasificar datos
- ‚úÖ Valida datos y detecta anomal√≠as
- ‚úÖ Categoriza datos correctamente
- ‚úÖ Genera archivos validados con predicciones

**Columnas agregadas en archivos validados:**
- `categoria`: Categor√≠a asignada por el modelo ML
- `confianza`: Nivel de confianza de la clasificaci√≥n
- `valido`: Indicador de validaci√≥n (True/False)
- `prediccion`: Predicci√≥n binaria (0/1)
- `anomalia`: Indicador de detecci√≥n de anomal√≠as

### 7. **Compilador Hist√≥rico** (`shared_code/compilador_historico.py`)

Consolida todos los archivos CSV validados en un dataset hist√≥rico √∫nico.

**Funcionalidades:**
- ‚úÖ Lee todos los CSV validados desde `validated/`
- ‚úÖ Concatena archivos en un DataFrame √∫nico
- ‚úÖ Elimina duplicados por `ID_tachada` (mantiene la primera ocurrencia)
- ‚úÖ Agrega columna `archivo_origen` para trazabilidad
- ‚úÖ Genera/actualiza `df_historico.csv` en `validated/`

**Resultado:**
- Dataset hist√≥rico consolidado con una fila por tachada
- Datos agregados por tachada (duraci√≥n, estad√≠sticas, etc.)
- Ubicaci√≥n: `validated/df_historico.csv` (misma carpeta que los archivos validados)

### 8. **Reporte Builder** (`shared_code/reporte_builder.py`)

M√≥dulo de generaci√≥n de reportes HTML con m√©tricas y gr√°ficos a partir del dataset hist√≥rico consolidado.

**Funcionalidades:**
- ‚úÖ Lee `df_historico.csv` desde `validated/`
- ‚úÖ Calcula m√©tricas estad√≠sticas por planta, sensor, variedad, tachada
- ‚úÖ Genera gr√°ficos y visualizaciones embebidos en HTML (base64)
- ‚úÖ Detecci√≥n din√°mica de columnas con fallbacks para compatibilidad
- ‚úÖ Genera reporte HTML completo con dise√±o profesional
- ‚úÖ Integra logo de la empresa desde Google Drive
- ‚úÖ Manejo robusto de columnas faltantes (no rompe si faltan datos)

**Estructura del reporte:**
- **BLOQUE 1**: Cantidad de tachadas (global, por secadora, evoluci√≥n semanal)
- **BLOQUE 2**: Temperaturas (por turno, variedad, distribuci√≥n)
- **BLOQUE 3**: Duraci√≥n de las tachadas (estad√≠sticas, distribuci√≥n, evoluci√≥n)
- **BLOQUE 4**: Comparaci√≥n con laboratorio (humedad inicial/final, diferencias)

**Salida:**
- Reporte HTML con gr√°ficos embebidos (base64)
- Ubicaci√≥n: Google Drive (carpeta `reportes/`)

### 9. **Config** (`shared_code/config.py`)

Gesti√≥n centralizada de configuraci√≥n de folder IDs de Google Drive.

**Funcionalidades:**
- ‚úÖ Obtiene folder IDs desde variables de entorno
- ‚úÖ Validaci√≥n de configuraci√≥n por planta
- ‚úÖ Funciones helper: `get_lab_folder_id()`, `get_processed_folder_id()`, `get_validated_folder_id()`, `get_reports_folder_id()`

**Variables de entorno requeridas:**
- `LAB_FOLDER_JPV`, `LAB_FOLDER_RB`: Carpetas de archivos de laboratorio
- `PROCESSED_FOLDER_JPV`, `PROCESSED_FOLDER_RB`: Carpetas de archivos procesados
- `VALIDATED_FOLDER_JPV`, `VALIDATED_FOLDER_RB`: Carpetas de archivos validados
- `REPORTS_FOLDER_JPV`, `REPORTS_FOLDER_RB`: Carpetas de reportes

---

## üîÑ Flujo de Datos

### Flujo Completo del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 1: CARGA DE DATOS (Google Apps Script - Interfaz Carga)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. INTERFAZ GOOGLE APPS SCRIPT (Operarios)
   ‚îú‚îÄ> Usuario carga archivos crudos extra√≠dos de sensores
   ‚îú‚îÄ> Selecciona planta (JPV o RB) y secadora
   ‚îú‚îÄ> Archivos se suben a carpeta correspondiente en Google Drive
   ‚îÇ   ‚îî‚îÄ> Ubicaci√≥n: Secado_Arroz/{PLANTA}/raw/{SENSOR}/
   ‚îî‚îÄ> Hace POST a Azure Function (etl_trigger) con metadata del archivo
       ‚îî‚îÄ> Metadata: fileId, fileName, secadora, planta, folderId, uploadDate, etc.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 2: PROCESAMIENTO ETL (etl_trigger)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2. AZURE FUNCTION (etl_trigger)
   ‚îú‚îÄ> Valida metadata recibida
   ‚îú‚îÄ> Inicializa GoogleDriveClient
   ‚îú‚îÄ> Inicializa TimestampManager
   ‚îÇ
   ‚îú‚îÄ> MODO INCREMENTAL (si tiene folderId y secadora):
   ‚îÇ   ‚îú‚îÄ> Lee timestamp de √∫ltima ejecuci√≥n desde etl_timestamps/
   ‚îÇ   ‚îú‚îÄ> Lista TODOS los archivos en la carpeta de esa secadora
   ‚îÇ   ‚îú‚îÄ> Filtra archivos modificados despu√©s del timestamp
   ‚îÇ   ‚îî‚îÄ> Procesa cada archivo nuevo
   ‚îÇ
   ‚îî‚îÄ> MODO LEGACY (si no tiene folderId o secadora):
       ‚îî‚îÄ> Procesa solo el archivo recibido

3. PROCESAMIENTO POR ARCHIVO:
   ‚îú‚îÄ> Descarga archivo RAW desde Google Drive
   ‚îú‚îÄ> Detecta formato (JPV TXT o RB CSV)
   ‚îú‚îÄ> Lee y normaliza datos de sensores
   ‚îú‚îÄ> Convierte a formato largo (long format)
   ‚îÇ   ‚îî‚îÄ> Estandariza variables: JPV y RB producen VOLT_HUM/VOLT_TEM
   ‚îú‚îÄ> Consolida datos de m√∫ltiples sensores (si existen)
   ‚îú‚îÄ> Busca archivo de laboratorio correspondiente
   ‚îú‚îÄ> Cruza datos con laboratorio (Lab Crosser)
   ‚îÇ   ‚îî‚îÄ> Agrega: Variedad, ID_tachada, HumedadInicial, HumedadFinal, En_duda
   ‚îú‚îÄ> Busca archivos de curvas de calibraci√≥n
   ‚îú‚îÄ> Aplica calibraci√≥n (calibracion.py)
   ‚îÇ   ‚îú‚îÄ> Convierte formato largo a ancho (to_wide)
   ‚îÇ   ‚îî‚îÄ> Convierte VOLT_HUM ‚Üí HUMEDAD, VOLT_TEM ‚Üí TEMPERATURA
   ‚îî‚îÄ> Genera CSV procesado (formato unificado para JPV y RB)

4. GUARDADO ETL:
   ‚îú‚îÄ> Sube CSV procesado a Google Drive
   ‚îÇ   ‚îî‚îÄ> Ubicaci√≥n: Secado_Arroz/{PLANTA}/processed/
   ‚îî‚îÄ> Actualiza timestamp (solo en modo incremental)

5. ORQUESTACI√ìN AUTOM√ÅTICA:
   ‚îî‚îÄ> Al completarse etl_trigger exitosamente, la l√≥gica de orquestaci√≥n
       (en Apps Script o en el mismo etl_trigger) dispara ml_trigger
       autom√°ticamente con la informaci√≥n del nuevo archivo procesado

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 3: VALIDACI√ìN ML (ml_trigger)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

6. AZURE FUNCTION (ml_trigger)
   ‚îú‚îÄ> Recibe solicitud para procesar archivo procesado
   ‚îú‚îÄ> Lee archivo CSV desde carpeta processed/
   ‚îú‚îÄ> Aplica modelos de Machine Learning:
   ‚îÇ   ‚îú‚îÄ> Clasifica datos seg√∫n categor√≠as predefinidas
   ‚îÇ   ‚îú‚îÄ> Valida calidad de datos
   ‚îÇ   ‚îú‚îÄ> Detecta anomal√≠as y outliers
   ‚îÇ   ‚îî‚îÄ> Categoriza datos correctamente
   ‚îî‚îÄ> Genera archivo validado con columnas adicionales

7. GUARDADO ML:
   ‚îî‚îÄ> Sube CSV validado a Google Drive
       ‚îî‚îÄ> Ubicaci√≥n: Secado_Arroz/{PLANTA}/validated/

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 4: CONSOLIDACI√ìN HIST√ìRICA (compilador_trigger)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

8. AZURE FUNCTION (compilador_trigger)
   ‚îú‚îÄ> Recibe solicitud desde interfaz de reporte (Apps Script)
   ‚îú‚îÄ> Lee todos los CSV validados desde validated/
   ‚îú‚îÄ> Concatena archivos en un DataFrame √∫nico
   ‚îú‚îÄ> Elimina duplicados por ID_tachada (mantiene primera ocurrencia)
   ‚îú‚îÄ> Agrega columna archivo_origen para trazabilidad
   ‚îî‚îÄ> Genera/actualiza df_historico.csv en validated/

9. GUARDADO HIST√ìRICO:
   ‚îî‚îÄ> Sube/actualiza df_historico.csv en Google Drive
       ‚îî‚îÄ> Ubicaci√≥n: Secado_Arroz/{PLANTA}/validated/df_historico.csv

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 5: GENERACI√ìN DE REPORTES (reporte_trigger)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

10. INTERFAZ GOOGLE APPS SCRIPT (Reporte - Latitud)
    ‚îú‚îÄ> Usuario selecciona planta (JPV o RB)
    ‚îú‚îÄ> Hace clic en "Generar Reporte"
    ‚îú‚îÄ> Llama primero a compilador_trigger (actualiza df_historico.csv)
    ‚îî‚îÄ> Al completarse exitosamente, llama a reporte_trigger

11. AZURE FUNCTION (reporte_trigger)
    ‚îú‚îÄ> Descarga df_historico.csv desde validated/
    ‚îú‚îÄ> Usa reporte_builder.py para generar reporte HTML:
    ‚îÇ   ‚îú‚îÄ> Detecci√≥n din√°mica de columnas (con fallbacks)
    ‚îÇ   ‚îú‚îÄ> Calcula m√©tricas estad√≠sticas por tachada, planta, variedad
    ‚îÇ   ‚îú‚îÄ> Genera gr√°ficos embebidos (base64)
    ‚îÇ   ‚îú‚îÄ> Incluye logo de la empresa
    ‚îÇ   ‚îî‚îÄ> Manejo robusto de columnas faltantes
    ‚îî‚îÄ> Crea reporte HTML completo con dise√±o profesional

12. GUARDADO REPORTE:
    ‚îî‚îÄ> Sube reporte HTML a Google Drive
        ‚îî‚îÄ> Ubicaci√≥n: Secado_Arroz/{PLANTA}/reportes/reporte_tachadas_{PLANTA}.html

13. DISTRIBUCI√ìN:
    ‚îî‚îÄ> Apps Script descarga el reporte generado
        ‚îî‚îÄ> Env√≠a correo con PDF adjunto y enlace/bot√≥n para acceder al HTML
```

### Ejemplo de Metadata Recibida por etl_trigger

```json
{
  "fileId": "1abc123xyz789",
  "fileName": "sensor_1_datos_2025_11_19.xlsx",
  "secadora": "Secadora 1",
  "planta": "JPV",
  "folderId": "1xyz789abc123",
  "uploadDate": "2025-11-19T14:30:00.000Z",
  "size": 245760,
  "mimeType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
  "fileUrl": "https://drive.google.com/file/d/1abc123xyz789/view",
  "driveUrl": "https://drive.google.com/file/d/1abc123xyz789/view"
}
```

### Ejemplo de Respuesta de etl_trigger (Modo Incremental)

```json
{
  "success": true,
  "message": "ETL incremental completado - 2 archivos procesados",
  "timestamp": "2025-11-19T15:00:00.000Z",
  "metadata": {
    "planta": "JPV",
    "secadora": "Secadora 1",
    "year": 2025,
    "total_files_processed": 2,
    "total_records_processed": 1500,
    "total_records_matched_lab": 1200,
    "total_records_unmatched": 300,
    "last_run_timestamp": "2025-11-19T10:00:00.000Z"
  },
  "processed_files": [
    {
      "fileId": "1abc123...",
      "fileName": "archivo_B.xlsx",
      "processedAt": "2025-11-19T15:00:00.000Z",
      "status": "success",
      "records_processed": 750,
      "records_matched_lab": 600,
      "records_unmatched": 150,
      "processed_file": "archivo_B_processed_20251119T150000Z.csv",
      "processed_path": "Secado_Arroz/JPV/processed/archivo_B_processed_20251119T150000Z.csv"
    }
  ]
}
```

---

## ‚è±Ô∏è Sistema de Timestamps Incremental

### Objetivo

Evitar reprocesar archivos ya procesados, procesando solo archivos nuevos desde la √∫ltima ejecuci√≥n.

### Funcionamiento

1. **Primera Ejecuci√≥n:**
   - No existe timestamp ‚Üí procesa TODOS los archivos en la carpeta
   - Guarda timestamp con fecha/hora actual

2. **Ejecuciones Subsecuentes:**
   - Lee timestamp de √∫ltima ejecuci√≥n desde `etl_timestamps/`
   - Lista todos los archivos en la carpeta
   - Filtra solo archivos con `modifiedTime > last_run`
   - Procesa solo archivos nuevos
   - Actualiza timestamp con el `modifiedTime` m√°s reciente

### Ventajas

‚úÖ **Eficiencia**: No reprocesa archivos ya procesados  
‚úÖ **Procesamiento en lote**: Procesa m√∫ltiples archivos nuevos en una ejecuci√≥n  
‚úÖ **Resiliencia**: Si falla un archivo, contin√∫a con los dem√°s  
‚úÖ **Trazabilidad**: Guarda metadata de archivos procesados  

### Ejemplo de Caso de Uso

**Escenario:**
- √öltima ejecuci√≥n: `2025-11-19 10:00:00`
- Carpeta tiene 3 archivos:
  - `archivo_A.xlsx` (modificado: `09:00:00`) ‚Üí ‚ùå NO procesar
  - `archivo_B.xlsx` (modificado: `11:00:00`) ‚Üí ‚úÖ S√ç procesar
  - `archivo_C.xlsx` (modificado: `12:00:00`) ‚Üí ‚úÖ S√ç procesar

**Resultado:**
- Procesa 2 archivos (B y C)
- Guarda timestamp: `2025-11-19 12:00:00`
- Pr√≥xima ejecuci√≥n solo procesar√° archivos posteriores a `12:00:00`

### Archivos de Timestamp

**Ubicaci√≥n:** `etl_timestamps/last_run_timestamp_{PLANTA}_{SECADORA}.json`

**Ejemplos:**
- `etl_timestamps/last_run_timestamp_JPV_Secadora_1.json`
- `etl_timestamps/last_run_timestamp_JPV_Secadora_2.json`
- `etl_timestamps/last_run_timestamp_RB_Secadora_1.json`

---

## üîê Autenticaci√≥n y Seguridad

### Service Account (Google Cloud)

El sistema usa **Service Account** para autenticaci√≥n autom√°tica sin interacci√≥n humana.

**Ventajas:**
- ‚úÖ No requiere navegador/consola
- ‚úÖ Ideal para entornos serverless (Azure Functions)
- ‚úÖ Autenticaci√≥n autom√°tica sin tokens expirables
- ‚úÖ No depende de sesiones de usuario

**Configuraci√≥n:**

1. **Crear Service Account en Google Cloud Console:**
   - IAM & Admin ‚Üí Service Accounts
   - Crear nueva Service Account
   - Descargar archivo JSON de credenciales

2. **Compartir carpetas en Google Drive:**
   - Compartir carpetas necesarias con el email de la Service Account
   - Formato: `nombre@proyecto.iam.gserviceaccount.com`
   - Permisos: Editor para carpetas de salida, Lector para carpetas de entrada

3. **Configurar en Azure Function App:**
   - Settings ‚Üí Configuration ‚Üí Application settings
   - Agregar variable: `GOOGLE_SERVICE_ACCOUNT_JSON`
   - Valor: Contenido completo del JSON (como string)

**Scopes requeridos:**
- `https://www.googleapis.com/auth/drive` (lectura y escritura)

### Seguridad

- ‚úÖ Credenciales almacenadas como variables de entorno (no en c√≥digo)
- ‚úÖ Service Account con permisos m√≠nimos necesarios
- ‚úÖ Autenticaci√≥n HTTP requerida en Azure Functions (`authLevel: "function"`)

---

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno (Azure Functions)

```json
{
  "GOOGLE_SERVICE_ACCOUNT_JSON": "{...contenido completo del JSON...}",
  "LAB_FOLDER_JPV": "folder_id_lab_jpv",
  "LAB_FOLDER_RB": "folder_id_lab_rb",
  "PROCESSED_FOLDER_JPV": "folder_id_processed_jpv",
  "PROCESSED_FOLDER_RB": "folder_id_processed_rb",
  "VALIDATED_FOLDER_JPV": "folder_id_validated_jpv",
  "VALIDATED_FOLDER_RB": "folder_id_validated_rb",
  "REPORTS_FOLDER_JPV": "folder_id_reports_jpv",
  "REPORTS_FOLDER_RB": "folder_id_reports_rb"
}
```

---

## üîÑ Procesamiento ETL

### Pasos del Procesamiento

1. **Descarga de Archivo**
   - Usa `fileId` directamente (m√°s eficiente)
   - Fallback a b√∫squeda por `file_path` si no hay `fileId`

2. **Detecci√≥n de Formato**
   - **JPV**: Archivos TXT, encoding UTF-16
   - **RB**: Archivos CSV, encoding UTF-8

3. **Lectura y Normalizaci√≥n**
   - **JPV**: Parsea timestamps desde `TimeString` o `Time`
   - **RB**: Construye timestamp desde `Date` + `LOC time` (detecci√≥n robusta de columnas)
   - Normaliza nombres de variables a formato est√°ndar
   - **RB**: Divide valores de voltaje por 100 (x0.01) para equiparar con JPV
   - Convierte valores a num√©rico
   - Filtra variables de metadata (ej: `$RT_*` para JPV)

4. **Conversi√≥n a Formato Largo**
   - Transforma de formato ancho a largo mediante `melt()`
   - **Estandarizaci√≥n de variables:**
     - JPV: Normaliza variables a `VOLT_HUM`, `VOLT_TEM`
     - RB: Mapea `V_HUM`/`V_HUM` ‚Üí `VOLT_HUM`, `V_TEM`/`V_TEM` ‚Üí `VOLT_TEM`
   - Columnas resultantes: `timestamp`, `variable` (`VOLT_HUM`/`VOLT_TEM`), `valor`
   - Agrega metadata: `planta`, `sensor_id`, `source_file`, `Date_raw`, `LOC_time_raw`

5. **Cruce con Laboratorio**
   - Busca archivo Excel de "Control Tachadas"
   - Cruza por `timestamp` y `sensor_id`
   - Agrega: `Variedad`, `ID_tachada`, `HumedadInicial`, `HumedadFinal`, `En_duda`

6. **Consolidaci√≥n de Sensores**
   - Agrupa datos de m√∫ltiples sensores si existen
   - Elimina duplicados por `timestamp` y `variable`
   - Unifica formato para preparar la calibraci√≥n

7. **Aplicaci√≥n de Calibraci√≥n**
   - Busca archivos de curvas de calibraci√≥n por planta y a√±o
   - Patr√≥n: `*{A√ëO}*Curvas*{PLANTA}*.xlsx`
   - **Importante**: Solo se aplica cuando hay informaci√≥n del laboratorio (variedad)
   - Convierte formato largo a ancho (`to_wide()`) para aplicar calibraci√≥n
   - Lee hoja "TEMPERATURA" para constantes AT, BT
   - Lee hoja de variedad para constantes AH, BH, CH
   - Aplica f√≥rmulas de calibraci√≥n:
     - **TEMPERATURA**: `VT * AT + BT + C_fix_T[sensor] - C_var_T[sensor, timestamp]`
     - **HUMEDAD**: `(VH¬≤) * AH + VH * BH + CH + C_fix_H[sensor] - C_var_H[sensor, timestamp]`
   - Agrega columnas: `TEMPERATURA`, `HUMEDAD`
   - Convierte de vuelta a formato largo si es necesario

8. **Generaci√≥n de CSV Procesado**
   - Ordena columnas consistentemente
   - Incluye columnas: `timestamp`, `variable`, `valor`, `planta`, `sensor_id`, `source_file`, `Variedad`, `ID_tachada`, `VOLT_HUM`, `VOLT_TEM`, `TEMPERATURA`, `HUMEDAD`
   - Guarda en formato CSV UTF-8
   - Nombre: `{archivo_original}_processed_{timestamp}.csv`
   - **Resultado**: Formato id√©ntico para JPV y RB

### Manejo de Errores

- ‚úÖ Si un archivo falla, contin√∫a con los dem√°s
- ‚úÖ Logging detallado de cada error
- ‚úÖ Respuesta JSON con status de cada archivo
- ‚úÖ Timestamp se actualiza solo si hay archivos procesados exitosamente

---

## üìÅ Integraci√≥n con Google Drive

### Estructura de Carpetas

```
Secado_Arroz/
‚îú‚îÄ‚îÄ JPV/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensor_1/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archivo_001.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ archivo_002.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensor_2/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ laboratorio/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Control_Tachadas_2025.xlsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calibracion/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ 2025 Curvas JPV.xlsx
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archivo_001_processed_20251119T150000Z.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ archivo_002_processed_20251119T160100Z.csv
‚îÇ   ‚îú‚îÄ‚îÄ validated/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archivo_001_validated_20251119T160000Z.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archivo_002_validated_20251119T160100Z.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ df_historico.csv  ‚Üê Dataset hist√≥rico consolidado
‚îÇ   ‚îî‚îÄ‚îÄ reportes/
‚îÇ       ‚îú‚îÄ‚îÄ reporte_tachadas_JPV.html
‚îÇ       ‚îî‚îÄ‚îÄ (im√°genes PNG de gr√°ficos)
‚îú‚îÄ‚îÄ RB/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensor_1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ laboratorio/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ validated/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ df_historico.csv  ‚Üê Dataset hist√≥rico consolidado
‚îÇ   ‚îî‚îÄ‚îÄ reportes/
‚îÇ       ‚îî‚îÄ‚îÄ reporte_tachadas_RB.html
‚îî‚îÄ‚îÄ etl_timestamps/
    ‚îú‚îÄ‚îÄ last_run_timestamp_JPV_Secadora_1.json
    ‚îú‚îÄ‚îÄ last_run_timestamp_JPV_Secadora_2.json
    ‚îî‚îÄ‚îÄ last_run_timestamp_RB_Secadora_1.json
```

### Operaciones en Google Drive

**Lectura:**
- Listar archivos por carpeta
- Descargar archivos por `fileId` o `file_path`
- Leer archivos de timestamps

**Escritura:**
- Subir archivos procesados (CSV)
- Subir archivos validados (CSV)
- Crear/actualizar `df_historico.csv`
- Subir reportes HTML e im√°genes
- Crear carpetas autom√°ticamente
- Actualizar archivos de timestamps

---

## üì± Interfaz Google Apps Script

### Dos Interfaces Diferenciadas

#### 1. Interfaz de Carga (Operarios)

Interfaz independiente para cada planta (JPV y RB) que permite cargar archivos de sensores.

**Funcionalidades:**
- ‚úÖ Interfaz de usuario para carga de archivos
- ‚úÖ Detecci√≥n autom√°tica de planta y sensor desde nombre de archivo o selecci√≥n manual
- ‚úÖ Subida autom√°tica a carpeta correspondiente en Google Drive
- ‚úÖ Trigger autom√°tico del ETL mediante POST a `etl_trigger`
- ‚úÖ Orquestaci√≥n autom√°tica: al completarse `etl_trigger`, dispara `ml_trigger` con el nuevo archivo procesado
- ‚úÖ Validaci√≥n de formato y estructura de archivos

**Flujo de Operaci√≥n:**

1. **Usuario carga archivo:**
   - Selecciona archivo desde dispositivo local
   - La interfaz detecta o permite seleccionar planta y sensor

2. **Subida a Google Drive:**
   - Archivo se sube a: `Secado_Arroz/{PLANTA}/raw/{SENSOR}/`
   - Se preserva nombre original o se renombra con timestamp

3. **Trigger de ETL:**
   - Hace POST a Azure Function `/api/etl_trigger`
   - Env√≠a metadata: `fileId`, `fileName`, `planta`, `secadora`, `folderId`, etc.
   - Azure Function procesa el archivo autom√°ticamente

4. **Orquestaci√≥n autom√°tica:**
   - Al completarse `etl_trigger` exitosamente, la l√≥gica de orquestaci√≥n (en Apps Script o en el mismo `etl_trigger`) dispara `ml_trigger` autom√°ticamente con la informaci√≥n del nuevo archivo procesado

**Ventajas:**
- ‚úÖ Interfaz familiar (Google Apps Script)
- ‚úÖ Sin necesidad de acceso directo a Google Drive API
- ‚úÖ Procesamiento autom√°tico sin intervenci√≥n manual
- ‚úÖ Trazabilidad completa del proceso

#### 2. Interfaz de Solicitud de Reporte (Latitud)

Interfaz para solicitar y distribuir reportes HTML.

**Funcionalidades:**
- ‚úÖ Selector de planta (JPV o RB)
- ‚úÖ Bot√≥n "Generar Reporte"
- ‚úÖ Orquestaci√≥n de dos pasos:
  1. Llama a `compilador_trigger` (actualiza `df_historico.csv`)
  2. Al completarse exitosamente, llama a `reporte_trigger` (genera HTML)
- ‚úÖ Descarga del reporte generado
- ‚úÖ Env√≠o de correo con PDF adjunto y enlace/bot√≥n para acceder al HTML

**Flujo de Operaci√≥n:**

1. **Usuario solicita reporte:**
   - Selecciona planta (JPV o RB)
   - Hace clic en "Generar Reporte"

2. **Consolidaci√≥n hist√≥rica:**
   - Llama a `compilador_trigger` con par√°metro `planta`
   - `compilador_trigger` lee todos los CSV validados desde `validated/`
   - Concatena y elimina duplicados por `ID_tachada`
   - Genera/actualiza `df_historico.csv` en `validated/`

3. **Generaci√≥n de reporte:**
   - Al completarse `compilador_trigger` exitosamente, llama a `reporte_trigger` con par√°metro `planta`
   - `reporte_trigger` descarga `df_historico.csv` desde `validated/`
   - Genera reporte HTML con m√©tricas y gr√°ficos
   - Sube reporte a `reportes/`

4. **Distribuci√≥n:**
   - Apps Script descarga el reporte generado
   - Env√≠a correo con PDF adjunto y enlace/bot√≥n para acceder al HTML

---

## ‚òÅÔ∏è Azure Functions

### Infraestructura en Azure

El sistema corre sobre **Azure Functions** con la siguiente configuraci√≥n:

1. **Storage Account (Blob Storage)**
   - Almacena artefactos internos y dependencias
   - Requerido por Azure Functions para operaci√≥n

2. **Function App**
   - Entorno Python 3.11
   - Plan de consumo (Consumption Plan)
   - Variables de entorno configuradas
   - C√≥digo compartido en carpeta `shared_code/`

3. **Azure Functions Individuales**
   - `etl_trigger`: Procesamiento ETL
   - `ml_trigger`: Validaci√≥n con Machine Learning
   - `compilador_trigger`: Consolidaci√≥n hist√≥rica
   - `reporte_trigger`: Generaci√≥n de reportes HTML

### Funciones Disponibles

#### 1. **`etl_trigger`** (HTTP Trigger)

- **Ruta:** `/api/etl_trigger`
- **M√©todo:** POST
- **Autenticaci√≥n:** Function key
- **Funci√≥n:** Procesa archivos RAW de sensores
- **Procesos:**
  - Descarga archivos desde Google Drive
  - Normaliza y transforma datos
  - Cruza con datos de laboratorio
  - Aplica calibraci√≥n
  - Genera archivos procesados
- **Salida:** CSV procesado en `processed/`

#### 2. **`ml_trigger`** (HTTP Trigger)

- **Ruta:** `/api/ml_trigger` o `/api/ml`
- **M√©todo:** POST
- **Autenticaci√≥n:** Function key
- **Funci√≥n:** Clasifica y valida datos procesados
- **Procesos:**
  - Lee archivos desde `processed/`
  - Aplica modelos de Machine Learning
  - Clasifica y categoriza datos
  - Valida calidad y detecta anomal√≠as
  - Genera archivos validados
- **Salida:** CSV validado en `validated/`

#### 3. **`compilador_trigger`** (HTTP Trigger)

- **Ruta:** `/api/compilador_trigger` o `/api/compilador`
- **M√©todo:** POST
- **Autenticaci√≥n:** Function key
- **Funci√≥n:** Consolida archivos validados en dataset hist√≥rico √∫nico
- **Procesos:**
  - Lee todos los CSV validados desde `validated/`
  - Concatena archivos en un DataFrame √∫nico
  - Elimina duplicados por `ID_tachada` (mantiene primera ocurrencia)
  - Agrega columna `archivo_origen` para trazabilidad
  - Genera/actualiza `df_historico.csv`
- **Salida:** `df_historico.csv` en `validated/`

#### 4. **`reporte_trigger`** (HTTP Trigger)

- **Ruta:** `/api/reporte_trigger` o `/api/reporte`
- **M√©todo:** POST
- **Autenticaci√≥n:** Function key
- **Funci√≥n:** Genera reportes HTML con m√©tricas y gr√°ficos
- **Procesos:**
  - Descarga `df_historico.csv` desde `validated/`
  - Calcula m√©tricas estad√≠sticas
  - Genera gr√°ficos y visualizaciones embebidos (base64)
  - Crea reporte HTML completo
- **Salida:** Reporte HTML en carpeta `reportes/`

### Configuraci√≥n de Azure Function App

**Requirements:**
- Python 3.11
- Dependencias en `requirements.txt`
- Variables de entorno configuradas

**Deployment:**
- C√≥digo en carpeta `shared_code/` se incluye autom√°ticamente
- Configuraci√≥n en `host.json`
- Bindings en `{function}/function.json`

### Costo Operativo

El sistema corre sobre un **Consumption Plan** de Azure Functions, con funciones ligeras y de corta duraci√≥n. El uso combinado de Azure (Functions + Storage) y Google Drive se mantiene dentro de los niveles gratuitos, por lo que el **costo operativo es 0** siempre que se respeten los vol√∫menes actuales de procesamiento.

---

## üìä Estructura de Datos

### Formato de Entrada (JPV)

```txt
Time	VarName	VarValue
2025-11-19 10:00:00	V_HUM	45.2
2025-11-19 10:00:00	TEMP	25.3
2025-11-19 10:01:00	V_HUM	45.5
2025-11-19 10:01:00	TEMP	25.4
```

### Formato de Entrada (RB)

**Formato 1 (separador `,`):**
```csv
Date,LOC_time,Record,V_HUM,TEMP,PRES
2025-11-19,10:00:00,1,4520,2380,1013.2
2025-11-19,10:01:00,2,4550,2390,1013.3
```

**Formato 2 (separador `;`):**
```csv
Date;Time;V_Hum;V_Tem
2025-11-19;10:00:00;4520;2380
2025-11-19;10:01:00;4550;2390
```

**Nota:** RB maneja m√∫ltiples variantes de nombres de columnas y separadores autom√°ticamente.

### Formato de Salida (CSV Procesado)

**Formato unificado para JPV y RB:**

```csv
timestamp,variable,valor,planta,sensor_id,source_file,Date_raw,LOC_time_raw,Variedad,ID_tachada,HumedadInicial,HumedadFinal,VOLT_HUM,VOLT_TEM,TEMPERATURA,HUMEDAD
2025-11-19T10:00:00Z,VOLT_HUM,2.45,JPV,1,archivo_001.txt,,,Variedad_A,123,25.5,13.2,2.45,3.12,25.3,15.8
2025-11-19T10:00:00Z,VOLT_TEM,3.12,JPV,1,archivo_001.txt,,,Variedad_A,123,25.5,13.2,2.45,3.12,25.3,15.8
2025-11-19T10:00:00Z,VOLT_HUM,45.20,RB,1,SENSOR1_RB.csv,2025-11-19,10:00:00,Variedad_A,123,25.5,13.2,45.20,31.20,25.3,15.8
2025-11-19T10:00:00Z,VOLT_TEM,31.20,RB,1,SENSOR1_RB.csv,2025-11-19,10:00:00,Variedad_A,123,25.5,13.2,45.20,31.20,25.3,15.8
```

**Nota:** Los valores de `variable` son siempre `VOLT_HUM` o `VOLT_TEM` para ambas plantas, estandarizado para el pipeline ML.

### Columnas del CSV Procesado

| Columna | Descripci√≥n | Ejemplo |
|---------|-------------|---------|
| `timestamp` | Fecha/hora en UTC (ISO 8601) | `2025-11-19T10:00:00Z` |
| `variable` | Nombre de la variable normalizado | `VOLT_HUM`, `VOLT_TEM` (estandarizado para ambas plantas) |
| `valor` | Valor num√©rico del voltaje | `2.45`, `3.12` (RB ya dividido por 100) |
| `Date_raw` | Fecha cruda (RB) o `None` (JPV) | `2025-11-19` o vac√≠o |
| `LOC_time_raw` | Hora cruda (RB) o `None` (JPV) | `10:00:00` o vac√≠o |
| `planta` | Planta de origen | `JPV`, `RB` |
| `sensor_id` | ID del sensor | `1`, `2`, `3` |
| `source_file` | Archivo de origen | `archivo_001.txt` |
| `Variedad` | Variedad de arroz (del laboratorio) | `Variedad_A` |
| `ID_tachada` | ID de la tachada (del laboratorio) | `123` |
| `HumedadInicial` | Humedad inicial de laboratorio | `25.5` |
| `HumedadFinal` | Humedad final de laboratorio | `13.2` |
| `VOLT_HUM` | Voltaje de humedad (raw) | `2.45` |
| `VOLT_TEM` | Voltaje de temperatura (raw) | `3.12` |
| `TEMPERATURA` | Temperatura calibrada (¬∞C) | `25.3` |
| `HUMEDAD` | Humedad calibrada (%) | `15.8` |

### Formato de Salida (CSV Validado)

```csv
timestamp,variable,valor,planta,sensor_id,Variedad,ID_tachada,TEMPERATURA,HUMEDAD,categoria,confianza,valido,prediccion,anomalia
2025-11-19T10:00:00Z,TEMPERATURA,25.3,JPV,1,Variedad_A,123,25.3,15.8,normal,0.95,True,0,False
2025-11-19T10:01:00Z,HUMEDAD,15.9,JPV,1,Variedad_B,124,25.4,15.9,normal,0.92,True,0,False
```

### Columnas Adicionales en CSV Validado

| Columna | Descripci√≥n | Ejemplo |
|---------|-------------|---------|
| `categoria` | Categor√≠a asignada por modelo ML | `normal`, `anomalia`, `outlier` |
| `confianza` | Nivel de confianza de clasificaci√≥n (0-1) | `0.95` |
| `valido` | Indicador de validaci√≥n | `True`, `False` |
| `prediccion` | Predicci√≥n binaria (0/1) | `0`, `1` |
| `anomalia` | Indicador de detecci√≥n de anomal√≠as | `True`, `False` |

### Formato de Salida (Dataset Hist√≥rico - df_historico.csv)

El archivo `df_historico.csv` es el resultado de la consolidaci√≥n de todos los archivos validados. Contiene datos agregados por tachada.

**Estructura principal:**
```csv
ID_tachada,planta,secadora,Variedad,fecha_inicio,fecha_fin,duracion_hs,TEMPERATURA_max,TEMPERATURA_mean,HUMEDAD_max,HUMEDAD_mean,HumedadInicial,HumedadFinal,...
123,JPV,Secadora 1,Variedad_A,2025-11-19 10:00:00,2025-11-20 14:00:00,28.5,45.2,42.1,18.5,15.8,25.5,13.2,...
124,RB,Secadora 1,Variedad_B,2025-11-19 11:00:00,2025-11-20 16:00:00,29.0,46.1,43.2,19.1,16.2,26.0,13.8,...
```

**Columnas principales:**
- `ID_tachada`: Identificador √∫nico de la tachada
- `planta`: Planta de origen (JPV o RB)
- `secadora`: Nombre de la secadora (sensor_id)
- `Variedad`: Variedad de arroz
- `fecha_inicio`, `fecha_fin`: Rango temporal de la tachada
- `duracion_hs`: Duraci√≥n en horas
- `TEMPERATURA_max`, `TEMPERATURA_mean`: Estad√≠sticas de temperatura
- `HUMEDAD_max`, `HUMEDAD_mean`: Estad√≠sticas de humedad
- `HumedadInicial`, `HumedadFinal`: Valores de laboratorio
- Otras columnas agregadas por el proceso ML

**Ubicaci√≥n:** `validated/df_historico.csv` (misma carpeta que los archivos validados)

**Nota:** Este formato es el que consume `reporte_builder.py` para generar los reportes HTML. El sistema es robusto y detecta din√°micamente las columnas disponibles, usando fallbacks cuando algunas columnas no existen.

---

## üîç Casos de Uso

### Caso 1: Procesamiento Incremental

**Escenario:** M√∫ltiples archivos se suben a Google Drive en un corto per√≠odo.

**Comportamiento:**
1. Google Apps Script detecta cada archivo y dispara la funci√≥n
2. Primera ejecuci√≥n procesa el archivo y guarda timestamp
3. Ejecuciones subsecuentes leen timestamp y procesan solo archivos nuevos
4. Si m√∫ltiples archivos est√°n en la misma carpeta, se procesan todos en una ejecuci√≥n

### Caso 2: Primera Ejecuci√≥n

**Escenario:** Primera vez que se ejecuta para una planta-secadora.

**Comportamiento:**
1. No existe timestamp ‚Üí procesa TODOS los archivos en la carpeta
2. Guarda timestamp con fecha/hora actual
3. Pr√≥ximas ejecuciones solo procesar√°n archivos nuevos

### Caso 3: Error en un Archivo

**Escenario:** Un archivo tiene formato incorrecto o est√° corrupto.

**Comportamiento:**
1. Detecta error al procesar el archivo
2. Registra error en logs y respuesta JSON
3. Contin√∫a procesando los dem√°s archivos
4. Timestamp se actualiza con archivos procesados exitosamente

### Caso 4: Generaci√≥n de Reporte

**Escenario:** Usuario solicita reporte desde interfaz de Apps Script.

**Comportamiento:**
1. Apps Script llama a `compilador_trigger` con par√°metro `planta`
2. `compilador_trigger` consolida todos los CSV validados en `df_historico.csv`
3. Al completarse exitosamente, Apps Script llama a `reporte_trigger` con par√°metro `planta`
4. `reporte_trigger` genera reporte HTML con m√©tricas y gr√°ficos
5. Apps Script descarga el reporte y env√≠a correo con PDF adjunto

---

## üìù Logging y Monitoreo

### Niveles de Logging

- **INFO**: Operaciones normales (descarga, procesamiento, subida)
- **WARNING**: Situaciones no cr√≠ticas (archivo de laboratorio no encontrado)
- **ERROR**: Errores que no detienen el procesamiento
- **EXCEPTION**: Errores cr√≠ticos con stack trace

### Informaci√≥n Registrada

- Metadata de archivos recibidos
- Timestamps de √∫ltima ejecuci√≥n
- Archivos procesados y sus resultados
- Errores y excepciones
- Estad√≠sticas de procesamiento (registros procesados, cruzados, etc.)

---

## üöÄ Pr√≥ximos Pasos y Mejoras

### Funcionalidades Implementadas

- [x] ETL Trigger: Procesamiento completo con calibraci√≥n y cruce con laboratorio
- [x] ML Trigger: Clasificaci√≥n y validaci√≥n de datos con Machine Learning
- [x] Compilador Trigger: Consolidaci√≥n de archivos validados en dataset hist√≥rico
- [x] Reporte Trigger: Generaci√≥n de reportes HTML con m√©tricas y gr√°ficos
- [x] Sistema de calibraci√≥n: Conversi√≥n de voltajes a valores reales
- [x] Interfaz Google Apps Script: Carga de archivos y trigger de ETL
- [x] Interfaz Google Apps Script: Solicitud y distribuci√≥n de reportes
- [x] Sistema incremental de timestamps: Evita reprocesamiento

### Funcionalidades en Desarrollo

- [ ] Notificaciones por email en caso de errores
- [ ] Integraci√≥n con Power BI
- [ ] Dashboard de monitoreo en tiempo real
- [ ] API REST para consulta de datos consolidados

### Mejoras Potenciales

- [ ] Procesamiento paralelo de archivos
- [ ] Compresi√≥n de archivos procesados
- [ ] Cache de archivos de laboratorio y calibraci√≥n
- [ ] Retry autom√°tico en caso de errores temporales
- [ ] M√©tricas y telemetr√≠a detallada
- [ ] Versionado de modelos ML
- [ ] Reportes programados autom√°ticos

---

## üìö Referencias

- [Gu√≠a de Instalaci√≥n](GUIA_INSTALACION.md)
- [Configuraci√≥n de Google Cloud](CONFIGURACION_GOOGLE_CLOUD.md)
- [Configuraci√≥n de Azure](CONFIGURACION_AZURE.md)
- [Troubleshooting](TROUBLESHOOTING_AUTENTICACION.md)
- [Automatizaci√≥n de Deployment](AUTOMATIZACION_DEPLOYMENT.md)

---

**√öltima actualizaci√≥n:** Diciembre 2025  
**Versi√≥n del sistema:** 4.0 (ETL + ML + Compilador Hist√≥rico + Reportes - Arquitectura completa automatizada)

