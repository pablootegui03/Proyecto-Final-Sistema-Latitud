{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cd526d",
   "metadata": {},
   "source": [
    "# Instalaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb7ce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\augus\\anaconda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\augus\\anaconda\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\augus\\anaconda\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\augus\\anaconda\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\augus\\anaconda\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\augus\\anaconda\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\augus\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\augus\\anaconda\\lib\\site-packages (from python-dateutil) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas openpyxl xlsxwriter python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20a77b",
   "metadata": {},
   "source": [
    "# Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55eee3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import tz\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b2c9459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Construyendo inventario...\n",
      "Archivos detectados: 263 (SAMPLE=5)\n",
      "Inventario preliminar: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\inventario.xlsx\n",
      ">> Procesando archivos...\n",
      "Registros en largo: 36708\n",
      ">> Pivot a formato ANCHO (estilo RB)...\n",
      "Filas en datos_wide: 7034 | Columnas: 10\n",
      ">> Guardando Excel...\n",
      "Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\inventario.xlsx\n",
      "Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\ConsolidadoSensores.xlsx\n"
     ]
    }
   ],
   "source": [
    "# consolidar_sensores.py\n",
    "# Autor: Augusto (pipeline para Tesis - Secado de arroz)\n",
    "# Objetivo: Consolidar JPV (.txt) y RB (.csv) en formato ANCHO (estilo RB) preservando columnas crudas.\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "# Editá estas tres rutas a tu gusto (sugerencia: dejarlas dentro del mismo OneDrive del proyecto)\n",
    "BASE_JPV = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\"\n",
    "BASE_RB  = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\"\n",
    "\n",
    "# Modo muestra: si querés probar con pocos archivos, poner un número (ej. 6). None procesa todo.\n",
    "SAMPLE_MAX_FILES: Optional[int] = 5\n",
    "\n",
    "# Zona horaria (no forzamos tz-aware; la usamos si quisieras localize más adelante)\n",
    "TZ = \"America/Montevideo\"\n",
    "\n",
    "# Si True, exporta también los datos en “largo” (auditoría).\n",
    "EXPORT_LONG = True\n",
    "\n",
    "DROP_WIDE_COLS = [\"TimeString\", \"HUMEDAD\", \"OFFSET\", \"TEMPERATURA\"]\n",
    "RB_VOLT_SCALE = 0.01  # dividir voltajes de RB por 100 para equiparar con JPV\n",
    "\n",
    "\n",
    "# ============================\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------- Utilidades ---------\n",
    "def extract_sensor_id_from_name(name: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    JPV: SENSOR10,20,...,60 -> 1..6\n",
    "    RB:  SENSOR1, SENSOR2, etc.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"SENSOR\\s*(\\d+)\", name.upper())\n",
    "    if not m:\n",
    "        return None\n",
    "    num = int(m.group(1))\n",
    "    return num // 10 if num in (10, 20, 30, 40, 50, 60) else num\n",
    "\n",
    "def is_plain_sensor_folder(name: str) -> bool:\n",
    "    \"\"\"\n",
    "    True solo si la carpeta es exactamente SENSOR<1..6> (sin 'b' ni 'c').\n",
    "    Aplica solo a JPV.\n",
    "    \"\"\"\n",
    "    m = re.fullmatch(r\"SENSOR([1-6])\", name.upper())\n",
    "    return m is not None\n",
    "\n",
    "def parse_tirada_jpv(path: Path) -> Tuple[Optional[int], Optional[datetime]]:\n",
    "    \"\"\"\n",
    "    Busca en los segmentos del path:\n",
    "      - usb <número>   (sin importar espacios / mayúsculas)\n",
    "      - una fecha con separador . o - o / (dd.mm.aa[aa] o dd-mm-aaaa, etc.)\n",
    "    Tolera texto adicional en el mismo nombre de carpeta.\n",
    "    \"\"\"\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    usb_num = None\n",
    "    date_dt = None\n",
    "\n",
    "    for part in path.parts:\n",
    "        # número de USB\n",
    "        if usb_num is None:\n",
    "            mnum = re.search(r\"\\busb\\s*(\\d+)\", part, flags=re.IGNORECASE)\n",
    "            if mnum:\n",
    "                usb_num = int(mnum.group(1))\n",
    "\n",
    "        # fecha\n",
    "        if date_dt is None:\n",
    "            mdate = re.search(DATE_RX, part)\n",
    "            if mdate:\n",
    "                d, m, y = mdate.groups()\n",
    "                y = int(y)\n",
    "                if y < 100:  # años 2 dígitos\n",
    "                    y += 2000\n",
    "                try:\n",
    "                    date_dt = datetime(y, int(m), int(d))\n",
    "                except ValueError:\n",
    "                    pass  # por si cae una fecha inválida\n",
    "\n",
    "        if usb_num is not None and date_dt is not None:\n",
    "            break\n",
    "\n",
    "    return usb_num, date_dt\n",
    "\n",
    "\n",
    "def parse_tirada_rb(path: Path) -> Optional[datetime]:\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    for part in path.parts[::-1]:  # de más profundo a más alto\n",
    "        mdate = re.search(DATE_RX, part)\n",
    "        if mdate:\n",
    "            d, m, y = mdate.groups()\n",
    "            y = int(y)\n",
    "            if y < 100:\n",
    "                y += 2000\n",
    "            try:\n",
    "                return datetime(y, int(m), int(d))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_jpv_txt(path: Path) -> pd.DataFrame:\n",
    "    # Intentos de lectura (UTF-16 comunes en estos exportes)\n",
    "    tried = [( \"utf-16\", \"\\t\"), (\"utf-16le\", \"\\t\"), (\"utf-8\", \"\\t\")]\n",
    "    last_err = None\n",
    "    for enc, sep in tried:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, encoding=enc, engine=\"python\", on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            df = None\n",
    "    if df is None:\n",
    "        raise last_err or RuntimeError(f\"No se pudo leer {path}\")\n",
    "\n",
    "    # Filtrar metadatos ($RT_*)\n",
    "    if \"VarName\" in df.columns:\n",
    "        df = df[~df[\"VarName\"].astype(str).str.startswith(\"$RT_\")]\n",
    "\n",
    "    # Selección mínima, preservando trazabilidad\n",
    "    keep_cols = [c for c in [\"VarName\", \"TimeString\", \"VarValue\", \"Validity\", \"Time_ms\"] if c in df.columns]\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    # Timestamp desde TimeString (no forcemos dayfirst aquí; viene bien formado)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df.get(\"TimeString\", pd.Series(dtype=str)), errors=\"coerce\")\n",
    "\n",
    "    # Variable normalizada y original\n",
    "    if \"VarName\" in df.columns:\n",
    "        df[\"VarName_original\"] = df[\"VarName\"].astype(str)\n",
    "        df[\"variable\"] = df[\"VarName\"].astype(str).str.replace(r\"^\\d+_\", \"\", regex=True)\n",
    "    else:\n",
    "        df[\"VarName_original\"] = np.nan\n",
    "        df[\"variable\"] = \"Var\"\n",
    "\n",
    "    # VarValue -> número (coma decimal → punto)\n",
    "    if \"VarValue\" in df.columns:\n",
    "        s = df[\"VarValue\"].astype(str).str.strip()\n",
    "        # Cambiamos coma por punto solo si parece número con coma\n",
    "        s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "        df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"valor\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    # normaliza: mayúsculas, sin espacios/guiones/puntos/paréntesis\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)]\", \"\", s.upper())\n",
    "\n",
    "def read_rb_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, engine=\"python\")\n",
    "\n",
    "    # detectar columnas de fecha/hora\n",
    "    date_col = None\n",
    "    lt_col = None\n",
    "    for c in df.columns:\n",
    "        cc = _canon(c)\n",
    "        if cc in (\"DATE\",\"FECHA\"):\n",
    "            date_col = c\n",
    "        if cc in (\"LOCTIME\",\"LOCTIEMPO\",\"LOCALTIME\"):\n",
    "            lt_col = c\n",
    "    # timestamp\n",
    "    if date_col and lt_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col].astype(str) + \" \" + df[lt_col].astype(str),\n",
    "                                         dayfirst=False, errors=\"coerce\")\n",
    "    elif date_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"timestamp\"] = pd.NaT\n",
    "\n",
    "    # detectar variables RB (V_HUM / V_TEM con variantes)\n",
    "    value_cols_map = {}\n",
    "    for c in df.columns:\n",
    "        cc = _canon(c)\n",
    "        if cc in (\"V_HUM\",\"VHUM\"):\n",
    "            value_cols_map[c] = \"V_HUM\"\n",
    "        elif cc in (\"V_TEM\",\"VTEM\",\"V_TEMP\",\"VTEMP\"):\n",
    "            value_cols_map[c] = \"V_TEM\"\n",
    "\n",
    "    value_cols = list(value_cols_map.keys())\n",
    "    # si no encontramos nada, devolvemos vacío (que luego no rompe)\n",
    "    if not value_cols:\n",
    "        # construir frame vacío con id_cols\n",
    "        id_cols = []\n",
    "        if \"Record\" in df.columns: id_cols.append(\"Record\")\n",
    "        if date_col: id_cols.append(date_col)\n",
    "        if lt_col:   id_cols.append(lt_col)\n",
    "        id_cols.append(\"timestamp\")\n",
    "        out = df[id_cols].copy()\n",
    "        out[\"variable\"] = np.nan\n",
    "        out[\"valor\"] = np.nan\n",
    "        if date_col: out.rename(columns={date_col: \"Date_raw\"}, inplace=True)\n",
    "        if lt_col:   out.rename(columns={lt_col: \"LOC_time_raw\"}, inplace=True)\n",
    "        return out[[\"Record\",\"Date_raw\",\"LOC_time_raw\",\"timestamp\",\"variable\",\"valor\"]]\n",
    "\n",
    "    # melt\n",
    "    id_cols = []\n",
    "    if \"Record\" in df.columns: id_cols.append(\"Record\")\n",
    "    if date_col: id_cols.append(date_col)\n",
    "    if lt_col:   id_cols.append(lt_col)\n",
    "    id_cols.append(\"timestamp\")\n",
    "\n",
    "    long_df = df[id_cols + value_cols].melt(id_vars=id_cols, var_name=\"variable_raw\", value_name=\"valor_raw\")\n",
    "    # variable canónica\n",
    "    long_df[\"variable\"] = long_df[\"variable_raw\"].map({c: value_cols_map[c] for c in value_cols})\n",
    "    # Date/LOC raw para trazabilidad\n",
    "    if date_col: long_df.rename(columns={date_col: \"Date_raw\"}, inplace=True)\n",
    "    if lt_col:   long_df.rename(columns={lt_col: \"LOC_time_raw\"}, inplace=True)\n",
    "\n",
    "    # valor a num (coma→punto si corresponde)\n",
    "    s = long_df[\"valor_raw\"].astype(str).str.strip()\n",
    "    s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "    long_df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    return long_df[[\"Record\",\"Date_raw\",\"LOC_time_raw\",\"timestamp\",\"variable\",\"valor\"]]\n",
    "\n",
    "def build_inventory() -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    # --- JPV ---\n",
    "    for root, dirs, files in os.walk(BASE_JPV):\n",
    "        root_p = Path(root)\n",
    "        # excluir carpetas SENSOR#b y SENSOR#c (solo JPV)\n",
    "        parent_name = root_p.name\n",
    "        if parent_name.upper().startswith(\"SENSOR\") and not is_plain_sensor_folder(parent_name):\n",
    "            continue\n",
    "\n",
    "        for f in files:\n",
    "            if not f.lower().endswith(\".txt\"): \n",
    "                continue\n",
    "            # aceptar solo archivos bajo una carpeta SENSOR<1..6>\n",
    "            if not is_plain_sensor_folder(root_p.name):\n",
    "                continue\n",
    "            # meta\n",
    "            p = root_p / f\n",
    "            planta = \"JPV\"\n",
    "            sensor_id = extract_sensor_id_from_name(f) or extract_sensor_id_from_name(root_p.name)\n",
    "            tirada_num, tirada_dt = parse_tirada_jpv(root_p)\n",
    "            año = None\n",
    "            # buscar el '2024 Datos Sensores JPV' en path para inferir año si aplica\n",
    "            m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+JPV\", str(p), flags=re.IGNORECASE)\n",
    "            if m: año = int(m.group(1))\n",
    "            rows.append({\n",
    "                \"planta\": planta, \"año\": año,\n",
    "                \"tirada_num\": tirada_num, \"tirada_fecha\": tirada_dt,\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"ext\": p.suffix.lower(), \"source_file\": f, \"source_path\": str(p)\n",
    "            })\n",
    "\n",
    "    # --- RB ---\n",
    "    for root, dirs, files in os.walk(BASE_RB):\n",
    "        root_p = Path(root)\n",
    "        for f in files:\n",
    "            if not f.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            p = root_p / f\n",
    "            planta = \"RB\"\n",
    "            sensor_id = extract_sensor_id_from_name(f) or extract_sensor_id_from_name(root_p.name)\n",
    "            tirada_dt = parse_tirada_rb(root_p)\n",
    "            # año desde '2024 Datos Sensores RB' si aparece\n",
    "            año = None\n",
    "            m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+RB\", str(p), flags=re.IGNORECASE)\n",
    "            if m: año = int(m.group(1))\n",
    "            rows.append({\n",
    "                \"planta\": planta, \"año\": año,\n",
    "                \"tirada_num\": None, \"tirada_fecha\": tirada_dt,\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"ext\": p.suffix.lower(), \"source_file\": f, \"source_path\": str(p)\n",
    "            })\n",
    "\n",
    "    inv = pd.DataFrame(rows)\n",
    "    if len(inv):\n",
    "        inv.sort_values([\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"source_file\"], inplace=True, ignore_index=True)\n",
    "    return inv\n",
    "\n",
    "def process_files(inv: pd.DataFrame, sample_strategy: str = \"balanced\") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Devuelve: (long_all, log_omisiones, qa_resumen)\n",
    "\n",
    "    sample_strategy:\n",
    "      - \"balanced\": mitad JPV / mitad RB (fallback con el resto)\n",
    "      - \"by_group\": 1 por grupo (planta, año, sensor) y luego completa hasta N\n",
    "      - \"head\":     simplemente los primeros N\n",
    "    \"\"\"\n",
    "    long_frames: List[pd.DataFrame] = []\n",
    "    log_rows: List[dict] = []\n",
    "\n",
    "    # ---------- Selección de muestra ----------\n",
    "    files_to_process = inv.copy()\n",
    "    if SAMPLE_MAX_FILES:\n",
    "        n = int(SAMPLE_MAX_FILES)\n",
    "\n",
    "        if sample_strategy == \"balanced\":\n",
    "            jpv = inv[inv[\"planta\"] == \"JPV\"]\n",
    "            rb  = inv[inv[\"planta\"] == \"RB\"]\n",
    "\n",
    "            per_jpv = n // 2\n",
    "            per_rb  = n - per_jpv\n",
    "\n",
    "            sel = pd.concat([jpv.head(per_jpv), rb.head(per_rb)], ignore_index=True)\n",
    "\n",
    "            # fallback si a una planta le faltan archivos\n",
    "            if len(sel) < n:\n",
    "                faltan = n - len(sel)\n",
    "                resto = inv.drop(sel.index).head(faltan)\n",
    "                sel = pd.concat([sel, resto], ignore_index=True)\n",
    "\n",
    "            files_to_process = sel\n",
    "\n",
    "        elif sample_strategy == \"by_group\":\n",
    "            base = (inv.sort_values([\"planta\",\"año\",\"sensor_id\",\"tirada_fecha\",\"source_file\"])\n",
    "                      .groupby([\"planta\",\"año\",\"sensor_id\"], group_keys=False)\n",
    "                      .head(1))\n",
    "\n",
    "            if len(base) < n:\n",
    "                resto = inv.drop(base.index)\n",
    "                base  = pd.concat([base, resto.head(n - len(base))], ignore_index=True)\n",
    "            else:\n",
    "                base  = base.head(n).reset_index(drop=True)\n",
    "\n",
    "            files_to_process = base\n",
    "\n",
    "        else:  # \"head\"\n",
    "            files_to_process = files_to_process.head(n)\n",
    "\n",
    "    # ---------- Procesamiento archivo x archivo ----------\n",
    "    for _, r in files_to_process.iterrows():\n",
    "        p = Path(r[\"source_path\"])\n",
    "        try:\n",
    "            if r[\"planta\"] == \"JPV\":\n",
    "                df = read_jpv_txt(p)\n",
    "            else:\n",
    "                df = read_rb_csv(p)\n",
    "\n",
    "            # columnas meta comunes\n",
    "            df[\"planta\"] = r[\"planta\"]\n",
    "            df[\"año\"] = r[\"año\"]\n",
    "            df[\"tirada_num\"] = r[\"tirada_num\"]\n",
    "            df[\"tirada_fecha\"] = pd.to_datetime(r[\"tirada_fecha\"]) if pd.notna(r[\"tirada_fecha\"]) else pd.NaT\n",
    "            df[\"sensor_id\"] = r[\"sensor_id\"]\n",
    "            df[\"source_file\"] = r[\"source_file\"]\n",
    "            df[\"source_path\"] = r[\"source_path\"]\n",
    "\n",
    "            # armonizar columnas crudas (por si no existen)\n",
    "            for col in [\"Date_raw\",\"LOC_time_raw\",\"VarName\",\"TimeString\",\"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.Series([np.nan]*len(df))\n",
    "\n",
    "            # clave de duplicado en largo\n",
    "            df[\"dup_key\"] = (\n",
    "                df[\"planta\"].astype(str) + \"|\" +\n",
    "                df[\"sensor_id\"].astype(str) + \"|\" +\n",
    "                df[\"timestamp\"].astype(str) + \"|\" +\n",
    "                df[\"variable\"].astype(str)\n",
    "            )\n",
    "\n",
    "            # detectar duplicados exactos\n",
    "            dups = df[df.duplicated(subset=[\"dup_key\"], keep=\"first\")]\n",
    "            if len(dups):\n",
    "                for _, dd in dups.iterrows():\n",
    "                    log_rows.append({\n",
    "                        \"tipo\": \"duplicado_largo\",\n",
    "                        \"planta\": r[\"planta\"], \"sensor_id\": r[\"sensor_id\"],\n",
    "                        \"timestamp\": dd[\"timestamp\"], \"variable\": dd[\"variable\"],\n",
    "                        \"source_file\": r[\"source_file\"], \"source_path\": r[\"source_path\"]\n",
    "                    })\n",
    "                # nos quedamos con la primera\n",
    "                df = df.drop_duplicates(subset=[\"dup_key\"], keep=\"first\")\n",
    "\n",
    "            long_frames.append(df.drop(columns=[\"dup_key\"]))\n",
    "\n",
    "        except Exception as e:\n",
    "            log_rows.append({\n",
    "                \"tipo\": \"error_lectura\",\n",
    "                \"planta\": r[\"planta\"], \"sensor_id\": r[\"sensor_id\"],\n",
    "                \"timestamp\": None, \"variable\": None,\n",
    "                \"source_file\": r[\"source_file\"], \"source_path\": r[\"source_path\"],\n",
    "                \"detalle\": str(e)\n",
    "            })\n",
    "\n",
    "    # ---------- Unión y QA ----------\n",
    "    long_all = pd.concat(long_frames, ignore_index=True) if long_frames else pd.DataFrame()\n",
    "\n",
    "    if not long_all.empty:\n",
    "        qa = (long_all\n",
    "                .groupby([\"planta\",\"año\",\"sensor_id\"])\n",
    "                .agg(registros=(\"valor\",\"size\"),\n",
    "                     fechas_min=(\"timestamp\",\"min\"),\n",
    "                     fechas_max=(\"timestamp\",\"max\"))\n",
    "                .reset_index())\n",
    "    else:\n",
    "        qa = pd.DataFrame(columns=[\"planta\",\"año\",\"sensor_id\",\"registros\",\"fechas_min\",\"fechas_max\"])\n",
    "\n",
    "    log_df = pd.DataFrame(log_rows, columns=[\"tipo\",\"planta\",\"sensor_id\",\"timestamp\",\"variable\",\"source_file\",\"source_path\",\"detalle\"])\n",
    "    return long_all, log_df, qa\n",
    "\n",
    "\n",
    "def to_wide(long_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ANCHO: unifica voltajes en VOLT_HUM / VOLT_TEM, escala RB, excluye HUMEDAD/TEMPERATURA/OFFSET/VARIEDAD\n",
    "    y evita columnas 'nan' en el pivot.\n",
    "    \"\"\"\n",
    "    if long_all.empty:\n",
    "        return long_all\n",
    "\n",
    "    # 1) Claves estables (sin tirada_num para no perder RB por NaN)\n",
    "    key_cols = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "    for c in key_cols:\n",
    "        if c not in long_all.columns:\n",
    "            long_all[c] = np.nan\n",
    "\n",
    "    # 2) Canonicalizar nombre de variable\n",
    "    svar = (long_all[\"variable\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.replace(r\"[\\s_\\.\\-]\", \"\", regex=True))\n",
    "\n",
    "    # Aliases que SÍ queremos (voltajes)\n",
    "    hum_aliases  = {\"VHUM\",\"VOLTHUM\",\"VOLTHUME\"}\n",
    "    tem_aliases  = {\"VTEM\",\"VOLTTEM\",\"VOLTTEMP\",\"VTEMP\"}\n",
    "\n",
    "    # Aliases que NO queremos (descartar de wide)\n",
    "    drop_aliases = {\"HUMEDAD\",\"TEMPERATURA\",\"OFFSET\",\"VARIEDAD\"}\n",
    "\n",
    "    mask_hum  = svar.isin(hum_aliases)\n",
    "    mask_tem  = svar.isin(tem_aliases)\n",
    "    mask_drop = svar.isin(drop_aliases)\n",
    "\n",
    "    # Nos quedamos SOLO con voltajes (descartamos HUMEDAD/TEMPERATURA/etc.)\n",
    "    keep_mask = (mask_hum | mask_tem) & (~mask_drop)\n",
    "    long_v = long_all[keep_mask].copy()\n",
    "\n",
    "    # 3) Nombre normalizado + escala RB\n",
    "    long_v[\"var_norm\"] = np.where(mask_hum[keep_mask], \"VOLT_HUM\", \"VOLT_TEM\")\n",
    "    scale = np.where(long_v[\"planta\"].eq(\"RB\"), RB_VOLT_SCALE, 1.0)\n",
    "    long_v[\"valor_norm\"] = pd.to_numeric(long_v[\"valor\"], errors=\"coerce\") * scale\n",
    "\n",
    "    # 4) Pivot SOLO con claves estables\n",
    "    wide = (long_v\n",
    "            .pivot_table(index=key_cols,\n",
    "                         columns=\"var_norm\",\n",
    "                         values=\"valor_norm\",\n",
    "                         aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "\n",
    "    # 5) Re-anexar crudos + tirada_num (primer no-NaN por grupo)\n",
    "    raw_keep = [c for c in [\"tirada_num\",\"Date_raw\",\"LOC_time_raw\",\"TimeString\"] if c in long_all.columns]\n",
    "    if raw_keep:\n",
    "        raw = (long_all[key_cols + raw_keep]\n",
    "               .groupby(key_cols, as_index=False)\n",
    "               .first())\n",
    "        wide = wide.merge(raw, on=key_cols, how=\"left\")\n",
    "\n",
    "    # 6) Orden de columnas\n",
    "    volt_cols = [c for c in [\"VOLT_HUM\",\"VOLT_TEM\"] if c in wide.columns]\n",
    "    other_vars = [c for c in wide.columns if c not in (key_cols + raw_keep + volt_cols)]\n",
    "    wide = wide[key_cols + raw_keep + volt_cols + other_vars]\n",
    "\n",
    "    # 7) Drop seguro de columnas 'nan' (por si quedara alguna etiqueta rara)\n",
    "    safe_cols = []\n",
    "    for c in wide.columns:\n",
    "        if isinstance(c, float) and pd.isna(c):\n",
    "            continue\n",
    "        if isinstance(c, str) and c.strip().lower() == \"nan\":\n",
    "            continue\n",
    "        safe_cols.append(c)\n",
    "    wide = wide[safe_cols]\n",
    "\n",
    "    # 8) Quitar columnas no deseadas configuradas\n",
    "    if \"DROP_WIDE_COLS\" in globals():\n",
    "        wide = wide.drop(columns=[c for c in DROP_WIDE_COLS if c in wide.columns], errors=\"ignore\")\n",
    "\n",
    "    return wide\n",
    "\n",
    "def save_outputs(inv: pd.DataFrame, long_all: pd.DataFrame, wide: pd.DataFrame,\n",
    "                 log_df: pd.DataFrame, qa: pd.DataFrame):\n",
    "    inv_path = Path(OUTPUT_DIR) / \"inventario.xlsx\"\n",
    "    out_path = Path(OUTPUT_DIR) / \"ConsolidadoSensores.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(inv_path, engine=\"xlsxwriter\") as w:\n",
    "        inv.to_excel(w, sheet_name=\"inventario\", index=False)\n",
    "\n",
    "    with pd.ExcelWriter(out_path, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "        wide.to_excel(w, sheet_name=\"datos_wide\", index=False)\n",
    "        if EXPORT_LONG:\n",
    "            # Guardamos versión “largo” para auditoría (acota columnas para tamaño)\n",
    "            cols_long = [\"planta\",\"año\",\"tirada_num\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "                         \"variable\",\"valor\",\"Record\",\"Date_raw\",\"LOC_time_raw\",\"VarName\",\"TimeString\",\n",
    "                         \"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\",\"source_file\",\"source_path\"]\n",
    "            cols_long = [c for c in cols_long if c in long_all.columns]\n",
    "            long_all[cols_long].to_excel(w, sheet_name=\"datos_long\", index=False)\n",
    "\n",
    "        # Diccionario\n",
    "        dicc = pd.DataFrame({\n",
    "            \"columna\": [\n",
    "                \"planta\",\"año\",\"tirada_num\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "                \"Date_raw\",\"LOC_time_raw\",\"TimeString\",\n",
    "                \"V_HUM\",\"V_TEM\",\"(otras variables pivotadas)\",\n",
    "                \"Record\",\"VarName\",\"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\",\n",
    "                \"source_file\",\"source_path\"\n",
    "            ],\n",
    "            \"descripcion\": [\n",
    "                \"Planta origen (JPV/RB)\",\"Año lógico extraído de la ruta\",\n",
    "                \"N° de tirada (JPV)\",\"Fecha de tirada\",\"ID de sensor (1–6)\",\n",
    "                \"Marca de tiempo unificada\",\n",
    "                \"Fecha cruda RB\",\"Hora local cruda RB\",\"Tiempo crudo JPV\",\n",
    "                \"Voltaje humedad\",\"Voltaje temperatura\",\"Otras señales según VarName/CSV\",\n",
    "                \"N° de registro RB\",\"Nombre de variable JPV original\",\"Valor JPV original\",\n",
    "                \"Bandera de validez JPV\",\"Tiempo en ms JPV\",\"VarName sin normalizar\",\n",
    "                \"Nombre archivo fuente\",\"Ruta completa del archivo fuente\"\n",
    "            ]\n",
    "        })\n",
    "        dicc.to_excel(w, sheet_name=\"diccionario\", index=False)\n",
    "\n",
    "        # Logs y QA\n",
    "        if not log_df.empty:\n",
    "            log_df.to_excel(w, sheet_name=\"log_omisiones\", index=False)\n",
    "        qa.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "\n",
    "    print(f\"Inventario: {inv_path}\")\n",
    "    print(f\"Consolidado: {out_path}\")\n",
    "\n",
    "def main():\n",
    "    print(\">> Construyendo inventario...\")\n",
    "    inv = build_inventory()\n",
    "    if inv.empty:\n",
    "        print(\"No se encontraron archivos. Revisá las rutas BASE_JPV y BASE_RB.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Archivos detectados: {len(inv)} (SAMPLE={SAMPLE_MAX_FILES})\")\n",
    "    inv_path = Path(OUTPUT_DIR) / \"inventario.xlsx\"\n",
    "    with pd.ExcelWriter(inv_path, engine=\"xlsxwriter\") as w:\n",
    "        inv.to_excel(w, sheet_name=\"inventario\", index=False)\n",
    "    print(f\"Inventario preliminar: {inv_path}\")\n",
    "\n",
    "    print(\">> Procesando archivos...\")\n",
    "    long_all, log_df, qa = process_files(inv)\n",
    "\n",
    "    print(f\"Registros en largo: {len(long_all)}\")\n",
    "    print(\">> Pivot a formato ANCHO (estilo RB)...\")\n",
    "    wide = to_wide(long_all)\n",
    "\n",
    "    print(f\"Filas en datos_wide: {len(wide)} | Columnas: {len(wide.columns)}\")\n",
    "    print(\">> Guardando Excel...\")\n",
    "    save_outputs(inv, long_all, wide, log_df, qa)\n",
    "\n",
    "# --- EJECUCIÓN AUTOMÁTICA ---\n",
    "if __name__ == \"__main__\":\n",
    "        main()\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ce3ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filas long_all: 36708\n",
      "prop. valor no nulo: 1.0\n",
      "\n",
      "Top variables detectadas:\n",
      "HUMEDAD        5660\n",
      "TEMPERATURA    5660\n",
      "VOLT_HUME      5660\n",
      "VOLT_TEMP      5660\n",
      "OFFSET         5660\n",
      "VARIEDAD       5660\n",
      "V_HUM          1374\n",
      "V_TEM          1374\n",
      "Name: variable, dtype: int64\n",
      "\n",
      "wide -> filas: 7034  columnas: 10\n",
      "  planta   año tirada_fecha  sensor_id           timestamp  tirada_num  \\\n",
      "0    JPV  2024   2024-03-25          2 2024-03-15 13:44:35         1.0   \n",
      "1    JPV  2024   2024-03-25          2 2024-03-15 13:49:35         1.0   \n",
      "2    JPV  2024   2024-03-25          2 2024-03-15 13:54:35         1.0   \n",
      "\n",
      "  Date_raw LOC_time_raw  VOLT_HUM  VOLT_TEM  \n",
      "0     None         None       0.0       0.0  \n",
      "1     None         None       0.0       0.0  \n",
      "2     None         None       0.0       0.0  \n",
      "\n",
      "Filas con NaN en alguna clave estable: 0.07486106570774763\n"
     ]
    }
   ],
   "source": [
    "# Muestra chica equilibrada\n",
    "SAMPLE_MAX_FILES = 5\n",
    "\n",
    "inv = build_inventory()\n",
    "long_all, log_df, qa = process_files(inv, sample_strategy=\"balanced\")\n",
    "\n",
    "# Chequeos clave\n",
    "print(\"filas long_all:\", len(long_all))\n",
    "print(\"prop. valor no nulo:\", long_all[\"valor\"].notna().mean())\n",
    "\n",
    "print(\"\\nTop variables detectadas:\")\n",
    "print(long_all[\"variable\"].value_counts().head(10))\n",
    "\n",
    "# ¿Por qué podía quedar vacío el wide? probamos con to_wide corregido\n",
    "wide = to_wide(long_all)\n",
    "print(\"\\nwide -> filas:\", len(wide), \" columnas:\", len(wide.columns))\n",
    "print(wide.head(3))\n",
    "\n",
    "# Diagnóstico extra: ¿alguna clave estable con NaN?\n",
    "key_cols = [\"planta\",\"año\",\"tirada_num\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "print(\"\\nFilas con NaN en alguna clave estable:\",\n",
    "      long_all[key_cols].isna().any(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a79abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filas wide: 7034\n",
      "por planta en wide:\n",
      " JPV    5660\n",
      "RB     1374\n",
      "Name: planta, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Chequeo archivo datos_wide\n",
    "\n",
    "SAMPLE_MAX_FILES = 5\n",
    "inv = build_inventory()\n",
    "long_all, log_df, qa = process_files(inv, sample_strategy=\"balanced\")\n",
    "wide = to_wide(long_all)\n",
    "\n",
    "print(\"filas wide:\", len(wide))\n",
    "print(\"por planta en wide:\\n\", wide[\"planta\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa49b03",
   "metadata": {},
   "source": [
    "# Chequeo de Consolidación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f6f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\inventario.xlsx\n",
      "Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\ConsolidadoSensores.xlsx\n"
     ]
    }
   ],
   "source": [
    "inv = build_inventory()\n",
    "long_all, log_df, qa = process_files(inv)\n",
    "wide = to_wide(long_all)\n",
    "save_outputs(inv, long_all, wide, log_df, qa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757378d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  planta  anio  tirada_num tirada_fecha  sensor_id   ext   source_file  \\\n",
       " 0    JPV  2024         1.0   2024-03-25          1  .txt  SENSOR10.txt   \n",
       " 1    JPV  2024         1.0   2024-03-25          2  .txt  SENSOR20.txt   \n",
       " 2    JPV  2024         1.0   2024-03-25          3  .txt  SENSOR30.txt   \n",
       " \n",
       "                                          source_path  \n",
       " 0  C:\\Users\\augus\\OneDrive - Universidad de Monte...  \n",
       " 1  C:\\Users\\augus\\OneDrive - Universidad de Monte...  \n",
       " 2  C:\\Users\\augus\\OneDrive - Universidad de Monte...  ,\n",
       " JPV    138\n",
       " RB     126\n",
       " Name: planta, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿La muestra de 10 archivos incluye JPV y RB?\n",
    "inv.head(3), inv[\"planta\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8545882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿Se excluyeron SENSORb/SENSORc en JPV?\n",
    "bad = inv[(inv[\"planta\"]==\"JPV\") & inv[\"source_path\"].str.contains(r\"SENSOR\\d+[bc]\", case=False, regex=True)]\n",
    "bad.shape  # debería ser (0, ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f68a4d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeString    0.0\n",
       "VarName       0.0\n",
       "VarValue      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿Columnas “crudas” preservadas según la planta?\n",
    "# RB debe tener Date_raw/LOC_time_raw con datos en datos_long\n",
    "rb_long = long_all[long_all[\"planta\"]==\"RB\"]\n",
    "rb_long[[\"Date_raw\",\"LOC_time_raw\"]].isna().mean()\n",
    "\n",
    "# JPV debe tener TimeString/VarName/VarValue con datos en datos_long\n",
    "jpv_long = long_all[long_all[\"planta\"]==\"JPV\"]\n",
    "jpv_long[[\"TimeString\",\"VarName\",\"VarValue\"]].isna().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d020fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿Formato final ancho ok (V_HUM, V_TEM en columnas)?\n",
    "[c for c in wide.columns if c.startswith(\"V_\")][:10]  # debería listar ['V_HUM','V_TEM'] si estaban en la muestra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0c143b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿Mapeo de sensor JPV SENSOR10 → 1, etc.?\n",
    "tmp = inv[(inv[\"planta\"]==\"JPV\")].copy()\n",
    "tmp[\"ok\"] = tmp.apply(lambda r: ((\"SENSOR10\" in r[\"source_file\"] and r[\"sensor_id\"]==1) or\n",
    "                                 (\"SENSOR20\" in r[\"source_file\"] and r[\"sensor_id\"]==2) or\n",
    "                                 (\"SENSOR30\" in r[\"source_file\"] and r[\"sensor_id\"]==3) or\n",
    "                                 (\"SENSOR40\" in r[\"source_file\"] and r[\"sensor_id\"]==4) or\n",
    "                                 (\"SENSOR50\" in r[\"source_file\"] and r[\"sensor_id\"]==5) or\n",
    "                                 (\"SENSOR60\" in r[\"source_file\"] and r[\"sensor_id\"]==6)), axis=1)\n",
    "tmp[\"ok\"].all()  # debería ser True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a332fa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "planta\n",
       "JPV    1.0\n",
       "Name: timestamp, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) ¿Timestamps construidos correctamente?\n",
    "# proporción de timestamps no nulos por planta\n",
    "long_all.groupby(\"planta\")[\"timestamp\"].apply(lambda s: s.notna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3217741a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: tipo, dtype: int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) ¿Duplicados tratados?\n",
    "log_df[\"tipo\"].value_counts()\n",
    "#Si hay duplicado_largo, quedaron registrados y en datos_long se dejó la primera aparición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "725900c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planta</th>\n",
       "      <th>anio</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>registros</th>\n",
       "      <th>fechas_min</th>\n",
       "      <th>fechas_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>36966</td>\n",
       "      <td>2023-05-20 13:50:34</td>\n",
       "      <td>2024-03-29 08:59:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>36972</td>\n",
       "      <td>2023-05-20 13:50:34</td>\n",
       "      <td>2024-03-29 08:59:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>36972</td>\n",
       "      <td>2023-05-20 13:50:34</td>\n",
       "      <td>2024-03-29 08:59:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>36972</td>\n",
       "      <td>2023-05-20 13:50:34</td>\n",
       "      <td>2024-03-29 08:59:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>16980</td>\n",
       "      <td>2024-03-15 13:44:35</td>\n",
       "      <td>2024-03-25 09:49:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JPV</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>16980</td>\n",
       "      <td>2024-03-15 13:44:35</td>\n",
       "      <td>2024-03-25 09:49:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  planta  anio  sensor_id  registros          fechas_min          fechas_max\n",
       "0    JPV  2024          1      36966 2023-05-20 13:50:34 2024-03-29 08:59:14\n",
       "1    JPV  2024          2      36972 2023-05-20 13:50:34 2024-03-29 08:59:14\n",
       "2    JPV  2024          3      36972 2023-05-20 13:50:34 2024-03-29 08:59:14\n",
       "3    JPV  2024          4      36972 2023-05-20 13:50:34 2024-03-29 08:59:14\n",
       "4    JPV  2024          5      16980 2024-03-15 13:44:35 2024-03-25 09:49:05\n",
       "5    JPV  2024          6      16980 2024-03-15 13:44:35 2024-03-25 09:49:05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿Rangos de tiempo sensatos por sensor (QA)?\n",
    "qa.sort_values([\"planta\",\"sensor_id\"]).head(12)\n",
    "\n",
    "# Revisar fechas_min y fechas_max por sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55135023",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlong_all\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39mmean()       \u001b[38;5;66;03m# debería ser > 0\u001b[39;00m\n\u001b[0;32m      2\u001b[0m long_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'long_all' is not defined"
     ]
    }
   ],
   "source": [
    "long_all[\"valor\"].notna().mean()       # debería ser > 0\n",
    "long_all[\"variable\"].value_counts().head()  # debería listar variables como V_HUM, V_TEM o VarName de JPV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86e43c",
   "metadata": {},
   "source": [
    "# SOLO RB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a5282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos RB detectados: 126\n",
      "Registros RB en largo: 318034\n",
      "Filas en datos_wide (RB): 159002 | Columnas: 9\n",
      "Inventario RB: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_RB\\inventario_RB.xlsx\n",
      "Consolidado RB: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_RB\\ConsolidadoSensores_RB.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === SOLO RB (autosuficiente) ===\n",
    "\n",
    "BASE_RB  = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_RB\"\n",
    "RB_VOLT_SCALE = 0.01\n",
    "SAMPLE_MAX_FILES = 5   # o un número chico p/ probar\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)_]\", \"\", str(s).upper())\n",
    "\n",
    "def extract_sensor_id_from_name(name: str):\n",
    "    m = re.search(r\"SENSOR\\s*(\\d+)\", str(name).upper())\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def parse_tirada_rb(path: Path):\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    for part in path.parts[::-1]:\n",
    "        mdate = re.search(DATE_RX, part)\n",
    "        if mdate:\n",
    "            d, m, y = mdate.groups()\n",
    "            y = int(y) + (2000 if int(y) < 100 else 0)\n",
    "            try:\n",
    "                return datetime(y, int(m), int(d))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def read_rb_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, engine=\"python\")\n",
    "    # detectar fecha/hora\n",
    "    date_col = next((c for c in df.columns if _canon(c) in (\"DATE\",\"FECHA\")), None)\n",
    "    lt_col   = next((c for c in df.columns if _canon(c) in (\"LOCTIME\",\"LOCTIEMPO\",\"LOCALTIME\")), None)\n",
    "    if date_col and lt_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col].astype(str)+\" \"+df[lt_col].astype(str), dayfirst=False, errors=\"coerce\")\n",
    "    elif date_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"timestamp\"] = pd.NaT\n",
    "\n",
    "    # map variables de RB\n",
    "    value_cols_map = {}\n",
    "    for c in df.columns:\n",
    "        cc = _canon(c)\n",
    "        if cc in (\"VHUM\", \"V_HUM\"):\n",
    "            value_cols_map[c] = \"V_HUM\"\n",
    "        elif cc in (\"VTEM\",\"V_TEM\",\"VTEMP\",\"V_TEMP\"):\n",
    "            value_cols_map[c] = \"V_TEM\"\n",
    "    value_cols = list(value_cols_map.keys())\n",
    "\n",
    "    id_cols = [\"timestamp\"]\n",
    "    if \"Record\" in df.columns: id_cols.insert(0, \"Record\")\n",
    "    if date_col: id_cols.insert(1, date_col)\n",
    "    if lt_col:   id_cols.insert(2, lt_col)\n",
    "\n",
    "    long_df = df[id_cols + value_cols].melt(id_vars=id_cols, var_name=\"variable_raw\", value_name=\"valor_raw\")\n",
    "    long_df[\"variable\"] = long_df[\"variable_raw\"].map(value_cols_map)\n",
    "    if date_col: long_df.rename(columns={date_col:\"Date_raw\"}, inplace=True)\n",
    "    if lt_col:   long_df.rename(columns={lt_col:\"LOC_time_raw\"}, inplace=True)\n",
    "\n",
    "    # valor num (coma → punto)\n",
    "    s = long_df[\"valor_raw\"].astype(str).str.strip()\n",
    "    s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "    long_df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # meta de path\n",
    "    long_df[\"planta\"] = \"RB\"\n",
    "    long_df[\"año\"] = None\n",
    "    m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+RB\", str(path), flags=re.IGNORECASE)\n",
    "    if m: long_df[\"año\"] = int(m.group(1))\n",
    "    long_df[\"tirada_fecha\"] = parse_tirada_rb(path.parent)\n",
    "    long_df[\"sensor_id\"] = extract_sensor_id_from_name(path.name) or extract_sensor_id_from_name(path.parent.name)\n",
    "    long_df[\"source_file\"] = path.name\n",
    "    long_df[\"source_path\"] = str(path)\n",
    "    return long_df\n",
    "\n",
    "# inventario RB\n",
    "# inventario RB\n",
    "rows = []\n",
    "for root, dirs, files in os.walk(BASE_RB):\n",
    "    # ⛔ omitir la carpeta \"Datos otro formato\" (no se desciende a ella)\n",
    "    dirs[:] = [d for d in dirs if _canon(d) != _canon(\"Datos otro formato\")]\n",
    "\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".csv\"):\n",
    "            p = Path(root)/f\n",
    "            rows.append({\"planta\":\"RB\",\"source_file\":f,\"source_path\":str(p)})\n",
    "\n",
    "inv_rb = pd.DataFrame(rows)\n",
    "print(\"Archivos RB detectados:\", len(inv_rb))\n",
    "\n",
    "if SAMPLE_MAX_FILES:\n",
    "    inv_rb = inv_rb.head(int(SAMPLE_MAX_FILES))\n",
    "\n",
    "# procesar archivos\n",
    "long_frames = []\n",
    "for _, r in inv_rb.iterrows():\n",
    "    try:\n",
    "        long_frames.append(read_rb_csv(Path(r[\"source_path\"])))\n",
    "    except Exception as e:\n",
    "        print(\"Error leyendo:\", r[\"source_path\"], e)\n",
    "\n",
    "long_rb = pd.concat(long_frames, ignore_index=True) if long_frames else pd.DataFrame()\n",
    "print(\"Registros RB en largo:\", len(long_rb))\n",
    "\n",
    "# wide unificado (solo voltajes, escalados)\n",
    "key_cols = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "svar = long_rb[\"variable\"].astype(str).str.upper().str.replace(r\"[\\s_\\.\\-]\",\"\", regex=True)\n",
    "hum = svar.isin({\"VHUM\"})\n",
    "tem = svar.isin({\"VTEM\"})\n",
    "keep = hum | tem\n",
    "lv = long_rb[keep].copy()\n",
    "lv[\"var_norm\"] = np.where(hum[keep], \"VOLT_HUM\", \"VOLT_TEM\")\n",
    "lv[\"valor_norm\"] = lv[\"valor\"] * RB_VOLT_SCALE\n",
    "\n",
    "wide_rb = (lv.pivot_table(index=key_cols, columns=\"var_norm\", values=\"valor_norm\", aggfunc=\"first\").reset_index())\n",
    "\n",
    "# anexar crudos\n",
    "raw_keep = [c for c in [\"Date_raw\",\"LOC_time_raw\"] if c in long_rb.columns]\n",
    "if raw_keep:\n",
    "    raw = (long_rb[key_cols + raw_keep].groupby(key_cols, as_index=False).first())\n",
    "    wide_rb = wide_rb.merge(raw, on=key_cols, how=\"left\")\n",
    "\n",
    "# ordenar columnas\n",
    "volt_cols = [c for c in [\"VOLT_HUM\",\"VOLT_TEM\"] if c in wide_rb.columns]\n",
    "other = [c for c in wide_rb.columns if c not in (key_cols + raw_keep + volt_cols)]\n",
    "wide_rb = wide_rb[key_cols + raw_keep + volt_cols + other]\n",
    "\n",
    "print(\"Filas en datos_wide (RB):\", len(wide_rb), \"| Columnas:\", len(wide_rb.columns))\n",
    "\n",
    "# exportar solo inventario + wide + diccionario + qa\n",
    "inv_path_rb = Path(OUTPUT_DIR) / \"inventario_RB.xlsx\"\n",
    "out_path_rb = Path(OUTPUT_DIR) / \"ConsolidadoSensores_RB.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(inv_path_rb, engine=\"xlsxwriter\") as w:\n",
    "    inv_rb.to_excel(w, sheet_name=\"inventario_RB\", index=False)\n",
    "\n",
    "qa_rb = (wide_rb.groupby([\"planta\",\"año\",\"sensor_id\"])\n",
    "         .agg(registros=(\"VOLT_HUM\",\"size\"),\n",
    "              fechas_min=(\"timestamp\",\"min\"),\n",
    "              fechas_max=(\"timestamp\",\"max\"))\n",
    "         .reset_index()) if not wide_rb.empty else pd.DataFrame(columns=[\"planta\",\"año\",\"sensor_id\",\"registros\",\"fechas_min\",\"fechas_max\"])\n",
    "\n",
    "with pd.ExcelWriter(out_path_rb, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "    wide_rb.to_excel(w, sheet_name=\"datos_wide\", index=False)\n",
    "    dicc = pd.DataFrame({\n",
    "        \"columna\": [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\"Date_raw\",\"LOC_time_raw\",\"VOLT_HUM\",\"VOLT_TEM\",\"source_file\",\"source_path\"],\n",
    "        \"descripcion\": [\"Planta origen (RB)\",\"Año\",\"Fecha tirada\",\"ID sensor\",\"Timestamp unificado\",\"Fecha cruda RB\",\"Hora local cruda RB\",\"Voltaje humedad (÷100)\",\"Voltaje temperatura (÷100)\",\"Archivo fuente\",\"Ruta fuente\"]\n",
    "    })\n",
    "    dicc.to_excel(w, sheet_name=\"diccionario\", index=False)\n",
    "    qa_rb.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "\n",
    "print(\"Inventario RB:\", inv_path_rb)\n",
    "print(\"Consolidado RB:\", out_path_rb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c7da6",
   "metadata": {},
   "source": [
    "# Solo JPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe4ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos JPV detectados: 137\n",
      "Registros JPV en largo: 1550621\n",
      "Filas en datos_wide (JPV): 258460 | Columnas: 8\n",
      "Inventario JPV: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_JPV\\inventario_JPV.xlsx\n",
      "Consolidado JPV: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_JPV\\Consolidado_Sensores_JPV.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === SOLO JPV (autosuficiente) ===\n",
    "\n",
    "# >>> CONFIG <<<\n",
    "BASE_JPV   = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_solo_JPV\"\n",
    "SAMPLE_MAX_FILES = None   # poné un número (p. ej. 20) para prueba rápida, o None para todo\n",
    "DROP_WIDE_COLS = [\"TimeString\", \"HUMEDAD\", \"OFFSET\", \"TEMPERATURA\", \"Date_raw\", \"LOC_time_raw\"] # excluir columnas del datos_wide\n",
    "# >>> FIN CONFIG <<<\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)_]\", \"\", str(s).upper())\n",
    "\n",
    "def is_plain_sensor_folder(name: str) -> bool:\n",
    "    # Acepta SENSOR1..SENSOR6 (exacto, sin sufijos b/c)\n",
    "    return re.fullmatch(r\"SENSOR([1-6])\", name.upper()) is not None\n",
    "\n",
    "def extract_sensor_id_from_name(name: str):\n",
    "    # JPV usa SENSOR10,20,...,60 (1..6) o carpetas SENSOR1..6\n",
    "    m = re.search(r\"SENSOR\\s*(\\d+)\", str(name).upper())\n",
    "    if not m:\n",
    "        return None\n",
    "    n = int(m.group(1))\n",
    "    return n//10 if n in (10,20,30,40,50,60) else n\n",
    "\n",
    "def parse_tirada_jpv(path: Path):\n",
    "    \"\"\"\n",
    "    Busca en los segmentos: 'usb <número>' y una fecha dd.mm.aa[aa] / dd-mm-aaaa / dd/mm/aaaa\n",
    "    Tolera texto adicional (p. ej., 'Datos USB ...', 'usb ... Cambio ...').\n",
    "    \"\"\"\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    usb_num = None\n",
    "    date_dt = None\n",
    "    for part in path.parts:\n",
    "        if usb_num is None:\n",
    "            mnum = re.search(r\"\\busb\\s*(\\d+)\", part, flags=re.IGNORECASE)\n",
    "            if mnum:\n",
    "                usb_num = int(mnum.group(1))\n",
    "        if date_dt is None:\n",
    "            mdate = re.search(DATE_RX, part)\n",
    "            if mdate:\n",
    "                d, m, y = mdate.groups()\n",
    "                y = int(y) + (2000 if int(y) < 100 else 0)\n",
    "                try:\n",
    "                    date_dt = datetime(y, int(m), int(d))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        if usb_num is not None and date_dt is not None:\n",
    "            break\n",
    "    return usb_num, date_dt\n",
    "\n",
    "def read_jpv_txt(path: Path) -> pd.DataFrame:\n",
    "    # Intentos de lectura típicos (UTF-16)\n",
    "    tried = [(\"utf-16\", \"\\t\"), (\"utf-16le\", \"\\t\"), (\"utf-8\", \"\\t\")]\n",
    "    last_err = None\n",
    "    df = None\n",
    "    for enc, sep in tried:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, encoding=enc, engine=\"python\", on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        raise last_err or RuntimeError(f\"No se pudo leer {path}\")\n",
    "\n",
    "    # Filtrar metadatos\n",
    "    if \"VarName\" in df.columns:\n",
    "        df = df[~df[\"VarName\"].astype(str).str.startswith(\"$RT_\")]\n",
    "\n",
    "    # Selección mínima + trazabilidad\n",
    "    keep = [c for c in [\"VarName\",\"TimeString\",\"VarValue\",\"Validity\",\"Time_ms\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    # Timestamp\n",
    "    df[\"timestamp\"] = pd.to_datetime(df.get(\"TimeString\", pd.Series(dtype=str)), errors=\"coerce\")\n",
    "\n",
    "    # Variable normalizada y original\n",
    "    if \"VarName\" in df.columns:\n",
    "        df[\"VarName_original\"] = df[\"VarName\"].astype(str)\n",
    "        df[\"variable\"] = df[\"VarName\"].astype(str).str.replace(r\"^\\d+_\", \"\", regex=True)\n",
    "    else:\n",
    "        df[\"VarName_original\"] = np.nan\n",
    "        df[\"variable\"] = \"Var\"\n",
    "\n",
    "    # VarValue -> número (coma decimal → punto si aplica)\n",
    "    if \"VarValue\" in df.columns:\n",
    "        s = df[\"VarValue\"].astype(str).str.strip()\n",
    "        s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "        df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"valor\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------- Inventario JPV (solo .txt bajo SENSOR1..6; excluye SENSORb/c) --------\n",
    "rows = []\n",
    "for root, dirs, files in os.walk(BASE_JPV):\n",
    "    root_p = Path(root)\n",
    "    # Si estoy parado en una carpeta SENSOR*, omitir si no es exactamente SENSOR1..6\n",
    "    if root_p.name.upper().startswith(\"SENSOR\") and not is_plain_sensor_folder(root_p.name):\n",
    "        # no descender a subcarpetas de estos sensores con sufijos b/c\n",
    "        dirs[:] = []\n",
    "        continue\n",
    "\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".txt\") and is_plain_sensor_folder(root_p.name):\n",
    "            p = root_p / f\n",
    "            planta = \"JPV\"\n",
    "            sensor_id = extract_sensor_id_from_name(f) or extract_sensor_id_from_name(root_p.name)\n",
    "            tirada_num, tirada_dt = parse_tirada_jpv(root_p)\n",
    "            # año desde carpeta \"20xx Datos Sensores JPV\" si aparece en la ruta\n",
    "            año = None\n",
    "            m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+JPV\", str(p), flags=re.IGNORECASE)\n",
    "            if m: año = int(m.group(1))\n",
    "            rows.append({\n",
    "                \"planta\": planta, \"año\": año,\n",
    "                \"tirada_num\": tirada_num, \"tirada_fecha\": tirada_dt,\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"source_file\": f, \"source_path\": str(p)\n",
    "            })\n",
    "\n",
    "inv_jpv = pd.DataFrame(rows)\n",
    "if inv_jpv.empty:\n",
    "    print(\"No se encontraron archivos JPV. Revisá BASE_JPV.\")\n",
    "else:\n",
    "    inv_jpv.sort_values([\"año\",\"tirada_fecha\",\"sensor_id\",\"source_file\"], inplace=True, ignore_index=True)\n",
    "print(\"Archivos JPV detectados:\", len(inv_jpv))\n",
    "\n",
    "# Muestra (opcional)\n",
    "if SAMPLE_MAX_FILES:\n",
    "    inv_jpv = inv_jpv.head(int(SAMPLE_MAX_FILES)).reset_index(drop=True)\n",
    "\n",
    "# -------- Procesamiento archivos (solo JPV) --------\n",
    "long_frames = []\n",
    "log_rows = []\n",
    "for _, r in inv_jpv.iterrows():\n",
    "    p = Path(r[\"source_path\"])\n",
    "    try:\n",
    "        df = read_jpv_txt(p)\n",
    "        # meta\n",
    "        df[\"planta\"] = \"JPV\"\n",
    "        df[\"año\"] = r[\"año\"]\n",
    "        df[\"tirada_num\"] = r[\"tirada_num\"]\n",
    "        df[\"tirada_fecha\"] = pd.to_datetime(r[\"tirada_fecha\"]) if pd.notna(r[\"tirada_fecha\"]) else pd.NaT\n",
    "        df[\"sensor_id\"] = r[\"sensor_id\"]\n",
    "        df[\"source_file\"] = r[\"source_file\"]\n",
    "        df[\"source_path\"] = r[\"source_path\"]\n",
    "        # asegurar columnas crudas\n",
    "        for c in [\"Date_raw\",\"LOC_time_raw\",\"VarName\",\"TimeString\",\"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\"]:\n",
    "            if c not in df.columns:\n",
    "                df[c] = pd.Series([np.nan]*len(df))\n",
    "        long_frames.append(df)\n",
    "    except Exception as e:\n",
    "        log_rows.append({\"tipo\":\"error_lectura\",\"source_path\":str(p),\"detalle\":str(e)})\n",
    "\n",
    "long_jpv = pd.concat(long_frames, ignore_index=True) if long_frames else pd.DataFrame()\n",
    "print(\"Registros JPV en largo:\", len(long_jpv))\n",
    "\n",
    "# -------- Wide (unificar voltajes en VOLT_HUM / VOLT_TEM; excluir HUMEDAD/TEMPERATURA/OFFSET) --------\n",
    "if long_jpv.empty:\n",
    "    wide_jpv = pd.DataFrame()\n",
    "else:\n",
    "    # claves estables\n",
    "    key_cols = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "    for c in key_cols:\n",
    "        if c not in long_jpv.columns:\n",
    "            long_jpv[c] = np.nan\n",
    "\n",
    "    # map variables JPV → VOLT_HUM / VOLT_TEM\n",
    "    svar = (long_jpv[\"variable\"].astype(str).str.upper().str.replace(r\"[\\s_\\.\\-]\", \"\", regex=True))\n",
    "    hum_aliases  = {\"VOLTHUM\",\"VOLTHUME\",\"VHUM\"}     # por si existiese VHUM alguna vez\n",
    "    tem_aliases  = {\"VOLTTEM\",\"VOLTTEMP\",\"VTEM\",\"VTEMP\"}\n",
    "    drop_aliases = {\"HUMEDAD\",\"TEMPERATURA\",\"OFFSET\",\"VARIEDAD\"}\n",
    "\n",
    "    mask_hum  = svar.isin(hum_aliases)\n",
    "    mask_tem  = svar.isin(tem_aliases)\n",
    "    mask_drop = svar.isin(drop_aliases)\n",
    "\n",
    "    keep = (mask_hum | mask_tem) & (~mask_drop)\n",
    "    lv = long_jpv[keep].copy()\n",
    "    lv[\"var_norm\"] = np.where(mask_hum[keep], \"VOLT_HUM\", \"VOLT_TEM\")\n",
    "    lv[\"valor_norm\"] = pd.to_numeric(lv[\"valor\"], errors=\"coerce\")  # JPV no requiere escala\n",
    "\n",
    "    wide_jpv = (lv.pivot_table(index=key_cols, columns=\"var_norm\", values=\"valor_norm\", aggfunc=\"first\")\n",
    "                  .reset_index())\n",
    "\n",
    "    # anexar crudos + tirada_num\n",
    "    raw_keep = [c for c in [\"tirada_num\",\"Date_raw\",\"LOC_time_raw\",\"TimeString\"] if c in long_jpv.columns]\n",
    "    if raw_keep:\n",
    "        raw = (long_jpv[key_cols + raw_keep].groupby(key_cols, as_index=False).first())\n",
    "        wide_jpv = wide_jpv.merge(raw, on=key_cols, how=\"left\")\n",
    "\n",
    "    # ordenar columnas\n",
    "    volt_cols = [c for c in [\"VOLT_HUM\",\"VOLT_TEM\"] if c in wide_jpv.columns]\n",
    "    other = [c for c in wide_jpv.columns if c not in (key_cols + raw_keep + volt_cols)]\n",
    "    wide_jpv = wide_jpv[key_cols + raw_keep + volt_cols + other]\n",
    "\n",
    "    # quitar columnas no deseadas\n",
    "    if DROP_WIDE_COLS:\n",
    "        wide_jpv = wide_jpv.drop(columns=[c for c in DROP_WIDE_COLS if c in wide_jpv.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"Filas en datos_wide (JPV):\", len(wide_jpv), \"| Columnas:\", len(wide_jpv.columns))\n",
    "\n",
    "# -------- QA y export --------\n",
    "qa_jpv = (\n",
    "    wide_jpv.groupby([\"planta\",\"año\",\"sensor_id\"])\n",
    "    .agg(registros=(\"VOLT_HUM\",\"size\"), fechas_min=(\"timestamp\",\"min\"), fechas_max=(\"timestamp\",\"max\"))\n",
    "    .reset_index()\n",
    ") if not wide_jpv.empty else pd.DataFrame(columns=[\"planta\",\"año\",\"sensor_id\",\"registros\",\"fechas_min\",\"fechas_max\"])\n",
    "\n",
    "out_dir = Path(OUTPUT_DIR)\n",
    "inv_path = out_dir / \"inventario_JPV.xlsx\"\n",
    "out_path = out_dir / \"Consolidado_Sensores_JPV.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(inv_path, engine=\"xlsxwriter\") as w:\n",
    "    inv_jpv.to_excel(w, sheet_name=\"inventario_JPV\", index=False)\n",
    "\n",
    "with pd.ExcelWriter(out_path, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "    wide_jpv.to_excel(w, sheet_name=\"datos_wide\", index=False)\n",
    "\n",
    "    dicc = pd.DataFrame({\n",
    "        \"columna\": [\"planta\",\"año\",\"tirada_num\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "                    \"Date_raw\",\"LOC_time_raw\",\"TimeString\",\"VOLT_HUM\",\"VOLT_TEM\",\n",
    "                    \"VarName\",\"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\",\"source_file\",\"source_path\"],\n",
    "        \"descripcion\": [\"Planta origen (JPV)\",\"Año\",\"N° tirada (si existía)\",\"Fecha tirada\",\n",
    "                        \"ID sensor (1–6)\",\"Timestamp unificado\",\n",
    "                        \"Fecha cruda RB (no aplica)\",\"Hora local cruda RB (no aplica)\",\"Tiempo crudo JPV\",\n",
    "                        \"Voltaje humedad (JPV)\",\"Voltaje temperatura (JPV)\",\n",
    "                        \"Nombre variable original JPV\",\"Valor crudo original\",\"Bandera JPV\",\"Tiempo en ms JPV\",\n",
    "                        \"Nombre original JPV\",\"Archivo fuente\",\"Ruta fuente\"]\n",
    "    })\n",
    "    dicc.to_excel(w, sheet_name=\"diccionario\", index=False)\n",
    "\n",
    "    if len(log_rows):\n",
    "        pd.DataFrame(log_rows).to_excel(w, sheet_name=\"log_omisiones\", index=False)\n",
    "    qa_jpv.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "\n",
    "print(\"Inventario JPV:\", inv_path)\n",
    "print(\"Consolidado JPV:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451934a1",
   "metadata": {},
   "source": [
    "# Solo RB por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da28e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos RB detectados: 126\n",
      "[2024] Archivos: 54\n",
      "[2024] datos_wide filas: 71874\n",
      "[2024] Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\inventario_RB_2024.xlsx\n",
      "[2024] Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2024.xlsx\n",
      "[2025] Archivos: 72\n",
      "[2025] datos_wide filas: 87128\n",
      "[2025] Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\inventario_RB_2025.xlsx\n",
      "[2025] Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === SOLO RB por año (autosuficiente) ===\n",
    "# >>> CONFIG <<<\n",
    "BASE_RB   = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\"\n",
    "SAMPLE_MAX_FILES = None       # poné un número para prueba (p. ej. 200) o None para todo\n",
    "RB_VOLT_SCALE = 0.01          # dividir RB por 100\n",
    "OMIT_DIR_NAME = \"Datos otro formato\"  # omitir esta carpeta\n",
    "# >>> FIN CONFIG <<<\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)_]\", \"\", str(s).upper())\n",
    "\n",
    "def extract_sensor_id_from_name(name: str):\n",
    "    m = re.search(r\"SENSOR\\s*(\\d+)\", str(name).upper())\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def parse_tirada_rb(path: Path):\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    for part in path.parts[::-1]:\n",
    "        mdate = re.search(DATE_RX, part)\n",
    "        if mdate:\n",
    "            d, m, y = mdate.groups()\n",
    "            y = int(y) + (2000 if int(y) < 100 else 0)\n",
    "            try:\n",
    "                return datetime(y, int(m), int(d))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def _canon_col(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)_]\", \"\", str(s).upper())\n",
    "\n",
    "def read_rb_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, engine=\"python\")\n",
    "\n",
    "    # detectar columnas fecha/hora\n",
    "    date_col = next((c for c in df.columns if _canon_col(c) in (\"DATE\",\"FECHA\")), None)\n",
    "    lt_col   = next((c for c in df.columns if _canon_col(c) in (\"LOCTIME\",\"LOCTIEMPO\",\"LOCALTIME\")), None)\n",
    "\n",
    "    # timestamp\n",
    "    if date_col and lt_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col].astype(str)+\" \"+df[lt_col].astype(str),\n",
    "                                         dayfirst=False, errors=\"coerce\")\n",
    "    elif date_col:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"timestamp\"] = pd.NaT\n",
    "\n",
    "    # map variables (voltajes)\n",
    "    value_cols_map = {}\n",
    "    for c in df.columns:\n",
    "        cc = _canon_col(c)\n",
    "        if cc in (\"VHUM\",\"V_HUM\"):\n",
    "            value_cols_map[c] = \"V_HUM\"\n",
    "        elif cc in (\"VTEM\",\"V_TEM\",\"VTEMP\",\"V_TEMP\"):\n",
    "            value_cols_map[c] = \"V_TEM\"\n",
    "    value_cols = list(value_cols_map.keys())\n",
    "\n",
    "    # id cols\n",
    "    id_cols = [\"timestamp\"]\n",
    "    if \"Record\" in df.columns: id_cols.insert(0, \"Record\")\n",
    "    if date_col: id_cols.insert(1, date_col)\n",
    "    if lt_col:   id_cols.insert(2, lt_col)\n",
    "\n",
    "    if not value_cols:\n",
    "        # no hay columnas de voltaje: devolvemos frame mínimo\n",
    "        out = df[id_cols].copy()\n",
    "        if date_col: out.rename(columns={date_col:\"Date_raw\"}, inplace=True)\n",
    "        if lt_col:   out.rename(columns={lt_col:\"LOC_time_raw\"}, inplace=True)\n",
    "        out[\"variable\"] = np.nan\n",
    "        out[\"valor\"] = np.nan\n",
    "        return out[[\"Record\" if \"Record\" in out.columns else None,\n",
    "                    \"Date_raw\" if \"Date_raw\" in out.columns else None,\n",
    "                    \"LOC_time_raw\" if \"LOC_time_raw\" in out.columns else None,\n",
    "                    \"timestamp\",\"variable\",\"valor\"]].dropna(how=\"all\", axis=1)\n",
    "\n",
    "    long_df = df[id_cols + value_cols].melt(id_vars=id_cols, var_name=\"variable_raw\", value_name=\"valor_raw\")\n",
    "    long_df[\"variable\"] = long_df[\"variable_raw\"].map(value_cols_map)\n",
    "\n",
    "    if date_col: long_df.rename(columns={date_col:\"Date_raw\"}, inplace=True)\n",
    "    if lt_col:   long_df.rename(columns={lt_col:\"LOC_time_raw\"}, inplace=True)\n",
    "\n",
    "    s = long_df[\"valor_raw\"].astype(str).str.strip()\n",
    "    s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "    long_df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # meta\n",
    "    long_df[\"planta\"] = \"RB\"\n",
    "    m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+RB\", str(path), flags=re.IGNORECASE)\n",
    "    long_df[\"año\"] = int(m.group(1)) if m else None\n",
    "    long_df[\"tirada_fecha\"] = parse_tirada_rb(path.parent)\n",
    "    long_df[\"sensor_id\"] = extract_sensor_id_from_name(path.name) or extract_sensor_id_from_name(path.parent.name)\n",
    "    long_df[\"source_file\"] = path.name\n",
    "    long_df[\"source_path\"] = str(path)\n",
    "    return long_df\n",
    "\n",
    "def build_inventory_rb() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for root, dirs, files in os.walk(BASE_RB):\n",
    "        # omitir carpeta \"Datos otro formato\"\n",
    "        dirs[:] = [d for d in dirs if _canon(d) != _canon(OMIT_DIR_NAME)]\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".csv\"):\n",
    "                p = Path(root)/f\n",
    "                rows.append({\"source_file\":f,\"source_path\":str(p)})\n",
    "    inv = pd.DataFrame(rows)\n",
    "    return inv\n",
    "\n",
    "def process_subset(inv_subset: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames, logs = [], []\n",
    "    for _, r in inv_subset.iterrows():\n",
    "        try:\n",
    "            long_frames.append(read_rb_csv(Path(r[\"source_path\"])))\n",
    "        except Exception as e:\n",
    "            logs.append({\"tipo\":\"error_lectura\",\"source_path\":r[\"source_path\"],\"detalle\":str(e)})\n",
    "    long_rb = pd.concat(long_frames, ignore_index=True) if long_frames else pd.DataFrame()\n",
    "\n",
    "    # wide unificado (VOLT_HUM / VOLT_TEM), escala ÷100\n",
    "    if long_rb.empty:\n",
    "        wide_rb = pd.DataFrame()\n",
    "    else:\n",
    "        key = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "        for c in key:\n",
    "            if c not in long_rb.columns: long_rb[c] = np.nan\n",
    "\n",
    "        svar = long_rb[\"variable\"].astype(str).str.upper().str.replace(r\"[\\s_\\.\\-]\",\"\", regex=True)\n",
    "        hum = svar.isin({\"VHUM\"})\n",
    "        tem = svar.isin({\"VTEM\"})\n",
    "        keep = hum | tem\n",
    "        lv = long_rb[keep].copy()\n",
    "        lv[\"var_norm\"] = np.where(hum[keep], \"VOLT_HUM\", \"VOLT_TEM\")\n",
    "        lv[\"valor_norm\"] = lv[\"valor\"] * RB_VOLT_SCALE\n",
    "\n",
    "        wide_rb = (lv.pivot_table(index=key, columns=\"var_norm\", values=\"valor_norm\", aggfunc=\"first\")\n",
    "                     .reset_index())\n",
    "\n",
    "        raw_keep = [c for c in [\"Date_raw\",\"LOC_time_raw\"] if c in long_rb.columns]\n",
    "        if raw_keep:\n",
    "            raw = (long_rb[key + raw_keep].groupby(key, as_index=False).first())\n",
    "            wide_rb = wide_rb.merge(raw, on=key, how=\"left\")\n",
    "\n",
    "        # ordenar\n",
    "        volt = [c for c in [\"VOLT_HUM\",\"VOLT_TEM\"] if c in wide_rb.columns]\n",
    "        other = [c for c in wide_rb.columns if c not in (key + raw_keep + volt)]\n",
    "        wide_rb = wide_rb[key + raw_keep + volt + other]\n",
    "\n",
    "    qa = (wide_rb.groupby([\"planta\",\"año\",\"sensor_id\"])\n",
    "          .agg(registros=(\"VOLT_HUM\",\"size\"),\n",
    "               fechas_min=(\"timestamp\",\"min\"),\n",
    "               fechas_max=(\"timestamp\",\"max\"))\n",
    "          .reset_index()) if not wide_rb.empty else pd.DataFrame(columns=[\"planta\",\"año\",\"sensor_id\",\"registros\",\"fechas_min\",\"fechas_max\"])\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    return wide_rb, qa, log_df\n",
    "\n",
    "# ===== Runner por año =====\n",
    "inv_all = build_inventory_rb()\n",
    "print(\"Archivos RB detectados:\", len(inv_all))\n",
    "\n",
    "# enriquecer inventario con meta básica (año/fecha/sensor) para dividir por año\n",
    "meta_frames = []\n",
    "for _, r in inv_all.iterrows():\n",
    "    try:\n",
    "        tmp = read_rb_csv(Path(r[\"source_path\"])).head(1)  # una fila para extraer meta rápido\n",
    "        meta_frames.append(tmp[[\"año\",\"tirada_fecha\",\"sensor_id\",\"source_file\",\"source_path\"]])\n",
    "    except Exception:\n",
    "        pass\n",
    "meta = pd.concat(meta_frames, ignore_index=True) if meta_frames else pd.DataFrame()\n",
    "inv_all = inv_all.merge(meta[[\"año\",\"source_path\"]], on=\"source_path\", how=\"left\")\n",
    "\n",
    "# fallback: si 'año' viene NaN pero hay tirada_fecha, lo inferimos\n",
    "if \"año\" in inv_all.columns:\n",
    "    if inv_all[\"año\"].isna().any():\n",
    "        # re-lee fecha solo cuando hace falta\n",
    "        for i in inv_all[inv_all[\"año\"].isna()].index:\n",
    "            tf = parse_tirada_rb(Path(inv_all.at[i,\"source_path\"]).parent)\n",
    "            inv_all.at[i,\"año\"] = tf.year if isinstance(tf, datetime) else np.nan\n",
    "\n",
    "# filtrar por año\n",
    "for yr in (2024, 2025):\n",
    "    inv_y = inv_all[inv_all[\"año\"] == yr].reset_index(drop=True)\n",
    "    if inv_y.empty:\n",
    "        print(f\"[{yr}] No hay archivos RB.\")\n",
    "        continue\n",
    "\n",
    "    # sample por año si se pide\n",
    "    if SAMPLE_MAX_FILES:\n",
    "        inv_y = inv_y.head(int(SAMPLE_MAX_FILES)).reset_index(drop=True)\n",
    "\n",
    "    print(f\"[{yr}] Archivos:\", len(inv_y))\n",
    "    wide_rb, qa_rb, log_rb = process_subset(inv_y)\n",
    "    print(f\"[{yr}] datos_wide filas:\", len(wide_rb))\n",
    "\n",
    "    out_dir = Path(OUTPUT_DIR)\n",
    "    inv_path = out_dir / f\"inventario_RB_{yr}.xlsx\"\n",
    "    out_path = out_dir / f\"Consolidado_Sensores_RB_{yr}.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(inv_path, engine=\"xlsxwriter\") as w:\n",
    "        inv_y.to_excel(w, sheet_name=f\"inventario_RB_{yr}\", index=False)\n",
    "\n",
    "    with pd.ExcelWriter(out_path, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "        wide_rb.to_excel(w, sheet_name=\"datos_wide\", index=False)\n",
    "        dicc = pd.DataFrame({\n",
    "            \"columna\": [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "                        \"Date_raw\",\"LOC_time_raw\",\"VOLT_HUM\",\"VOLT_TEM\",\"source_file\",\"source_path\"],\n",
    "            \"descripcion\": [\"Planta origen (RB)\",\"Año\",\"Fecha tirada\",\"ID sensor\",\"Timestamp unificado\",\n",
    "                            \"Fecha cruda RB\",\"Hora local cruda RB\",\"Voltaje humedad (÷100)\",\"Voltaje temperatura (÷100)\",\n",
    "                            \"Archivo fuente\",\"Ruta fuente\"]\n",
    "        })\n",
    "        dicc.to_excel(w, sheet_name=\"diccionario\", index=False)\n",
    "        if not log_rb.empty:\n",
    "            log_rb.to_excel(w, sheet_name=\"log_omisiones\", index=False)\n",
    "        qa_rb.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "\n",
    "    print(f\"[{yr}] Inventario: {inv_path}\")\n",
    "    print(f\"[{yr}] Consolidado: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e98362",
   "metadata": {},
   "source": [
    "# Solo JPV por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2559062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos JPV detectados: 137\n",
      "[2024] Archivos: 59\n",
      "[2024] datos_wide filas: 128934\n",
      "[2024] Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\inventario_JPV_2024.xlsx\n",
      "[2024] Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\Consolidado_Sensores_JPV_2024.xlsx\n",
      "[2025] Archivos: 78\n",
      "[2025] datos_wide filas: 129526\n",
      "[2025] Inventario: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\inventario_JPV_2025.xlsx\n",
      "[2025] Consolidado: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\Consolidado_Sensores_JPV_2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === SOLO JPV por año (autosuficiente) ===\n",
    "# >>> CONFIG <<<\n",
    "BASE_JPV   = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\"\n",
    "SAMPLE_MAX_FILES = None       # poner un número para prueba o None\n",
    "DROP_WIDE_COLS = [\"TimeString\",\"HUMEDAD\",\"OFFSET\",\"TEMPERATURA\",\"Date_raw\",\"LOC_time_raw\"]  # no queremos estos en wide JPV\n",
    "# >>> FIN CONFIG <<<\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\.\\(\\)_]\", \"\", str(s).upper())\n",
    "\n",
    "def is_plain_sensor_folder(name: str) -> bool:\n",
    "    return re.fullmatch(r\"SENSOR([1-6])\", name.upper()) is not None\n",
    "\n",
    "def extract_sensor_id_from_name(name: str):\n",
    "    m = re.search(r\"SENSOR\\s*(\\d+)\", str(name).upper())\n",
    "    if not m: return None\n",
    "    n = int(m.group(1))\n",
    "    return n//10 if n in (10,20,30,40,50,60) else n\n",
    "\n",
    "def parse_tirada_jpv(path: Path):\n",
    "    DATE_RX = r\"([0-3]?\\d)[\\.\\-\\/]([01]?\\d)[\\.\\-\\/](\\d{2,4})\"\n",
    "    usb_num = None\n",
    "    date_dt = None\n",
    "    for part in path.parts:\n",
    "        if usb_num is None:\n",
    "            mnum = re.search(r\"\\busb\\s*(\\d+)\", part, flags=re.IGNORECASE)\n",
    "            if mnum: usb_num = int(mnum.group(1))\n",
    "        if date_dt is None:\n",
    "            mdate = re.search(DATE_RX, part)\n",
    "            if mdate:\n",
    "                d, m, y = mdate.groups()\n",
    "                y = int(y) + (2000 if int(y) < 100 else 0)\n",
    "                try:\n",
    "                    date_dt = datetime(y, int(m), int(d))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        if usb_num is not None and date_dt is not None:\n",
    "            break\n",
    "    return usb_num, date_dt\n",
    "\n",
    "def read_jpv_txt(path: Path) -> pd.DataFrame:\n",
    "    tried = [(\"utf-16\",\"\\t\"),(\"utf-16le\",\"\\t\"),(\"utf-8\",\"\\t\")]\n",
    "    df, last_err = None, None\n",
    "    for enc, sep in tried:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, encoding=enc, engine=\"python\", on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        raise last_err or RuntimeError(f\"No se pudo leer {path}\")\n",
    "\n",
    "    if \"VarName\" in df.columns:\n",
    "        df = df[~df[\"VarName\"].astype(str).str.startswith(\"$RT_\")]\n",
    "\n",
    "    keep = [c for c in [\"VarName\",\"TimeString\",\"VarValue\",\"Validity\",\"Time_ms\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df.get(\"TimeString\", pd.Series(dtype=str)), errors=\"coerce\")\n",
    "\n",
    "    if \"VarName\" in df.columns:\n",
    "        df[\"VarName_original\"] = df[\"VarName\"].astype(str)\n",
    "        df[\"variable\"] = df[\"VarName\"].astype(str).str.replace(r\"^\\d+_\", \"\", regex=True)\n",
    "    else:\n",
    "        df[\"VarName_original\"] = np.nan\n",
    "        df[\"variable\"] = \"Var\"\n",
    "\n",
    "    if \"VarValue\" in df.columns:\n",
    "        s = df[\"VarValue\"].astype(str).str.strip()\n",
    "        s = np.where(s.str.contains(r\"^\\s*-?\\d+,\\d+\\s*$\"), s.str.replace(\",\", \".\", regex=False), s)\n",
    "        df[\"valor\"] = pd.to_numeric(s, errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"valor\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_inventory_jpv() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for root, dirs, files in os.walk(BASE_JPV):\n",
    "        root_p = Path(root)\n",
    "        # no descender en sensores b/c\n",
    "        if root_p.name.upper().startswith(\"SENSOR\") and not is_plain_sensor_folder(root_p.name):\n",
    "            dirs[:] = []\n",
    "            continue\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".txt\") and is_plain_sensor_folder(root_p.name):\n",
    "                p = root_p / f\n",
    "                usb_num, tdt = parse_tirada_jpv(root_p)\n",
    "                año = None\n",
    "                m = re.search(r\"(20\\d{2})\\s+Datos\\s+Sensores\\s+JPV\", str(p), flags=re.IGNORECASE)\n",
    "                if m: año = int(m.group(1))\n",
    "                rows.append({\n",
    "                    \"source_file\": f,\n",
    "                    \"source_path\": str(p),\n",
    "                    \"sensor_id\": extract_sensor_id_from_name(f) or extract_sensor_id_from_name(root_p.name),\n",
    "                    \"tirada_num\": usb_num,\n",
    "                    \"tirada_fecha\": tdt,\n",
    "                    \"año\": año\n",
    "                })\n",
    "    inv = pd.DataFrame(rows)\n",
    "    return inv\n",
    "\n",
    "def process_subset(inv_subset: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames, logs = [], []\n",
    "    for _, r in inv_subset.iterrows():\n",
    "        p = Path(r[\"source_path\"])\n",
    "        try:\n",
    "            df = read_jpv_txt(p)\n",
    "            df[\"planta\"] = \"JPV\"\n",
    "            df[\"año\"] = r[\"año\"]\n",
    "            df[\"tirada_num\"] = r[\"tirada_num\"]\n",
    "            df[\"tirada_fecha\"] = pd.to_datetime(r[\"tirada_fecha\"]) if pd.notna(r[\"tirada_fecha\"]) else pd.NaT\n",
    "            df[\"sensor_id\"] = r[\"sensor_id\"]\n",
    "            df[\"source_file\"] = r[\"source_file\"]\n",
    "            df[\"source_path\"] = r[\"source_path\"]\n",
    "            for c in [\"Date_raw\",\"LOC_time_raw\",\"VarName\",\"TimeString\",\"VarValue\",\"Validity\",\"Time_ms\",\"VarName_original\"]:\n",
    "                if c not in df.columns: df[c] = pd.Series([np.nan]*len(df))\n",
    "            long_frames.append(df)\n",
    "        except Exception as e:\n",
    "            logs.append({\"tipo\":\"error_lectura\",\"source_path\":str(p),\"detalle\":str(e)})\n",
    "\n",
    "    long_jpv = pd.concat(long_frames, ignore_index=True) if long_frames else pd.DataFrame()\n",
    "\n",
    "    if long_jpv.empty:\n",
    "        wide_jpv = pd.DataFrame()\n",
    "    else:\n",
    "        key = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "        for c in key:\n",
    "            if c not in long_jpv.columns: long_jpv[c] = np.nan\n",
    "\n",
    "        svar = long_jpv[\"variable\"].astype(str).str.upper().str.replace(r\"[\\s_\\.\\-]\",\"\", regex=True)\n",
    "        hum_aliases  = {\"VOLTHUM\",\"VOLTHUME\",\"VHUM\"}\n",
    "        tem_aliases  = {\"VOLTTEM\",\"VOLTTEMP\",\"VTEM\",\"VTEMP\"}\n",
    "        drop_aliases = {\"HUMEDAD\",\"TEMPERATURA\",\"OFFSET\",\"VARIEDAD\"}\n",
    "\n",
    "        hum = svar.isin(hum_aliases)\n",
    "        tem = svar.isin(tem_aliases)\n",
    "        drop = svar.isin(drop_aliases)\n",
    "\n",
    "        keep = (hum | tem) & (~drop)\n",
    "        lv = long_jpv[keep].copy()\n",
    "        lv[\"var_norm\"] = np.where(hum[keep], \"VOLT_HUM\", \"VOLT_TEM\")\n",
    "        lv[\"valor_norm\"] = pd.to_numeric(lv[\"valor\"], errors=\"coerce\")  # JPV sin escala\n",
    "\n",
    "        wide_jpv = (lv.pivot_table(index=key, columns=\"var_norm\", values=\"valor_norm\", aggfunc=\"first\")\n",
    "                      .reset_index())\n",
    "\n",
    "        # anexar solo crudos deseados (tirada_num y NO Date/LOC para JPV)\n",
    "        raw_keep = [c for c in [\"tirada_num\",\"TimeString\"] if c in long_jpv.columns]\n",
    "        if raw_keep:\n",
    "            raw = (long_jpv[key + raw_keep].groupby(key, as_index=False).first())\n",
    "            wide_jpv = wide_jpv.merge(raw, on=key, how=\"left\")\n",
    "\n",
    "        # ordenar\n",
    "        volt = [c for c in [\"VOLT_HUM\",\"VOLT_TEM\"] if c in wide_jpv.columns]\n",
    "        other = [c for c in wide_jpv.columns if c not in (key + raw_keep + volt)]\n",
    "        wide_jpv = wide_jpv[key + raw_keep + volt + other]\n",
    "\n",
    "        # quitar columnas no deseadas\n",
    "        if DROP_WIDE_COLS:\n",
    "            wide_jpv = wide_jpv.drop(columns=[c for c in DROP_WIDE_COLS if c in wide_jpv.columns], errors=\"ignore\")\n",
    "\n",
    "    qa = (wide_jpv.groupby([\"planta\",\"año\",\"sensor_id\"])\n",
    "          .agg(registros=(\"VOLT_HUM\",\"size\"),\n",
    "               fechas_min=(\"timestamp\",\"min\"),\n",
    "               fechas_max=(\"timestamp\",\"max\"))\n",
    "          .reset_index()) if not wide_jpv.empty else pd.DataFrame(columns=[\"planta\",\"año\",\"sensor_id\",\"registros\",\"fechas_min\",\"fechas_max\"])\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    return wide_jpv, qa, log_df\n",
    "\n",
    "# ===== Runner por año =====\n",
    "inv_all = build_inventory_jpv()\n",
    "print(\"Archivos JPV detectados:\", len(inv_all))\n",
    "\n",
    "# si falta 'año' pero hay tirada_fecha, inferir desde la fecha\n",
    "if not inv_all.empty and inv_all[\"año\"].isna().any():\n",
    "    for i in inv_all[inv_all[\"año\"].isna()].index:\n",
    "        tf = inv_all.at[i,\"tirada_fecha\"]\n",
    "        if isinstance(tf, datetime):\n",
    "            inv_all.at[i,\"año\"] = tf.year\n",
    "\n",
    "# ordenar e intentar sample por año\n",
    "for yr in (2024, 2025):\n",
    "    inv_y = inv_all[inv_all[\"año\"] == yr].reset_index(drop=True)\n",
    "    if inv_y.empty:\n",
    "        print(f\"[{yr}] No hay archivos JPV.\")\n",
    "        continue\n",
    "\n",
    "    if SAMPLE_MAX_FILES:\n",
    "        inv_y = inv_y.head(int(SAMPLE_MAX_FILES)).reset_index(drop=True)\n",
    "\n",
    "    print(f\"[{yr}] Archivos:\", len(inv_y))\n",
    "    wide_jpv, qa_jpv, log_jpv = process_subset(inv_y)\n",
    "    print(f\"[{yr}] datos_wide filas:\", len(wide_jpv))\n",
    "\n",
    "    out_dir = Path(OUTPUT_DIR)\n",
    "    inv_path = out_dir / f\"inventario_JPV_{yr}.xlsx\"\n",
    "    out_path = out_dir / f\"Consolidado_Sensores_JPV_{yr}.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(inv_path, engine=\"xlsxwriter\") as w:\n",
    "        inv_y.to_excel(w, sheet_name=f\"inventario_JPV_{yr}\", index=False)\n",
    "\n",
    "    with pd.ExcelWriter(out_path, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "        wide_jpv.to_excel(w, sheet_name=\"datos_wide\", index=False)\n",
    "        dicc = pd.DataFrame({\n",
    "            \"columna\": [\"planta\",\"año\",\"tirada_num\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "                        \"TimeString\",\"VOLT_HUM\",\"VOLT_TEM\",\"source_file\",\"source_path\"],\n",
    "            \"descripcion\": [\"Planta origen (JPV)\",\"Año\",\"N° tirada (si estaba)\",\"Fecha tirada\",\"ID sensor\",\n",
    "                            \"Timestamp unificado\",\"Tiempo crudo JPV\",\n",
    "                            \"Voltaje humedad (JPV)\",\"Voltaje temperatura (JPV)\",\"Archivo fuente\",\"Ruta fuente\"]\n",
    "        })\n",
    "        dicc.to_excel(w, sheet_name=\"diccionario\", index=False)\n",
    "        if not log_jpv.empty:\n",
    "            log_jpv.to_excel(w, sheet_name=\"log_omisiones\", index=False)\n",
    "        qa_jpv.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "\n",
    "    print(f\"[{yr}] Inventario: {inv_path}\")\n",
    "    print(f\"[{yr}] Consolidado: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2b284",
   "metadata": {},
   "source": [
    "# Identificación de tachadas para JPV (por año)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddde888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas anotadas: 128934 | sin match: 82291\n",
      "Archivo tachadas: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\Consolidado_Sensores_JPV_2024_TACHADAS.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Identificación de tachadas para JPV (por año) ===\n",
    "\n",
    "# ======== CONFIG – CAMBIAR ESTAS DOS RUTAS SEGÚN EL AÑO ========\n",
    "# JPV 2024:\n",
    "INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\Consolidado_Sensores_JPV_2024.xlsx\"\n",
    "LAB_FILE        = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\\REVISADO UM Control Tachadas Secado JPV 2024.xlsx\"\n",
    "\n",
    "# JPV 2025 (descomentar estas dos líneas para ese año):\n",
    "#INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\Consolidado_Sensores_JPV_2025.xlsx\"\n",
    "#LAB_FILE        = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\\REVISADO UM Control Tachadas Secado JPV 2025.xlsx\"\n",
    "\n",
    "OUTPUT_PATH = Path(INPUT_WIDE_FILE).with_name(Path(INPUT_WIDE_FILE).stem + \"_TACHADAS.xlsx\")\n",
    "SHEET_WIDE  = \"datos_wide\"   # hoja en tu consolidado\n",
    "SHEET_LAB   = None           # si la dejás en None, intentamos detectar \"Tachadas\"\n",
    "DAYFIRST    = True           # fechas dd/mm/yyyy hh:mm\n",
    "# ================================================================\n",
    "\n",
    "def _canon_col(s: str) -> str:\n",
    "    \"\"\"quita espacios/guiones/underscores y acentos, pasa a mayúsculas\"\"\"\n",
    "    s = unidecode(str(s))\n",
    "    s = re.sub(r\"[\\s\\-_\\.]+\", \"\", s)\n",
    "    return s.upper()\n",
    "\n",
    "def _find_sheet_with(like: str, xls: pd.ExcelFile) -> str | None:\n",
    "    like_c = _canon_col(like)\n",
    "    for sh in xls.sheet_names:\n",
    "        if like_c in _canon_col(sh):\n",
    "            return sh\n",
    "    return None\n",
    "\n",
    "def _map_cols(df: pd.DataFrame, expected: dict) -> dict:\n",
    "    \"\"\"\n",
    "    expected = {\"Variedad\":\"Variedad\", \"Identificador\":\"Identificador\", \"Inicio\":\"Inicio\", \"Fin\":\"Fin\", \"Sensor\":\"Sensor\", \"Descarte\":\"Descarte\", \"EnDuda\":\"En duda\"}\n",
    "    Devuelve {dest_col: actual_col_en_df}\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    canon_map = {_canon_col(c): c for c in df.columns}\n",
    "    for dest, want in expected.items():\n",
    "        key = _canon_col(want)\n",
    "        # buscar clave exacta canónica; si no, heurística por startswith\n",
    "        if key in canon_map:\n",
    "            mapping[dest] = canon_map[key]\n",
    "            continue\n",
    "        alt = next((canon_map[k] for k in canon_map if key in k or k in key), None)\n",
    "        if alt:\n",
    "            mapping[dest] = alt\n",
    "        else:\n",
    "            mapping[dest] = None\n",
    "    return mapping\n",
    "\n",
    "# ---------- 1) Leer consolidado (datos_wide) ----------\n",
    "wide = pd.read_excel(INPUT_WIDE_FILE, sheet_name=SHEET_WIDE)\n",
    "if \"timestamp\" not in wide.columns:\n",
    "    raise RuntimeError(\"La hoja datos_wide no tiene columna 'timestamp'.\")\n",
    "\n",
    "# ---------- 2) Leer laboratorio ----------\n",
    "xls = pd.ExcelFile(LAB_FILE)\n",
    "lab_sheet = SHEET_LAB or _find_sheet_with(\"Tach\", xls) or xls.sheet_names[0]\n",
    "lab_raw = pd.read_excel(xls, sheet_name=lab_sheet)\n",
    "\n",
    "# mapear columnas esperadas (robusto a variaciones menores de nombre)\n",
    "exp = {\n",
    "    \"Variedad\": \"Variedad\",\n",
    "    \"Identificador\": \"Identificador\",\n",
    "    \"Inicio\": \"Inicio\",\n",
    "    \"Fin\": \"Fin\",\n",
    "    \"Sensor\": \"Sensor\",\n",
    "    \"Descarte\": \"Descarte\",   # puede no existir (no es error)\n",
    "    \"EnDuda\": \"En duda\"       # puede no existir (no es error)\n",
    "}\n",
    "m = _map_cols(lab_raw, exp)\n",
    "\n",
    "missing = [k for k in [\"Variedad\",\"Identificador\",\"Inicio\",\"Fin\",\"Sensor\"] if m[k] is None]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan columnas esenciales en el laboratorio: {missing}\")\n",
    "\n",
    "lab = pd.DataFrame({\n",
    "    \"Variedad\": lab_raw[m[\"Variedad\"]],\n",
    "    \"ID_tachada\": lab_raw[m[\"Identificador\"]],\n",
    "    \"Inicio\": pd.to_datetime(lab_raw[m[\"Inicio\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"Fin\": pd.to_datetime(lab_raw[m[\"Fin\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"sensor_id\": pd.to_numeric(lab_raw[m[\"Sensor\"]], errors=\"coerce\").astype(\"Int64\"),\n",
    "})\n",
    "\n",
    "# opcionales (si existen)\n",
    "if m.get(\"Descarte\") and m[\"Descarte\"] in lab_raw.columns:\n",
    "    lab[\"Descarte\"] = lab_raw[m[\"Descarte\"]]\n",
    "else:\n",
    "    lab[\"Descarte\"] = np.nan\n",
    "\n",
    "if m.get(\"EnDuda\") and m[\"EnDuda\"] in lab_raw.columns:\n",
    "    lab[\"En duda\"] = lab_raw[m[\"EnDuda\"]]\n",
    "else:\n",
    "    lab[\"En duda\"] = np.nan\n",
    "\n",
    "# limpiar filas inválidas\n",
    "lab = lab.dropna(subset=[\"Inicio\",\"Fin\",\"sensor_id\"]).reset_index(drop=True)\n",
    "if lab.empty:\n",
    "    raise RuntimeError(\"El archivo de laboratorio quedó vacío tras limpieza (fechas o sensor inválidos).\")\n",
    "\n",
    "# ---------- 3) Normalizar tipos en wide ----------\n",
    "wide[\"timestamp\"] = pd.to_datetime(wide[\"timestamp\"], errors=\"coerce\")\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    # algunos wide pueden tener 'sensor' o 'Sensor'\n",
    "    for alt in [\"sensor\",\"Sensor\"]:\n",
    "        if alt in wide.columns:\n",
    "            wide.rename(columns={alt: \"sensor_id\"}, inplace=True)\n",
    "            break\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    raise RuntimeError(\"datos_wide no tiene columna 'sensor_id' (ni 'sensor'/'Sensor').\")\n",
    "\n",
    "# ---------- 4) Interval join por sensor ----------\n",
    "# Método eficiente: para cada sensor, buscar por búsqueda binaria en Inicios y validar con Fin\n",
    "annot = pd.DataFrame(index=wide.index, columns=[\"Variedad\",\"ID_tachada\",\"Descarte\",\"En duda\"])\n",
    "\n",
    "for sid, sub in wide.groupby(\"sensor_id\"):\n",
    "    if pd.isna(sid): \n",
    "        continue\n",
    "    L = lab[lab[\"sensor_id\"] == sid].sort_values(\"Inicio\").reset_index(drop=True)\n",
    "    if L.empty:\n",
    "        continue\n",
    "\n",
    "    starts = L[\"Inicio\"].values\n",
    "    ends   = L[\"Fin\"].values\n",
    "\n",
    "    # índices en wide a anotar\n",
    "    idx = sub.index\n",
    "    t   = sub[\"timestamp\"].values\n",
    "\n",
    "    # pos = última tachada cuyo Inicio <= t\n",
    "    pos = np.searchsorted(starts, t, side=\"right\") - 1\n",
    "    # inválidos si pos < 0\n",
    "    valid = (pos >= 0)\n",
    "    pos[~valid] = 0  # placeholder\n",
    "\n",
    "    # condición de estar dentro del intervalo [Inicio, Fin]\n",
    "    in_range = valid & (t <= ends[pos])\n",
    "\n",
    "    # crear arrays de anotaciones\n",
    "    variedad  = np.where(in_range, L.loc[pos, \"Variedad\"].values, None)\n",
    "    ident     = np.where(in_range, L.loc[pos, \"ID_tachada\"].values, None)\n",
    "    descarte  = np.where(in_range, L.loc[pos, \"Descarte\"].values, None)\n",
    "    enduda    = np.where(in_range, L.loc[pos, \"En duda\"].values, None)\n",
    "\n",
    "    annot.loc[idx, \"Variedad\"]   = variedad\n",
    "    annot.loc[idx, \"ID_tachada\"] = ident\n",
    "    annot.loc[idx, \"Descarte\"]   = descarte\n",
    "    annot.loc[idx, \"En duda\"]    = enduda\n",
    "\n",
    "# ---------- 5) Combinar y logs ----------\n",
    "wide_annot = pd.concat([wide, annot], axis=1)\n",
    "\n",
    "sin_match = wide_annot[wide_annot[\"Variedad\"].isna()][[\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]].copy() \\\n",
    "            if \"planta\" in wide_annot.columns else \\\n",
    "            wide_annot[wide_annot[\"Variedad\"].isna()][[\"sensor_id\",\"timestamp\"]].copy()\n",
    "sin_match_count = len(sin_match)\n",
    "\n",
    "qa = (wide_annot\n",
    "      .groupby([\"sensor_id\"], dropna=False)\n",
    "      .agg(registros=(\"timestamp\",\"size\"),\n",
    "           con_match=(\"Variedad\", lambda s: s.notna().sum()),\n",
    "           sin_match=(\"Variedad\", lambda s: s.isna().sum()))\n",
    "      .reset_index())\n",
    "\n",
    "print(f\"Filas anotadas: {len(wide_annot)} | sin match: {sin_match_count}\")\n",
    "\n",
    "# ---------- 6) Guardar ----------\n",
    "with pd.ExcelWriter(OUTPUT_PATH, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "    wide_annot.to_excel(w, sheet_name=\"datos_wide_anot\", index=False)\n",
    "    lab.to_excel(w, sheet_name=\"lab_usado\", index=False)\n",
    "    qa.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "    if sin_match_count:\n",
    "        sin_match.to_excel(w, sheet_name=\"log_sin_match\", index=False)\n",
    "\n",
    "print(\"Archivo tachadas:\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f609c8",
   "metadata": {},
   "source": [
    "# Identificación de tachadas para RB (por año)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fae247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TOL_MIN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 159>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Tolerancia de intervalos del lab en ±TOL_MIN\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     starts \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mto_datetime(L[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInicio\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(\u001b[43mTOL_MIN\u001b[49m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    170\u001b[0m     ends   \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mto_datetime(L[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFin\u001b[39m\u001b[38;5;124m\"\u001b[39m])    \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(TOL_MIN, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    172\u001b[0m     idx \u001b[38;5;241m=\u001b[39m sub\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TOL_MIN' is not defined"
     ]
    }
   ],
   "source": [
    "# === Anotación de tachadas para RB (por año) ===\n",
    "\n",
    "# ======== CONFIG – CAMBIAR ESTAS DOS RUTAS SEGÚN EL AÑO ========\n",
    "\n",
    "# RB 2025 (descomentar estas dos líneas cuando se quiera correr el de 2025):\n",
    "#INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2025.xlsx\"\n",
    "#LAB_FILE        = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\2025 Control Tachadas RB.xlsx\"\n",
    "\n",
    "# RB 2024 (descomentar estas dos líneas cuando se quiera correr el de 2024):\n",
    "INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2024.xlsx\"\n",
    "LAB_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\2024 Control Tachadas RB.xlsx\"\n",
    "\n",
    "OUTPUT_PATH = Path(INPUT_WIDE_FILE).with_name(Path(INPUT_WIDE_FILE).stem + \"_TACHADAS.xlsx\")\n",
    "SHEET_WIDE  = \"datos_wide\"   # hoja en tu consolidado\n",
    "SHEET_LAB   = None           # si la dejás en None, intentamos detectar por nombre (contenga \"Tach\")\n",
    "DAYFIRST    = True           # fechas dd/mm/yyyy hh:mm\n",
    "REQUIRE_SENSOR_MATCH = True  # cruce por sensor obligatorio (recomendado)\n",
    "# ================================================================\n",
    "\n",
    "def _canon_col(s: str) -> str:\n",
    "    \"\"\"quita acentos, espacios/guiones/underscores/puntos y pasa a mayúsculas\"\"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[\\s\\-_\\.]+\", \"\", s)\n",
    "    return s.upper()\n",
    "\n",
    "def _find_sheet_with(like: str, xls: pd.ExcelFile) -> str | None:\n",
    "    like_c = _canon_col(like)\n",
    "    for sh in xls.sheet_names:\n",
    "        if like_c in _canon_col(sh):\n",
    "            return sh\n",
    "    return None\n",
    "\n",
    "def _map_cols(df: pd.DataFrame, expected: dict) -> dict:\n",
    "    \"\"\"\n",
    "    expected = {\"Variedad\":\"Variedad\", \"Identificador\":\"Identificador\", \"Inicio\":\"Inicio\",\n",
    "                \"Fin\":\"Fin\", \"Sensor\":\"Sensor\",\n",
    "                \"Comentario\":\"Comentario\", \"ErrTemp\":\"Error entemperatura\", \"ErrHum\":\"Error enhumedad\"}\n",
    "    Devuelve {dest_col: actual_col_en_df (o None si no está)}\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    canon_map = {_canon_col(c): c for c in df.columns}\n",
    "    for dest, want in expected.items():\n",
    "        key = _canon_col(want)\n",
    "        if key in canon_map:\n",
    "            mapping[dest] = canon_map[key]\n",
    "            continue\n",
    "        # heurística: contiene / contenido\n",
    "        alt = next((canon_map[k] for k in canon_map if key in k or k in key), None)\n",
    "        mapping[dest] = alt\n",
    "    return mapping\n",
    "\n",
    "def _detect_header_row(df_preview: pd.DataFrame, expected_names: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Dado un DataFrame leído con header=None, devuelve el índice de la fila\n",
    "    que más se parece a la fila de encabezados (match por nombres esperados).\n",
    "    \"\"\"\n",
    "    exp_can = {_canon_col(x) for x in expected_names}\n",
    "    best_row, best_hits = 0, -1\n",
    "    # miramos, por ejemplo, las primeras 15 filas\n",
    "    max_rows = min(15, len(df_preview))\n",
    "    for r in range(max_rows):\n",
    "        vals = [str(v) for v in df_preview.iloc[r].tolist()]\n",
    "        can = {_canon_col(v) for v in vals if isinstance(v, (str, bytes))}\n",
    "        hits = len(exp_can & can)\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            best_row = r\n",
    "    return best_row\n",
    "\n",
    "\n",
    "# ---------- 1) Leer consolidado (datos_wide) ----------\n",
    "wide = pd.read_excel(INPUT_WIDE_FILE, sheet_name=SHEET_WIDE)\n",
    "if \"timestamp\" not in wide.columns:\n",
    "    raise RuntimeError(\"La hoja datos_wide no tiene columna 'timestamp'.\")\n",
    "\n",
    "# Normalizar tipos básicos\n",
    "wide[\"timestamp\"] = pd.to_datetime(wide[\"timestamp\"], errors=\"coerce\")\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    for alt in [\"sensor\",\"Sensor\"]:\n",
    "        if alt in wide.columns:\n",
    "            wide.rename(columns={alt: \"sensor_id\"}, inplace=True)\n",
    "            break\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    raise RuntimeError(\"datos_wide no tiene columna 'sensor_id' (ni 'sensor'/'Sensor').\")\n",
    "\n",
    "# ---------- 2) Leer laboratorio ----------\n",
    "xls = pd.ExcelFile(LAB_FILE)\n",
    "lab_sheet = SHEET_LAB or _find_sheet_with(\"Tach\", xls) or xls.sheet_names[0]\n",
    "\n",
    "# --- leer laboratorio detectando la fila de encabezados ---\n",
    "# primero sin encabezado para detectar en qué fila están\n",
    "lab_preview = pd.read_excel(xls, sheet_name=lab_sheet, header=None)\n",
    "header_row = _detect_header_row(\n",
    "    lab_preview,\n",
    "    expected_names=[\"Variedad\",\"Identificador\",\"Inicio\",\"Fin\",\"Sensor\",\n",
    "                    \"Comentario\",\"Error entemperatura\",\"Error enhumedad\"]\n",
    ")\n",
    "# ahora leemos con ese header\n",
    "lab_raw = pd.read_excel(xls, sheet_name=lab_sheet, header=header_row)\n",
    "\n",
    "# Mapear columnas (robusto a variaciones de nombre)\n",
    "exp = {\n",
    "    \"Variedad\": \"Variedad\",\n",
    "    \"Identificador\": \"Identificador\",\n",
    "    \"Inicio\": \"Inicio\",\n",
    "    \"Fin\": \"Fin\",\n",
    "    \"Sensor\": \"Sensor\",\n",
    "    # opcionales en RB 2025:\n",
    "    \"Comentario\": \"Comentario\",\n",
    "    \"ErrTemp\": \"Error entemperatura\",\n",
    "    \"ErrHum\": \"Error enhumedad\",\n",
    "}\n",
    "m = _map_cols(lab_raw, exp)\n",
    "\n",
    "missing = [k for k in [\"Variedad\",\"Identificador\",\"Inicio\",\"Fin\",\"Sensor\"] if m[k] is None]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan columnas esenciales en el laboratorio: {missing}\")\n",
    "\n",
    "lab = pd.DataFrame({\n",
    "    \"Variedad\": lab_raw[m[\"Variedad\"]],\n",
    "    \"ID_tachada\": lab_raw[m[\"Identificador\"]],\n",
    "    \"Inicio\": pd.to_datetime(lab_raw[m[\"Inicio\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"Fin\": pd.to_datetime(lab_raw[m[\"Fin\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"sensor_id\": pd.to_numeric(lab_raw[m[\"Sensor\"]], errors=\"coerce\").astype(\"Int64\"),\n",
    "})\n",
    "\n",
    "# columnas opcionales (si existen)\n",
    "if m.get(\"Comentario\") and m[\"Comentario\"] in lab_raw.columns:\n",
    "    lab[\"Comentario\"] = lab_raw[m[\"Comentario\"]]\n",
    "else:\n",
    "    lab[\"Comentario\"] = np.nan\n",
    "\n",
    "if m.get(\"ErrTemp\") and m[\"ErrTemp\"] in lab_raw.columns:\n",
    "    lab[\"Error entemperatura\"] = lab_raw[m[\"ErrTemp\"]]\n",
    "else:\n",
    "    lab[\"Error entemperatura\"] = np.nan\n",
    "\n",
    "if m.get(\"ErrHum\") and m[\"ErrHum\"] in lab_raw.columns:\n",
    "    lab[\"Error enhumedad\"] = lab_raw[m[\"ErrHum\"]]\n",
    "else:\n",
    "    lab[\"Error enhumedad\"] = np.nan\n",
    "\n",
    "# limpiar filas inválidas\n",
    "lab = lab.dropna(subset=[\"Inicio\",\"Fin\",\"sensor_id\"]).reset_index(drop=True)\n",
    "if lab.empty:\n",
    "    raise RuntimeError(\"El archivo de laboratorio quedó vacío tras limpieza (fechas o sensor inválidos).\")\n",
    "\n",
    "# ---------- 3) Interval join [Inicio, Fin] por sensor ----------\n",
    "annot_cols = [\"Variedad\",\"ID_tachada\",\"Comentario\",\"Error entemperatura\",\"Error enhumedad\"]\n",
    "annot = pd.DataFrame(index=wide.index, columns=annot_cols)\n",
    "\n",
    "if REQUIRE_SENSOR_MATCH:\n",
    "    grupos = wide.groupby(\"sensor_id\")\n",
    "else:\n",
    "    # ⚠️ modo emergencia: ignora sensor (solo por tiempo)\n",
    "    grupos = [(-1, wide)]\n",
    "\n",
    "for sid, sub in grupos:\n",
    "    if REQUIRE_SENSOR_MATCH:\n",
    "        L = lab[lab[\"sensor_id\"] == sid].sort_values(\"Inicio\").reset_index(drop=True)\n",
    "    else:\n",
    "        L = lab.sort_values(\"Inicio\").reset_index(drop=True)\n",
    "\n",
    "    if L.empty:\n",
    "        continue\n",
    "\n",
    "    starts = L[\"Inicio\"].values\n",
    "    ends   = L[\"Fin\"].values\n",
    "    idx = sub.index\n",
    "    t   = sub[\"timestamp\"].values\n",
    "\n",
    "    # última fila de lab con Inicio <= t\n",
    "    pos = np.searchsorted(starts, t, side=\"right\") - 1\n",
    "    valid = (pos >= 0)\n",
    "    pos[~valid] = 0  # placeholder\n",
    "    in_range = valid & (t <= ends[pos])\n",
    "\n",
    "    annot.loc[idx, \"Variedad\"]             = np.where(in_range, L.loc[pos, \"Variedad\"].values, None)\n",
    "    annot.loc[idx, \"ID_tachada\"]           = np.where(in_range, L.loc[pos, \"ID_tachada\"].values, None)\n",
    "    annot.loc[idx, \"Comentario\"]           = np.where(in_range, L.loc[pos, \"Comentario\"].values, None)\n",
    "    annot.loc[idx, \"Error entemperatura\"]  = np.where(in_range, L.loc[pos, \"Error entemperatura\"].values, None)\n",
    "    annot.loc[idx, \"Error enhumedad\"]      = np.where(in_range, L.loc[pos, \"Error enhumedad\"].values, None)\n",
    "\n",
    "# ---------- 4) Combinar y logs ----------\n",
    "wide_annot = pd.concat([wide, annot], axis=1)\n",
    "\n",
    "cols_base = [\"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\"]\n",
    "sin_match_cols = [c for c in cols_base if c in wide_annot.columns]\n",
    "sin_match = wide_annot[wide_annot[\"Variedad\"].isna()][sin_match_cols].copy()\n",
    "sin_match_count = len(sin_match)\n",
    "\n",
    "qa = (wide_annot\n",
    "      .groupby([\"sensor_id\"], dropna=False)\n",
    "      .agg(registros=(\"timestamp\",\"size\"),\n",
    "           con_match=(\"Variedad\", lambda s: s.notna().sum()),\n",
    "           sin_match=(\"Variedad\", lambda s: s.isna().sum()))\n",
    "      .reset_index())\n",
    "\n",
    "print(f\"Filas anotadas: {len(wide_annot)} | sin match: {sin_match_count}\")\n",
    "\n",
    "# ---------- 5) Guardar ----------\n",
    "with pd.ExcelWriter(OUTPUT_PATH, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "    wide_annot.to_excel(w, sheet_name=\"datos_wide_anot\", index=False)\n",
    "    lab.to_excel(w, sheet_name=\"lab_usado\", index=False)\n",
    "    qa.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "    if sin_match_count:\n",
    "        sin_match.to_excel(w, sheet_name=\"log_sin_match\", index=False)\n",
    "\n",
    "print(\"Archivo anotado:\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a1fe1",
   "metadata": {},
   "source": [
    "# Identificación de tachadas para RB (por año, con +1 en sensor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df22336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste aplicado: sensor_id remapeado (1→2, 2→3, 3→4, 4→5) en datos de sensores RB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste aplicado: sensor_id remapeado (1→2, 2→3, 3→4, 4→5) en laboratorio RB 2025.\n",
      "Filas anotadas: 87128 | sin match: 39664\n",
      "Archivo anotado creado en: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2025_TACHADAS.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Anotación de tachadas para RB (por año, +1 sensores SIEMPRE y +1 lab SOLO si es 2025) ===\n",
    "\n",
    "# ======== CONFIG ========\n",
    "# RB 2025 :\n",
    "INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2025.xlsx\"\n",
    "LAB_FILE        = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\REVISADO UM Control Tachadas Secado RB 2025.xlsx\"\n",
    "\n",
    "# RB 2024 (descomentar cuando se quiera correr):\n",
    "#INPUT_WIDE_FILE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\Consolidado_Sensores_RB_2024.xlsx\"\n",
    "#LAB_FILE        = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\2024 Control Tachadas RB.xlsx\"\n",
    "\n",
    "OUTPUT_PATH = Path(INPUT_WIDE_FILE).with_name(Path(INPUT_WIDE_FILE).stem + \"_TACHADAS.xlsx\")\n",
    "SHEET_WIDE  = \"datos_wide\"\n",
    "SHEET_LAB   = None\n",
    "DAYFIRST    = True\n",
    "REQUIRE_SENSOR_MATCH = True\n",
    "\n",
    "# Overrides opcionales (dejar en None para auto):\n",
    "APPLY_PLUS1_LAB     = None          # None = auto: +1 si LAB_FILE contiene \"2025\"\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def _canon_col(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[\\s\\-_\\.]+\", \"\", s)\n",
    "    return s.upper()\n",
    "\n",
    "def _find_sheet_with(like: str, xls: pd.ExcelFile) -> str | None:\n",
    "    like_c = _canon_col(like)\n",
    "    for sh in xls.sheet_names:\n",
    "        if like_c in _canon_col(sh):\n",
    "            return sh\n",
    "    return None\n",
    "\n",
    "def _map_cols(df: pd.DataFrame, expected: dict) -> dict:\n",
    "    mapping = {}\n",
    "    canon_map = {_canon_col(c): c for c in df.columns}\n",
    "    for dest, want in expected.items():\n",
    "        key = _canon_col(want)\n",
    "        if key in canon_map:\n",
    "            mapping[dest] = canon_map[key]\n",
    "            continue\n",
    "        alt = next((canon_map[k] for k in canon_map if key in k or k in key), None)\n",
    "        mapping[dest] = alt\n",
    "    return mapping\n",
    "\n",
    "def _detect_header_row(df_preview: pd.DataFrame, expected_names: list[str]) -> int:\n",
    "    exp_can = {_canon_col(x) for x in expected_names}\n",
    "    best_row, best_hits = 0, -1\n",
    "    max_rows = min(15, len(df_preview))\n",
    "    for r in range(max_rows):\n",
    "        vals = [str(v) for v in df_preview.iloc[r].tolist()]\n",
    "        can = {_canon_col(v) for v in vals if isinstance(v, (str, bytes))}\n",
    "        hits = len(exp_can & can)\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            best_row = r\n",
    "    return best_row\n",
    "\n",
    "\n",
    "# ---------- 1) Leer consolidado ----------\n",
    "wide = pd.read_excel(INPUT_WIDE_FILE, sheet_name=SHEET_WIDE)\n",
    "wide[\"timestamp\"] = pd.to_datetime(wide[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    for alt in [\"sensor\", \"Sensor\"]:\n",
    "        if alt in wide.columns:\n",
    "            wide.rename(columns={alt: \"sensor_id\"}, inplace=True)\n",
    "            break\n",
    "if \"sensor_id\" not in wide.columns:\n",
    "    raise RuntimeError(\"datos_wide no tiene columna sensor_id.\")\n",
    "\n",
    "# Ajuste sensores RB: mapear 1→2, 2→3, 3→4, 4→5 (sin tocar otros valores)\n",
    "MAP_SENS = {1: 2, 2: 3, 3: 4, 4: 5}\n",
    "wide[\"sensor_id\"] = pd.to_numeric(wide[\"sensor_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "wide[\"sensor_id\"] = wide[\"sensor_id\"].map(MAP_SENS).fillna(wide[\"sensor_id\"]).astype(\"Int64\")\n",
    "print(\"Ajuste aplicado: sensor_id remapeado (1→2, 2→3, 3→4, 4→5) en datos de sensores RB.\")\n",
    "\n",
    "\n",
    "# ---------- 2) Leer laboratorio ----------\n",
    "xls = pd.ExcelFile(LAB_FILE)\n",
    "lab_sheet = SHEET_LAB or _find_sheet_with(\"Tach\", xls) or xls.sheet_names[0]\n",
    "\n",
    "# detectar fila de encabezado y leer\n",
    "lab_preview = pd.read_excel(xls, sheet_name=lab_sheet, header=None)\n",
    "header_row = _detect_header_row(\n",
    "    lab_preview,\n",
    "    expected_names=[\"Variedad\",\"Identificador\",\"Inicio\",\"Fin\",\"Sensor\",\n",
    "                    \"Comentario\",\"Error entemperatura\",\"Error enhumedad\"]\n",
    ")\n",
    "lab_raw = pd.read_excel(xls, sheet_name=lab_sheet, header=header_row)\n",
    "\n",
    "# mapear columnas\n",
    "exp = {\n",
    "    \"Variedad\": \"Variedad\",\n",
    "    \"Identificador\": \"Identificador\",\n",
    "    \"Inicio\": \"Inicio\",\n",
    "    \"Fin\": \"Fin\",\n",
    "    \"Sensor\": \"Sensor\",\n",
    "    \"Comentario\": \"Comentario\",\n",
    "    \"ErrTemp\": \"Error entemperatura\",\n",
    "    \"ErrHum\": \"Error enhumedad\",\n",
    "    \"LAB_DESCARTE\": \"DESCARTE\",\n",
    "    \"LAB_EN_DUDA\": \"ENDUDA\"\n",
    "}\n",
    "m = _map_cols(lab_raw, exp)\n",
    "missing = [k for k in [\"Variedad\",\"Identificador\",\"Inicio\",\"Fin\",\"Sensor\"] if m[k] is None]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan columnas esenciales en el laboratorio: {missing}\")\n",
    "\n",
    "lab = pd.DataFrame({\n",
    "    \"Variedad\": lab_raw[m[\"Variedad\"]],\n",
    "    \"ID_tachada\": lab_raw[m[\"Identificador\"]],\n",
    "    \"Inicio\": pd.to_datetime(lab_raw[m[\"Inicio\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"Fin\": pd.to_datetime(lab_raw[m[\"Fin\"]], dayfirst=DAYFIRST, errors=\"coerce\"),\n",
    "    \"sensor_id\": pd.to_numeric(lab_raw[m[\"Sensor\"]], errors=\"coerce\").astype(\"Int64\"),\n",
    "})\n",
    "\n",
    "# --- Estandarizar flags del LAB a \"Descarte\" y \"En duda\" ---\n",
    "for out_col, in_key in [(\"Descarte\", \"LAB_DESCARTE\"), (\"En duda\", \"LAB_EN_DUDA\")]:\n",
    "    src = m.get(in_key)\n",
    "    if src and src in lab_raw.columns:\n",
    "        s = lab_raw[src]\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "        flag = (sn == 1)\n",
    "        sval = s.astype(str).str.strip().str.casefold()\n",
    "        flag = flag | sval.isin({\"true\", \"si\", \"sí\", \"x\"})\n",
    "        lab[out_col] = flag.astype(\"int8\")\n",
    "    else:\n",
    "        lab[out_col] = np.nan  # si ese año no trae estas columnas\n",
    "\n",
    "# columnas opcionales\n",
    "for col_out, col_in in [\n",
    "    (\"Comentario\", \"Comentario\"),\n",
    "    (\"Error entemperatura\", \"ErrTemp\"),\n",
    "    (\"Error enhumedad\", \"ErrHum\"),\n",
    "]:\n",
    "    if m.get(col_in) and m[col_in] in lab_raw.columns:\n",
    "        lab[col_out] = lab_raw[m[col_in]]\n",
    "    else:\n",
    "        lab[col_out] = np.nan\n",
    "\n",
    "lab = lab.dropna(subset=[\"Inicio\",\"Fin\",\"sensor_id\"]).reset_index(drop=True)\n",
    "\n",
    "# --- decidir si aplicar +1 al laboratorio SOLO según el nombre del archivo ---\n",
    "# (ej.: \"2025 Control Tachadas.xlsx\" → 2025; \"2024 Control Tachadas.xlsx\" → 2024)\n",
    "\n",
    "if APPLY_PLUS1_LAB is None:\n",
    "    basename = Path(LAB_FILE).name  # solo el nombre, sin la carpeta\n",
    "    m = re.search(r'\\b(20\\d{2})\\b', basename)\n",
    "    lab_year = int(m.group(1)) if m else None\n",
    "    apply_lab = (lab_year == 2025)\n",
    "else:\n",
    "    apply_lab = bool(APPLY_PLUS1_LAB)\n",
    "\n",
    "if apply_lab:\n",
    "    lab[\"sensor_id\"] = lab[\"sensor_id\"].map(MAP_SENS).fillna(lab[\"sensor_id\"]).astype(\"Int64\")\n",
    "    print(\"Ajuste aplicado: sensor_id remapeado (1→2, 2→3, 3→4, 4→5) en laboratorio RB 2025.\")\n",
    "else:\n",
    "    print(\"Ajuste +1 laboratorio: NO aplicado.\")\n",
    "\n",
    "\n",
    "# ---------- 3) Interval join ----------\n",
    "annot_cols = [\"Variedad\",\"ID_tachada\",\"Comentario\",\n",
    "              \"Error entemperatura\",\"Error enhumedad\",\n",
    "              \"Descarte\",\"En duda\"]\n",
    "annot = pd.DataFrame(index=wide.index, columns=annot_cols)\n",
    "\n",
    "if REQUIRE_SENSOR_MATCH:\n",
    "    grupos = wide.groupby(\"sensor_id\")\n",
    "else:\n",
    "    grupos = [(-1, wide)]\n",
    "\n",
    "for sid, sub in grupos:\n",
    "    if REQUIRE_SENSOR_MATCH:\n",
    "        L = lab[lab[\"sensor_id\"] == sid].sort_values(\"Inicio\").reset_index(drop=True)\n",
    "    else:\n",
    "        L = lab.sort_values(\"Inicio\").reset_index(drop=True)\n",
    "    if L.empty:\n",
    "        continue\n",
    "\n",
    "    starts = L[\"Inicio\"].values\n",
    "    ends   = L[\"Fin\"].values\n",
    "    idx = sub.index\n",
    "    t   = sub[\"timestamp\"].values\n",
    "    pos = np.searchsorted(starts, t, side=\"right\") - 1\n",
    "    valid = (pos >= 0)\n",
    "    pos[~valid] = 0\n",
    "    in_range = valid & (t <= ends[pos])\n",
    "\n",
    "    for col in annot_cols:\n",
    "        annot.loc[idx, col] = np.where(in_range, L.loc[pos, col].values, None)\n",
    "\n",
    "# ---------- 4) Combinar, QA y guardar ----------\n",
    "wide_annot = pd.concat([wide, annot], axis=1)\n",
    "sin_match = wide_annot[wide_annot[\"Variedad\"].isna()]\n",
    "qa = (wide_annot\n",
    "      .groupby([\"sensor_id\"], dropna=False)\n",
    "      .agg(registros=(\"timestamp\",\"size\"),\n",
    "           con_match=(\"Variedad\", lambda s: s.notna().sum()),\n",
    "           sin_match=(\"Variedad\", lambda s: s.isna().sum()))\n",
    "      .reset_index())\n",
    "\n",
    "# resumen consola\n",
    "sin_match_count = len(sin_match)\n",
    "print(f\"Filas anotadas: {len(wide_annot)} | sin match: {sin_match_count}\")\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_PATH, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd HH:MM:SS\") as w:\n",
    "    wide_annot.to_excel(w, sheet_name=\"datos_wide_anot\", index=False)\n",
    "    lab.to_excel(w, sheet_name=\"lab_usado\", index=False)\n",
    "    qa.to_excel(w, sheet_name=\"qa_resumen\", index=False)\n",
    "    if sin_match_count:\n",
    "        sin_match.to_excel(w, sheet_name=\"log_sin_match\", index=False)\n",
    "\n",
    "print(\"Archivo anotado creado en:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faceadf",
   "metadata": {},
   "source": [
    "# Limpieza Tachadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b95be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Procesando archivos de RB ---\n",
      "Procesando: Consolidado_Sensores_RB_2024_TACHADAS.xlsx\n",
      "  → DESCARTAR=1 en 13902 filas\n",
      "  ✅ Guardado: Consolidado_Sensores_RB_2024_TACHADAS_LIMPIAS.xlsx\n",
      "Procesando: Consolidado_Sensores_RB_2025_TACHADAS.xlsx\n",
      "  → DESCARTAR=1 en 8464 filas\n",
      "  ✅ Guardado: Consolidado_Sensores_RB_2025_TACHADAS_LIMPIAS.xlsx\n",
      "--- Limpieza finalizada para RB ---\n",
      "\n",
      "✅ Proceso completado para todas las plantas y años.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACIÓN DE RUTAS ---\n",
    "carpeta_jpv = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\"\n",
    "carpeta_rb  = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\"\n",
    "\n",
    "# Columnas a conservar en cada planta (incluye DESCARTAR al final)\n",
    "cols_rb = [\n",
    "    \"planta\", \"año\", \"tirada_fecha\", \"sensor_id\", \"timestamp\",\n",
    "    \"Date_raw\", \"LOC_time_raw\", \"VOLT_HUM\", \"VOLT_TEM\", \"Variedad\",\n",
    "    \"ID_tachada\",\"DESCARTE\",\"ENDUDA\",\"DESCARTAR\"\n",
    "]\n",
    "\n",
    "cols_jpv = [\n",
    "    \"planta\", \"año\", \"tirada_fecha\", \"sensor_id\", \"timestamp\",\n",
    "    \"tirada_num\", \"VOLT_HUM\", \"VOLT_TEM\", \"Variedad\",\n",
    "    \"ID_tachada\",\"Descarte\",\"En duda\",\"DESCARTAR\"\n",
    "]\n",
    "\n",
    "# --------- Helpers de normalización ---------\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normaliza texto: sin acentos, minúscula, sin espacios/guiones/underscores/puntos.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = ''.join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c)).casefold()\n",
    "    s = re.sub(r\"[\\s_\\-\\.]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def _to_bool01_series(df: pd.DataFrame, colname_candidates) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Devuelve Serie booleana (True si valor==1 o {true, si, sí, x}) para la primera\n",
    "    columna encontrada (match por nombre normalizado). Si ninguna existe, retorna todo False.\n",
    "    \"\"\"\n",
    "    norm_map = {_norm(c): c for c in df.columns}\n",
    "    chosen = None\n",
    "    for name in colname_candidates:\n",
    "        key = _norm(name)\n",
    "        if key in norm_map:\n",
    "            chosen = norm_map[key]\n",
    "            break\n",
    "\n",
    "    if chosen is None:\n",
    "        # columna faltante → todo False\n",
    "        return pd.Series(False, index=df.index)\n",
    "\n",
    "    s = df[chosen]\n",
    "\n",
    "    # Intento numérico (1 = True)\n",
    "    snum = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = (snum == 1)\n",
    "\n",
    "    # Para no numéricos, acepto \"true\", \"si\", \"sí\", \"x\"\n",
    "    sval = s.astype(str).str.strip().str.casefold()\n",
    "    out = out | sval.isin({\"true\", \"si\", \"sí\", \"x\"})\n",
    "    return out.fillna(False)\n",
    "\n",
    "# --- FUNCIÓN DE PROCESAMIENTO ---\n",
    "def limpiar_tachadas(carpeta, tipo_planta):\n",
    "    print(f\"\\n--- Procesando archivos de {tipo_planta} ---\")\n",
    "\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        if archivo.endswith(\"_TACHADAS.xlsx\"):\n",
    "            ruta = os.path.join(carpeta, archivo)\n",
    "            print(f\"Procesando: {archivo}\")\n",
    "\n",
    "            xls = pd.ExcelFile(ruta)\n",
    "            if \"datos_wide_anot\" not in xls.sheet_names:\n",
    "                print(f\"  ⚠️  Hoja 'datos_wide_anot' no encontrada en {archivo}\")\n",
    "                continue\n",
    "\n",
    "            # Leer hoja\n",
    "            df = pd.read_excel(xls, sheet_name=\"datos_wide_anot\")\n",
    "\n",
    "            # ---- construir DESCARTAR a partir de Descarte / En duda (robusto) ----\n",
    "            desc_candidates   = [\"Descarte\", \"DESCARTE\"]\n",
    "            enduda_candidates = [\"En duda\", \"En Duda\", \"ENDUDA\", \"EN_DUDA\", \"en_duda\"]\n",
    "\n",
    "            desc   = _to_bool01_series(df, desc_candidates)\n",
    "            enduda = _to_bool01_series(df, enduda_candidates)\n",
    "\n",
    "            df[\"DESCARTAR\"] = (desc | enduda).astype(\"int8\")\n",
    "            print(f\"  → DESCARTAR=1 en {int(df['DESCARTAR'].sum())} filas\")\n",
    "            # ---- fin agregado ----\n",
    "\n",
    "            # Elegir columnas correctas según planta\n",
    "            if tipo_planta == \"RB\":\n",
    "                cols_deseadas = cols_rb\n",
    "            else:\n",
    "                cols_deseadas = cols_jpv\n",
    "\n",
    "            # Mantener solo columnas existentes (y en el orden pedido)\n",
    "            columnas_validas = [c for c in cols_deseadas if c in df.columns]\n",
    "            df = df[columnas_validas]\n",
    "\n",
    "            # Dropear filas sin match de tachada\n",
    "            if \"ID_tachada\" in df.columns:\n",
    "                df = df.dropna(subset=[\"ID_tachada\"])\n",
    "            else:\n",
    "                print(\"  ⚠️  Columna 'ID_tachada' no existe; no puedo filtrar tachadas.\")\n",
    "                # si quisieras, podrías continuar sin dropear\n",
    "\n",
    "            # Guardar archivo limpio\n",
    "            nombre_salida = archivo.replace(\"_TACHADAS.xlsx\", \"_TACHADAS_LIMPIAS.xlsx\")\n",
    "            ruta_salida = os.path.join(carpeta, nombre_salida)\n",
    "\n",
    "            df.to_excel(ruta_salida, index=False)\n",
    "            print(f\"  ✅ Guardado: {nombre_salida}\")\n",
    "\n",
    "    print(f\"--- Limpieza finalizada para {tipo_planta} ---\\n\")\n",
    "\n",
    "\n",
    "# --- EJECUCIÓN ---\n",
    "#limpiar_tachadas(carpeta_jpv, \"JPV\")\n",
    "limpiar_tachadas(carpeta_rb,  \"RB\")\n",
    "\n",
    "print(\"✅ Proceso completado para todas las plantas y años.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7dfb5d",
   "metadata": {},
   "source": [
    "# Validación de Tachadas Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1889d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validando RB ---\n",
      "\n",
      "📄 Consolidado_Sensores_RB_2024_TACHADAS_LIMPIAS.xlsx  |  Año: 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\augus\\AppData\\Local\\Temp\\ipykernel_5352\\3235998358.py:233: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pd.Series(sobrantes_en_limpios, name=COL_ID_LIMPIOS).to_frame().to_excel(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Únicos LAB (control, sin dups): 480\n",
      "  Únicos LIMPIOS:                 269\n",
      "  ➖ En LAB y faltan en LIMPIOS:  211\n",
      "  ➕ En LIMPIOS y no en LAB:      0\n",
      "     Ejemplos faltantes: ['18801', '18802', '18803', '18804', '18805', '18807', '18808', '18809', '18810', '18811']\n",
      "  💾 Validación exportada: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\validación_tachadas_identificadas\\Consolidado_Sensores_RB_2024_TACHADAS_LIMPIAS_VALIDACION_TACHADAS.xlsx\n",
      "\n",
      "📄 Consolidado_Sensores_RB_2025_TACHADAS_LIMPIAS.xlsx  |  Año: 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\Users\\augus\\anaconda\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Únicos LAB (control, sin dups): 404\n",
      "  Únicos LIMPIOS:                 357\n",
      "  ➖ En LAB y faltan en LIMPIOS:  47\n",
      "  ➕ En LIMPIOS y no en LAB:      0\n",
      "     Ejemplos faltantes: ['1111', '17800', '18613', '18697', '18709', '18747', '18748', '18749', '18750', '18751']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\AppData\\Local\\Temp\\ipykernel_5352\\3235998358.py:233: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pd.Series(sobrantes_en_limpios, name=COL_ID_LIMPIOS).to_frame().to_excel(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  💾 Validación exportada: C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\validación_tachadas_identificadas\\Consolidado_Sensores_RB_2025_TACHADAS_LIMPIAS_VALIDACION_TACHADAS.xlsx\n",
      "\n",
      "✅ Validación finalizada.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#   CONFIGURACIÓN\n",
    "# =========================\n",
    "\n",
    "# Carpetas donde están los archivos LIMPIOS por planta\n",
    "carpeta_jpv = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\"\n",
    "carpeta_rb  = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\"\n",
    "\n",
    "# Carpeta de salida para VALIDACIONES\n",
    "validacion_dir_jpv = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\JPV_por_año\\validación_tachadas_identificadas\"\n",
    "validacion_dir_rb  = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\RB_por_año\\validación_tachadas_identificadas\"\n",
    "\n",
    "# Diccionarios: año -> lista de rutas a archivos de \"Control Tachadas\" del LABORATORIO\n",
    "# Cambiar por rutas correspondientes de Control Tachadas para cada año\n",
    "CONTROL_TACHADAS_JPV = {\n",
    "    \"2024\": [\n",
    "        r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\\REVISADO UM Control Tachadas Secado JPV 2024.xlsx\"\n",
    "    ],\n",
    "    \"2025\": [\n",
    "        r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos JPV 2024 2025\\REVISADO UM Control Tachadas Secado JPV 2025.xlsx\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "CONTROL_TACHADAS_RB = {\n",
    "    \"2024\": [\n",
    "        r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\2024 Control Tachadas RB.xlsx\"\n",
    "    ],\n",
    "    \"2025\": [\n",
    "        r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\Datos RB 2024 2025\\REVISADO UM Control Tachadas Secado RB 2025.xlsx\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Nombre de columnas a usar\n",
    "COL_ID_LIMPIOS = \"ID_tachada\"\n",
    "COL_ID_LAB     = \"Identificador\"\n",
    "\n",
    "\n",
    "# Nombre de hoja a usar en LAB (si \"Control Tachadas\" tienen una hoja específica, poner aquí;\n",
    "# si es None, se leerá la PRIMERA hoja de cada archivo de control)\n",
    "\n",
    "HOJA_CONTROL_TACHADAS = None\n",
    "\n",
    "# Exportar un Excel de validación por archivo?\n",
    "EXPORTAR_VALIDACION = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "#   HELPERS\n",
    "# =========================\n",
    "\n",
    "NUMERIC_PATTERN = re.compile(r'^\\s*\\d+(?:[.,]\\d+)?\\s*$')  # solo números (permite \".0\" o \",0\")\n",
    "\n",
    "def normalizar_id(x: str) -> str | None:\n",
    "    \"\"\" Normalización conservadora (numéricos → enteros como str; alfanuméricos se conservan) \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "    if NUMERIC_PATTERN.match(s):\n",
    "        s = s.replace(\",\", \".\")\n",
    "        try:\n",
    "            f = float(s)\n",
    "            i = int(f)\n",
    "            return str(i) if f == i else str(f)\n",
    "        except ValueError:\n",
    "            return s\n",
    "    else:\n",
    "        return re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    \n",
    "    \n",
    "def _encontrar_fila_header(path: str, sheet_name=None, key_header: str = \"Identificador\", max_probe_rows: int = 40):\n",
    "    \"\"\"\n",
    "    Escanea las primeras filas de la hoja para encontrar en qué fila aparece el texto 'Identificador'\n",
    "    (insensible a mayúsculas/espacios). Devuelve el índice 0-based para usar como 'header' en read_excel.\n",
    "    Si no lo encuentra, devuelve 0 (primera fila como cabecera).\n",
    "    \"\"\"\n",
    "    probe = pd.read_excel(path, sheet_name=sheet_name, header=None, nrows=max_probe_rows, dtype=str)\n",
    "    key = str(key_header).strip().upper()\n",
    "    for i in range(len(probe)):\n",
    "        row = probe.iloc[i].astype(str).str.strip().str.upper()\n",
    "        if row.eq(key).any():\n",
    "            return i\n",
    "    return 0\n",
    "\n",
    "\n",
    "def cargar_df_control_tachadas(rutas_archivos: list[str], hoja: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee una lista de archivos de 'Control Tachadas' del LAB y concatena TODAS SUS COLUMNAS.\n",
    "    Detecta dinámicamente la fila de cabecera (útil para RB donde la cabecera arranca en la fila 4, etc.).\n",
    "    Si 'hoja' es None, usa la primera hoja del archivo.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for path in rutas_archivos:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  ⚠️ No existe archivo de Control Tachadas: {path}\")\n",
    "            continue\n",
    "        try:\n",
    "            # Determinar hoja a leer\n",
    "            if hoja is None:\n",
    "                xl = pd.ExcelFile(path)\n",
    "                sheet_name = xl.sheet_names[0]\n",
    "            else:\n",
    "                sheet_name = hoja\n",
    "\n",
    "            # Detectar fila de cabecera buscando 'Identificador'\n",
    "            header_row = _encontrar_fila_header(path, sheet_name=sheet_name, key_header=COL_ID_LAB)\n",
    "\n",
    "            # Leer con la fila detectada como cabecera\n",
    "            df = pd.read_excel(path, sheet_name=sheet_name, header=header_row)\n",
    "\n",
    "            # Limpieza suave: quitar columnas totalmente vacías\n",
    "            df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error leyendo Control Tachadas '{os.path.basename(path)}': {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "def normalizar_y_unicos(serie: pd.Series) -> pd.Series:\n",
    "    s = serie.dropna().map(normalizar_id)\n",
    "    s = s.dropna()\n",
    "    s = s[s.astype(str).str.len() > 0].astype(str)\n",
    "    return pd.Series(pd.unique(s), dtype=str)\n",
    "\n",
    "def archivos_limpios_en_carpeta(carpeta: str) -> list[str]:\n",
    "    if not os.path.isdir(carpeta):\n",
    "        return []\n",
    "    return [os.path.join(carpeta, f) for f in os.listdir(carpeta) if f.endswith(\"_TACHADAS_LIMPIAS.xlsx\")]\n",
    "\n",
    "def obtener_año_desde_nombre(nombre: str) -> str | None:\n",
    "    m = re.search(r\"(20\\d{2})\", nombre)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "# =========================\n",
    "#   VALIDACIÓN\n",
    "# =========================\n",
    "\n",
    "def validar_planta(carpeta_limpios: str, controles_por_año: dict[str, list[str]], etiqueta_planta: str, carpeta_salida: str):\n",
    "    print(f\"\\n--- Validando {etiqueta_planta} ---\")\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "    limpios = archivos_limpios_en_carpeta(carpeta_limpios)\n",
    "    if not limpios:\n",
    "        print(\"  (No se encontraron *_TACHADAS_LIMPIAS.xlsx aquí)\")\n",
    "        return\n",
    "\n",
    "    for ruta_limpio in limpios:\n",
    "        nombre = os.path.basename(ruta_limpio)\n",
    "        año = obtener_año_desde_nombre(nombre)\n",
    "        if año is None:\n",
    "            print(f\"  ⚠️ No pude inferir el año desde el nombre: {nombre}\")\n",
    "            continue\n",
    "\n",
    "        rutas_control = controles_por_año.get(año, [])\n",
    "        print(f\"\\n📄 {nombre}  |  Año: {año}\")\n",
    "        if not rutas_control:\n",
    "            print(\"  ⚠️ No hay archivos de Control Tachadas configurados para este año.\")\n",
    "            continue\n",
    "\n",
    "        # 1) Leer LIMPIO\n",
    "        try:\n",
    "            df_limpio = pd.read_excel(ruta_limpio, dtype={COL_ID_LIMPIOS: object})\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error leyendo LIMPIO: {e}\")\n",
    "            continue\n",
    "\n",
    "        if COL_ID_LIMPIOS not in df_limpio.columns:\n",
    "            print(f\"  ⚠️ Falta columna '{COL_ID_LIMPIOS}' en LIMPIO.\")\n",
    "            continue\n",
    "\n",
    "        ids_limpios_unq = normalizar_y_unicos(df_limpio[COL_ID_LIMPIOS])\n",
    "\n",
    "        # 2) LAB: DF completo + clave normalizada por fila\n",
    "        df_lab_control = cargar_df_control_tachadas(rutas_control, hoja=HOJA_CONTROL_TACHADAS)\n",
    "        if df_lab_control.empty:\n",
    "            print(\"  ⚠️ No se pudo armar DF de Control Tachadas.\")\n",
    "            continue\n",
    "        if COL_ID_LAB not in df_lab_control.columns:\n",
    "            print(f\"  ⚠️ Falta columna '{COL_ID_LAB}' en Control Tachadas del LAB.\")\n",
    "            continue\n",
    "\n",
    "        df_lab_aux = df_lab_control.copy()\n",
    "        df_lab_aux[\"_id_norm\"] = df_lab_aux[COL_ID_LAB].map(normalizar_id)\n",
    "\n",
    "        # Únicos LAB (normalizados)\n",
    "        ids_lab_unq = normalizar_y_unicos(df_lab_aux[COL_ID_LAB])\n",
    "\n",
    "        # 3) Comparación\n",
    "        set_lab = set(ids_lab_unq)\n",
    "        set_limpios = set(ids_limpios_unq)\n",
    "\n",
    "        faltantes_en_limpios = sorted(set_lab - set_limpios, key=lambda x: (len(x), x))\n",
    "        sobrantes_en_limpios = sorted(set_limpios - set_lab, key=lambda x: (len(x), x))\n",
    "\n",
    "        print(f\"  Únicos LAB (control, sin dups): {len(set_lab)}\")\n",
    "        print(f\"  Únicos LIMPIOS:                 {len(set_limpios)}\")\n",
    "        print(f\"  ➖ En LAB y faltan en LIMPIOS:  {len(faltantes_en_limpios)}\")\n",
    "        print(f\"  ➕ En LIMPIOS y no en LAB:      {len(sobrantes_en_limpios)}\")\n",
    "\n",
    "        if len(faltantes_en_limpios) == 0:\n",
    "            print(\"  ✅ Chequeo principal OK (no faltan tachadas en LIMPIOS).\")\n",
    "        else:\n",
    "            print(f\"     Ejemplos faltantes: {faltantes_en_limpios[:10]}\")\n",
    "\n",
    "        # 4) Exportar validación (en subcarpeta)\n",
    "        if EXPORTAR_VALIDACION:\n",
    "            base_out = os.path.splitext(nombre)[0]\n",
    "            ruta_out = os.path.join(carpeta_salida, f\"{base_out}_VALIDACION_TACHADAS.xlsx\")\n",
    "            with pd.ExcelWriter(ruta_out, engine=\"openpyxl\") as xw:\n",
    "                # Resumen\n",
    "                pd.DataFrame({\n",
    "                    \"archivo\": [nombre],\n",
    "                    \"año\": [año],\n",
    "                    \"unicos_lab_sin_dups\": [len(set_lab)],\n",
    "                    \"unicos_limpios\": [len(set_limpios)],\n",
    "                    \"faltantes_en_limpios\": [len(faltantes_en_limpios)],\n",
    "                    \"sobrantes_en_limpios\": [len(sobrantes_en_limpios)],\n",
    "                }).to_excel(xw, index=False, sheet_name=\"resumen\")\n",
    "\n",
    "                # Listas\n",
    "                pd.Series(faltantes_en_limpios, name=COL_ID_LAB).to_frame().to_excel(\n",
    "                    xw, index=False, sheet_name=\"faltantes_en_limpias\"\n",
    "                )\n",
    "                pd.Series(sobrantes_en_limpios, name=COL_ID_LIMPIOS).to_frame().to_excel(\n",
    "                    xw, index=False, sheet_name=\"sobrantes_en_limpias\"\n",
    "                )\n",
    "\n",
    "                # Réplica total del control (todas las columnas)\n",
    "                df_lab_control.to_excel(xw, index=False, sheet_name=\"lab_control_original\")\n",
    "\n",
    "                # LAB sin duplicados por Identificador (normalizado)\n",
    "                df_lab_sin_dups = df_lab_aux.drop_duplicates(subset=[\"_id_norm\"]).drop(columns=[\"_id_norm\"])\n",
    "                df_lab_sin_dups.to_excel(xw, index=False, sheet_name=\"lab_control_sin_dups\")\n",
    "\n",
    "                # Detalle del LAB solo para los FALTANTES (todas sus columnas)\n",
    "                if faltantes_en_limpios:\n",
    "                    faltantes_set = set(faltantes_en_limpios)\n",
    "                    lab_faltantes_detalle = df_lab_aux[df_lab_aux[\"_id_norm\"].isin(faltantes_set)].drop(columns=[\"_id_norm\"])\n",
    "                    lab_faltantes_detalle.to_excel(xw, index=False, sheet_name=\"lab_faltantes_detalle\")\n",
    "                else:\n",
    "                    # Hoja vacía pero creada para consistencia\n",
    "                    pd.DataFrame(columns=df_lab_control.columns).to_excel(\n",
    "                        xw, index=False, sheet_name=\"lab_faltantes_detalle\"\n",
    "                    )\n",
    "\n",
    "            print(f\"  💾 Validación exportada: {ruta_out}\")\n",
    "\n",
    "# =========================\n",
    "#   EJECUCIÓN\n",
    "# =========================\n",
    "#validar_planta(carpeta_jpv, CONTROL_TACHADAS_JPV, \"JPV\", validacion_dir_jpv)\n",
    "validar_planta(carpeta_rb,  CONTROL_TACHADAS_RB,  \"RB\",  validacion_dir_rb)\n",
    "print(\"\\n✅ Validación finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926ec2b",
   "metadata": {},
   "source": [
    "# Cálculo de Humedad y Temperatura\n",
    "## Cruce con curvas de calibración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213f70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AVISO] JPV 2024: variedades sin hoja de calibración -> inov, sli9193\n",
      "Listo -> C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\JPV_2024_calibrado.xlsx\n",
      "[AVISO] JPV 2025: variedades sin hoja de calibración -> inov, nan\n",
      "Listo -> C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\JPV_2025_calibrado.xlsx\n",
      "[AVISO] RB 2024: variedades sin hoja de calibración -> cl1294\n",
      "Listo -> C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\RB_2024_calibrado.xlsx\n",
      "[AVISO] RB 2025: variedades sin hoja de calibración -> xp 117\n",
      "Listo -> C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\RB_2025_calibrado.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===================== RUTAS BASE =====================\n",
    "BASE_EDITABLE = Path(r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\")\n",
    "\n",
    "BASE_CONS_JPV = BASE_EDITABLE / r\"_consolidados\\JPV_por_año\"\n",
    "BASE_CONS_RB  = BASE_EDITABLE / r\"_consolidados\\RB_por_año\"\n",
    "\n",
    "# Salidas\n",
    "BASE_OUT = BASE_EDITABLE / r\"_consolidados\\_salidas_calibradas\"\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===================== ESQUEMA CONSOLIDADO =====================\n",
    "ORIG = [\n",
    "    \"planta\",\"año\",\"tirada_fecha\",\"sensor_id\",\"timestamp\",\n",
    "    \"VOLT_HUM\",\"VOLT_TEM\",\"Variedad\",\"ID_tachada\",\"DESCARTAR\"\n",
    "]\n",
    "COL_SENSOR_ID = \"sensor_id\"\n",
    "COL_VARIEDAD  = \"Variedad\"\n",
    "COL_VH        = \"VOLT_HUM\"\n",
    "COL_VT        = \"VOLT_TEM\"\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def deaccent(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return ''.join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c)).casefold()\n",
    "\n",
    "def norm_str(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return deaccent(' '.join(s.strip().split()))\n",
    "\n",
    "def guess_secadora(sensor_id) -> Optional[int]:\n",
    "    if pd.isna(sensor_id):\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\", str(sensor_id))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def find_cell(df: pd.DataFrame, value: str):\n",
    "    matches = np.where(df.values == value)\n",
    "    if len(matches[0]) == 0:\n",
    "        return None, None\n",
    "    return int(matches[0][0]), int(matches[1][0])\n",
    "\n",
    "def parse_humedad_sheet(df_raw: pd.DataFrame):\n",
    "    AH = float(df_raw.iloc[1, 0]); BH = float(df_raw.iloc[1, 1]); CH = float(df_raw.iloc[1, 2])\n",
    "    r, c = find_cell(df_raw, \"Fecha\")\n",
    "    if r is None: raise ValueError(\"No encontré 'Fecha' en hoja de HUMEDAD.\")\n",
    "    cfix_row = r + 1\n",
    "    cols = list(range(c + 1, c + 7))  # 6 secadoras\n",
    "    cfix = {i: float(df_raw.iloc[cfix_row, col]) for i, col in enumerate(cols, start=1)}\n",
    "    data = df_raw.iloc[(r + 2):, [c] + cols].copy()\n",
    "    data.columns = [\"fecha\"] + [f\"s{i}\" for i in range(1, 7)]\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data[\"fecha\"] = pd.to_datetime(data[\"fecha\"], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"fecha\"])\n",
    "    cvar_tbl = {}\n",
    "    for i in range(1, 7):\n",
    "        tmp = data[[\"fecha\", f\"s{i}\"]].rename(columns={f\"s{i}\": \"craw\"})\n",
    "        tmp[\"craw\"] = tmp[\"craw\"].replace(0, np.nan)\n",
    "        tmp = tmp.sort_values(\"fecha\")\n",
    "        tmp[\"cvar\"] = tmp[\"craw\"].ffill().fillna(0.0)\n",
    "        cvar_tbl[i] = tmp[[\"fecha\", \"cvar\"]].reset_index(drop=True)\n",
    "    return AH, BH, CH, cfix, cvar_tbl\n",
    "\n",
    "def parse_temperatura_sheet(df_raw: pd.DataFrame):\n",
    "    AT = float(df_raw.iloc[1, 0]); BT = float(df_raw.iloc[1, 1])\n",
    "    r, c = find_cell(df_raw, \"Fecha\")\n",
    "    if r is None: raise ValueError(\"No encontré 'Fecha' en hoja TEMPERATURA.\")\n",
    "    cfix_row = r + 1\n",
    "    cols = list(range(c + 1, c + 7))\n",
    "    cfix = {i: float(df_raw.iloc[cfix_row, col]) for i, col in enumerate(cols, start=1)}\n",
    "    data = df_raw.iloc[(r + 2):, [c] + cols].copy()\n",
    "    data.columns = [\"fecha\"] + [f\"s{i}\" for i in range(1, 7)]\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data[\"fecha\"] = pd.to_datetime(data[\"fecha\"], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"fecha\"])\n",
    "    cvar_tbl = {}\n",
    "    for i in range(1, 7):\n",
    "        tmp = data[[\"fecha\", f\"s{i}\"]].rename(columns={f\"s{i}\": \"craw\"})\n",
    "        tmp[\"craw\"] = tmp[\"craw\"].replace(0, np.nan)\n",
    "        tmp = tmp.sort_values(\"fecha\")\n",
    "        tmp[\"cvar\"] = tmp[\"craw\"].ffill().fillna(0.0)\n",
    "        cvar_tbl[i] = tmp[[\"fecha\", \"cvar\"]].reset_index(drop=True)\n",
    "    return AT, BT, cfix, cvar_tbl\n",
    "\n",
    "def merge_asof_cvar(df, key_fecha, key_secadora, cvar_tbl, out_col):\n",
    "    df[out_col] = 0.0\n",
    "    for s in range(1, 7):\n",
    "        mask = df[key_secadora] == s\n",
    "        if not mask.any(): continue\n",
    "        base = df.loc[mask, [key_fecha]].copy().sort_values(key_fecha)\n",
    "        cv = cvar_tbl[s].sort_values(\"fecha\")\n",
    "        merged = pd.merge_asof(base, cv.rename(columns={\"fecha\": key_fecha}), on=key_fecha, direction=\"backward\")\n",
    "        merged[\"cvar\"] = merged[\"cvar\"].fillna(0.0)\n",
    "        idx_sorted = df.loc[mask].sort_values(key_fecha).index\n",
    "        df.loc[idx_sorted, out_col] = merged.set_index(idx_sorted)[\"cvar\"].values\n",
    "    return df\n",
    "\n",
    "# === Equivalencias de variedades (no destructivas) ===\n",
    "ALIAS_EQUIV = {\n",
    "    # Grupo Merín / L5903\n",
    "    \"merin\": {\"merin\", \"l5903\"},\n",
    "    \"l5903\": {\"merin\", \"l5903\"},\n",
    "\n",
    "    # Grupo SLIO9193 / SLI9193 / 9193\n",
    "    \"slio9193\": {\"slio9193\", \"sli9193\", \"9193\"},\n",
    "    \"sli9193\": {\"slio9193\", \"sli9193\", \"9193\"},\n",
    "    \"9193\": {\"slio9193\", \"sli9193\", \"9193\"},\n",
    "}\n",
    "\n",
    "def resolve_variedad_key(key_norm: str, name_map: Dict[str, str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Dado el nombre normalizado de la variedad (key_norm) y el mapa name_map\n",
    "    (nombre_normalizado_de_hoja -> nombre_hoja_original), devuelve cuál\n",
    "    de los equivalentes existe como hoja en el Excel de curvas.\n",
    "    \"\"\"\n",
    "    candidatos = ALIAS_EQUIV.get(key_norm, {key_norm})\n",
    "    for cand in candidatos:\n",
    "        if cand in name_map:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "# ===================== PATHS POR PLANTA =====================\n",
    "def path_consolidado(planta: str, año: int) -> Path:\n",
    "    planta = planta.upper().strip()\n",
    "    if planta == \"JPV\":\n",
    "        base = BASE_CONS_JPV\n",
    "        nombre = f\"Consolidado_Sensores_JPV_{año}_TACHADAS_LIMPIAS.xlsx\"\n",
    "    elif planta == \"RB\":\n",
    "        base = BASE_CONS_RB\n",
    "        nombre = f\"Consolidado_Sensores_RB_{año}_TACHADAS_LIMPIAS.xlsx\"\n",
    "    else:\n",
    "        raise ValueError(\"planta debe ser 'JPV' o 'RB'\")\n",
    "    p = base / nombre\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"No encontré el consolidado para {planta} {año}: {p}\")\n",
    "    return p\n",
    "\n",
    "def path_curvas(planta: str, año: int) -> Path:\n",
    "    planta = planta.upper().strip()\n",
    "    if planta == \"JPV\":\n",
    "        patron = f\"*Datos JPV*/*{año}*Curvas*JPV*.xlsx\"\n",
    "    elif planta == \"RB\":\n",
    "        patron = f\"*Datos RB*/*{año}*Curvas*RB*.xlsx\"\n",
    "    else:\n",
    "        raise ValueError(\"planta debe ser 'JPV' o 'RB'\")\n",
    "    candidatos = list(BASE_EDITABLE.rglob(patron))\n",
    "    if not candidatos:\n",
    "        raise FileNotFoundError(f\"No encontré el Excel de curvas para {planta} {año} usando patrón {patron}\")\n",
    "    return candidatos[0]\n",
    "\n",
    "# ===================== PIPELINE GENERAL =====================\n",
    "def calibrar_por_planta_y_año(planta: str, año: int) -> Path:\n",
    "    # 1) Consolidado\n",
    "    p_in = path_consolidado(planta, año)\n",
    "    df = pd.read_excel(p_in)\n",
    "    faltan = [c for c in ORIG if c not in df.columns]\n",
    "    if faltan:\n",
    "        raise KeyError(f\"Faltan columnas en el consolidado: {faltan}\")\n",
    "\n",
    "    # 2) Timestamp (con hora) y secadora (solo para cálculos internos)\n",
    "    fecha_ref = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")  # sin normalize: respeta hora\n",
    "    secadora = df[\"secadora\"] if \"secadora\" in df.columns else df[COL_SENSOR_ID].apply(guess_secadora)\n",
    "\n",
    "    # 3) Voltajes limpios\n",
    "    vh = pd.to_numeric(df[COL_VH], errors=\"coerce\")\n",
    "    vt = pd.to_numeric(df[COL_VT], errors=\"coerce\")\n",
    "    mask_vh_zero = (vh.fillna(0) == 0)\n",
    "    mask_vt_zero = (vt.fillna(0) == 0)\n",
    "\n",
    "    # 4) Curvas\n",
    "    p_curvas = path_curvas(planta, año)\n",
    "\n",
    "    # --- TEMPERATURA ---\n",
    "    df_temp_raw = pd.read_excel(p_curvas, sheet_name=\"TEMPERATURA\", header=None)\n",
    "    AT, BT, cfix_T, cvar_T = parse_temperatura_sheet(df_temp_raw)\n",
    "    aux_T = pd.DataFrame({\"fecha_ref\": fecha_ref, \"secadora\": secadora})\n",
    "    aux_T = merge_asof_cvar(aux_T, \"fecha_ref\", \"secadora\", cvar_T, \"cvar_T\")\n",
    "    cfix_T_series = aux_T[\"secadora\"].map(cfix_T).astype(float)\n",
    "    TEMPERATURA = vt * AT + BT + cfix_T_series - aux_T[\"cvar_T\"]\n",
    "    TEMPERATURA = TEMPERATURA.mask(mask_vt_zero | vt.isna(), np.nan)\n",
    "\n",
    "    # --- HUMEDAD (por variedad) ---\n",
    "    xl = pd.ExcelFile(p_curvas)\n",
    "    hojas_hum = [s for s in xl.sheet_names if norm_str(s) != norm_str(\"TEMPERATURA\")]\n",
    "    name_map = {norm_str(s): s for s in hojas_hum}\n",
    "    cache_params: Dict[str, Tuple[float,float,float,Dict[int,float],Dict[int,pd.DataFrame]]] = {}\n",
    "\n",
    "    HUMEDAD = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    variedad_norm = df[COL_VARIEDAD].astype(str).map(norm_str)\n",
    "\n",
    "    faltantes: List[str] = []\n",
    "\n",
    "    for key_norm in variedad_norm.dropna().unique():\n",
    "        key_lookup = resolve_variedad_key(key_norm, name_map)\n",
    "        if key_lookup is None:\n",
    "            faltantes.append(key_norm)\n",
    "            continue\n",
    "\n",
    "        if key_lookup not in cache_params:\n",
    "            raw = pd.read_excel(p_curvas, sheet_name=name_map[key_lookup], header=None)\n",
    "            cache_params[key_lookup] = parse_humedad_sheet(raw)\n",
    "\n",
    "        AH, BH, CH, cfix_H, cvar_H = cache_params[key_lookup]\n",
    "        mask = (variedad_norm == key_norm)\n",
    "\n",
    "        aux_H = pd.DataFrame({\"fecha_ref\": fecha_ref[mask], \"secadora\": secadora[mask]})\n",
    "        aux_H = merge_asof_cvar(aux_H, \"fecha_ref\", \"secadora\", cvar_H, \"cvar_H\")\n",
    "        cfix_H_series = aux_H[\"secadora\"].map(cfix_H).astype(float)\n",
    "        HUMEDAD.loc[mask] = (vh[mask]**2) * AH + vh[mask] * BH + CH + cfix_H_series - aux_H[\"cvar_H\"]\n",
    "\n",
    "\n",
    "    HUMEDAD = HUMEDAD.mask(mask_vh_zero | vh.isna(), np.nan)\n",
    "\n",
    "    if faltantes:\n",
    "        faltantes_uniq = sorted(set(faltantes))\n",
    "        print(f\"[AVISO] {planta} {año}: variedades sin hoja de calibración -> {', '.join(faltantes_uniq)}\")\n",
    "\n",
    "    # 5) Salida: 10 originales + HUMEDAD, TEMPERATURA\n",
    "    out = df[[c for c in ORIG if c in df.columns]].copy()\n",
    "    out[\"HUMEDAD\"] = HUMEDAD\n",
    "    out[\"TEMPERATURA\"] = TEMPERATURA\n",
    "\n",
    "    p_out = BASE_OUT / f\"{planta.upper()}_{año}_calibrado.xlsx\"\n",
    "    with pd.ExcelWriter(p_out, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd hh:mm:ss\") as xw:\n",
    "        out.to_excel(xw, index=False, sheet_name=\"datos\")\n",
    "    print(f\"Listo -> {p_out}\")\n",
    "    return p_out\n",
    "\n",
    "# --------- EJECUCIÓN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Elegir cuales ejecutar:\n",
    "    jobs = [\n",
    "         (\"JPV\", 2024),\n",
    "         (\"JPV\", 2025),\n",
    "         (\"RB\", 2024),\n",
    "         (\"RB\", 2025),\n",
    "    ]\n",
    "    for planta, año in jobs:\n",
    "        try:\n",
    "            calibrar_por_planta_y_año(planta, año)\n",
    "        except Exception as e:\n",
    "                print(f\"[ERROR] {planta} {año}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537556d",
   "metadata": {},
   "source": [
    "# Generación de Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbb9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Procesando: RB_2025_calibrado.xlsx\n",
      "\n",
      "🧾 Listo. Imágenes generadas: 351\n"
     ]
    }
   ],
   "source": [
    "# ---------- CONFIG ----------\n",
    "INPUT_FILES = [\n",
    "#r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\JPV_2024_calibrado.xlsx\",\n",
    "#r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\JPV_2025_calibrado.xlsx\",\n",
    "#r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\RB_2024_calibrado.xlsx\",\n",
    "r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\\RB_2025_calibrado.xlsx\"\n",
    "]\n",
    "\n",
    "# Carpeta base donde se creará \"Gráficos Tachadas\"\n",
    "OUTPUT_BASE = r\"C:\\Users\\augus\\OneDrive - Universidad de Montevideo\\Tésis - Latitud\\Secado Latitud - UM - Editable\\_consolidados\\_salidas_calibradas\"\n",
    "\n",
    "# Columna de tiempo y nombres alternativos de temperatura y humedad (detección flexible)\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "TEMP_CANDIDATES = [\"Temperatura\",\"TEMPERATURA\"]\n",
    "HUM_CANDIDATES  = [\"Humedad\",\"HUMEDAD\"]\n",
    "\n",
    "# Escalas fijas (pueden cambiarse o desactivarse)\n",
    "FORCE_YLIMS = True\n",
    "TEMP_YLIM   = (25, 55)   # °C\n",
    "HUM_YLIM    = (11, 25)   # %\n",
    "TEMP_YTICK  = 2          # paso de ticks (None para automático)\n",
    "HUM_YTICK   = 2\n",
    "\n",
    "# Sistema de muestras (True y definir max para no generar todos, False para generar todos)\n",
    "SAMPLE_MODE = False\n",
    "SAMPLE_MAX  = 10\n",
    "\n",
    "FIGSIZE = (6, 6.5)  # más cuadrado como el ejemplo\n",
    "DPI     = 150\n",
    "\n",
    "# ---------- UTILIDADES ----------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def slugify(value: str) -> str:\n",
    "    value = str(value)\n",
    "    value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    value = re.sub(r\"[^\\w\\s-]\", \"\", value).strip()\n",
    "    value = re.sub(r\"[-\\s]+\", \" \", value)\n",
    "    return value.replace(\" \", \"_\")\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: list):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"No se encontró ninguna de estas columnas en el archivo: {candidates}\")\n",
    "\n",
    "def short_year(y):\n",
    "    try:\n",
    "        return str(int(y))[-2:]\n",
    "    except:\n",
    "        return str(y)[-2:]\n",
    "\n",
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() in {\".xlsx\", \".xls\"}:\n",
    "        return pd.read_excel(path)\n",
    "    elif path.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato no soportado: {path.suffix}\")\n",
    "\n",
    "def add_min_acum(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        raise KeyError(f\"Falta la columna de timestamp: {TIMESTAMP_COL}\")\n",
    "    df = df.copy()\n",
    "    df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL])\n",
    "    df = df.sort_values([\"planta\", \"año\", \"ID_tachada\", TIMESTAMP_COL])\n",
    "    df[\"min_acum\"] = (\n",
    "        df.groupby([\"planta\",\"año\",\"ID_tachada\"])[TIMESTAMP_COL]\n",
    "          .transform(lambda s: (s - s.min()).dt.total_seconds()/60.0)\n",
    "          .round(2)\n",
    "    )\n",
    "    df[\"horas\"] = (df[\"min_acum\"]/60.0).round(3)\n",
    "    return df\n",
    "\n",
    "# ---------- GRÁFICO ----------\n",
    "def plot_tachada(\n",
    "    gdf: pd.DataFrame,\n",
    "    variedad: str, id_tachada, planta: str, año,\n",
    "    temp_col: str, hum_col: str, out_dir: Path\n",
    "):\n",
    "    title = f\"{variedad} tachada {id_tachada}_{short_year(año)}\"\n",
    "    fname = slugify(title) + \".png\"\n",
    "    out_path = out_dir / fname\n",
    "\n",
    "    # Fondo BLANCO explícito\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=FIGSIZE, dpi=DPI, sharex=True,\n",
    "        gridspec_kw={\"height_ratios\":[1,1]}, facecolor=\"white\"\n",
    "    )\n",
    "\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_facecolor(\"white\")\n",
    "        ax.grid(True, axis=\"y\", linestyle=\"-\", linewidth=0.6, alpha=0.35, color=\"#7f7f7f\")\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(\"black\")\n",
    "        ax.tick_params(colors=\"black\", labelsize=9)\n",
    "\n",
    "    # Título DENTRO de la figura\n",
    "    ax1.set_title(title, fontsize=12, color=\"black\", pad=6)\n",
    "\n",
    "    # Temperatura (azul)\n",
    "    ax1.plot(gdf[\"horas\"], gdf[temp_col], linewidth=1.8, color=\"#1f77b4\")\n",
    "    ax1.set_ylabel(\"Temperatura (°C)\", color=\"black\")\n",
    "\n",
    "    # Humedad (rojo)\n",
    "    ax2.plot(gdf[\"horas\"], gdf[hum_col], linewidth=1.8, color=\"#d62728\")\n",
    "    ax2.set_xlabel(\"Tiempo (h)\", color=\"black\")\n",
    "    ax2.set_ylabel(\"Humedad (%)\", color=\"black\")\n",
    "\n",
    "    # Escalas fijas + ticks como el ejemplo\n",
    "    if FORCE_YLIMS:\n",
    "        ax1.set_ylim(*TEMP_YLIM)\n",
    "        ax2.set_ylim(*HUM_YLIM)\n",
    "        if TEMP_YTICK:\n",
    "            ax1.set_yticks(np.arange(TEMP_YLIM[0], TEMP_YLIM[1]+1e-9, TEMP_YTICK))\n",
    "        if HUM_YTICK:\n",
    "            ax2.set_yticks(np.arange(HUM_YLIM[0], HUM_YLIM[1]+1e-9, HUM_YTICK))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    base_dir = ensure_dir(Path(OUTPUT_BASE) / \"Gráficos Tachadas\")\n",
    "    total_imgs = 0\n",
    "\n",
    "    for f in INPUT_FILES:\n",
    "        fpath = Path(f)\n",
    "        if not fpath.exists():\n",
    "            print(f\"⚠️  No se encontró: {fpath}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n📄 Procesando: {fpath.name}\")\n",
    "        df = read_any(fpath)\n",
    "\n",
    "        needed = [\"planta\",\"año\",\"ID_tachada\",\"Variedad\", TIMESTAMP_COL]\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"   ❌ Faltan columnas: {missing}. Salteado.\")\n",
    "            continue\n",
    "\n",
    "        temp_col = pick_col(df, TEMP_CANDIDATES)\n",
    "        hum_col  = pick_col(df, HUM_CANDIDATES)\n",
    "\n",
    "        # Evitar que falte humedad/temperatura\n",
    "        for c in (temp_col, hum_col):\n",
    "            if df[c].isna().all():\n",
    "                print(f\"   ⚠️  La columna '{c}' está vacía en este archivo.\")\n",
    "        \n",
    "        df = add_min_acum(df)\n",
    "        groups = df.groupby([\"planta\",\"año\",\"ID_tachada\",\"Variedad\"], sort=True)\n",
    "\n",
    "        iterable = list(groups)\n",
    "        if SAMPLE_MODE:\n",
    "            iterable = iterable[:SAMPLE_MAX]\n",
    "\n",
    "        for (planta, año, id_t, variedad), g in iterable:\n",
    "            out_dir = ensure_dir(base_dir / str(planta) / str(int(año)))\n",
    "            try:\n",
    "                out = plot_tachada(\n",
    "                    g.sort_values(\"horas\"),\n",
    "                    variedad=variedad, id_tachada=id_t, planta=planta, año=año,\n",
    "                    temp_col=temp_col, hum_col=hum_col, out_dir=out_dir\n",
    "                )\n",
    "                total_imgs += 1\n",
    "                #print(f\"   ✅ {planta}/{año} -> {out.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Error graficando {planta}-{año}-{id_t}: {e}\")\n",
    "\n",
    "    print(f\"\\n🧾 Listo. Imágenes generadas: {total_imgs}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45200b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
